{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0268e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Python library and module and configure running information.\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops\n",
    "from easydict import EasyDict as edict\n",
    "cfg = edict({\n",
    " 'name': 'movie review',\n",
    " 'pre_trained': False,\n",
    " 'num_classes': 2,\n",
    " 'batch_size': 64,\n",
    " 'epoch_size': 4,\n",
    " 'weight_decay': 3e-5,\n",
    " 'data_path': 'C:/Users/DELL/Documents/HCIA/HCIA-AI V3.5/X-Experiment-Guide/2. Deep Learning and AI Development Framework Lab Guide/TextCNN/data/',\n",
    " 'device_target': 'CPU',\n",
    " 'device_id': 0,\n",
    " 'keep_checkpoint_max': 1,\n",
    " 'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n",
    " 'word_len': 51,\n",
    " 'vec_length': 40\n",
    "})\n",
    "mindspore.set_device(cfg.device_target)\n",
    "context.set_context(mode=context.GRAPH_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d50ac340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and process data.\n",
    "with open(\"C:/Users/DELL/Documents/HCIA/HCIA-AI V3.5/X-Experiment-Guide/2. Deep Learning and AI Development Framework Lab Guide/TextCNN/data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    " print(\"Negative reivews:\")\n",
    " for i in range(5):\n",
    "     print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"C:/Users/DELL/Documents/HCIA/HCIA-AI V3.5/X-Experiment-Guide/2. Deep Learning and AI Development Framework Lab Guide/TextCNN/data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    " print(\"Positive reivews:\")\n",
    " for i in range(5):\n",
    "     print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7521eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generation class.\n",
    "class Generator():\n",
    " def __init__(self, input_list):\n",
    "     self.input_list=input_list\n",
    " def __getitem__(self,item):\n",
    "     return (np.array(self.input_list[item][0],dtype=np.int32), np.array(self.input_list[item][1],dtype=np.int32))\n",
    " def __len__(self):\n",
    "     return len(self.input_list)\n",
    "    \n",
    "class MovieReview:\n",
    " '''\n",
    " Movie review dataset\n",
    " '''\n",
    " def __init__(self, root_dir, maxlen, split):\n",
    "     '''\n",
    "     input:\n",
    "         root_dir: movie review data directory\n",
    "         maxlen: maximum length of a sentence\n",
    "         split: the training/validation ratio in the dataset\n",
    "     '''\n",
    "     self.path = root_dir\n",
    "     self.feelMap = {\n",
    "         'neg':0,\n",
    "         'pos':1\n",
    "         }\n",
    "     self.files = []\n",
    "     self.doConvert = False\n",
    "\n",
    "     mypath = Path(self.path)\n",
    "     if not mypath.exists() or not mypath.is_dir():\n",
    "         print(\"please check the root_dir!\")\n",
    "         raise ValueError\n",
    "     # Find the files in the data directory.\n",
    "     for root,_,filename in os.walk(self.path):\n",
    "         for each in filename:\n",
    "             self.files.append(os.path.join(root,each))\n",
    "         break\n",
    "     # Check whether there are two files: .neg and .pos.\n",
    "     if len(self.files) != 2:\n",
    "         print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "         raise ValueError\n",
    "            \n",
    "     # Read data.\n",
    "     self.word_num = 0\n",
    "     self.maxlen = 0\n",
    "     self.minlen = float(\"inf\")\n",
    "     self.maxlen = float(\"-inf\")\n",
    "     self.Pos = []\n",
    "     self.Neg = []\n",
    "     for filename in self.files:\n",
    "         self.read_data(filename)\n",
    "     self.text2vec(maxlen=maxlen)\n",
    "     self.split_dataset(split=split)\n",
    " \n",
    " def read_data(self, filePath):\n",
    "     with open(filePath,'r') as f:\n",
    "         for sentence in f.readlines():\n",
    "             sentence = sentence.replace('\\n', '')\\\n",
    "                               .replace('\"', '')\\\n",
    "                               .replace('\\'', '')\\\n",
    "                               .replace('.', '')\\\n",
    "                               .replace(',', '')\\\n",
    "                               .replace('[', '')\\\n",
    "                               .replace(']', '')\\\n",
    "                               .replace('(', '')\\\n",
    "                               .replace(')', '')\\\n",
    "                               .replace(':', '')\\\n",
    "                               .replace('--', '')\\\n",
    "                               .replace('-', ' ')\\\n",
    "                               .replace('\\\\', '')\\\n",
    "                               .replace('0', '')\\\n",
    "                               .replace('1', '')\\\n",
    "                               .replace('2', '')\\\n",
    "                               .replace('3', '')\\\n",
    "                               .replace('4', '')\\\n",
    "                               .replace('5', '')\\\n",
    "                               .replace('6', '')\\\n",
    "                               .replace('7', '')\\\n",
    "                               .replace('8', '')\\\n",
    "                               .replace('9', '')\\\n",
    "                               .replace('`', '')\\\n",
    "                               .replace('=', '')\\\n",
    "                               .replace('$', '')\\\n",
    "                               .replace('/', '')\\\n",
    "                               .replace('*', '')\\\n",
    "                               .replace(';', '')\\\n",
    "                               .replace('<b>', '')\\\n",
    "                               .replace('%', '')\n",
    "            \n",
    "             sentence = sentence.split(' ')\n",
    "             sentence = list(filter(lambda x: x, sentence))\n",
    "             if sentence:\n",
    "                 self.word_num += len(sentence)\n",
    "                 self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                 self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                 if 'pos' in filePath:\n",
    "                     self.Pos.append([sentence,self.feelMap['pos']])\n",
    "                 else:\n",
    "                     self.Neg.append([sentence,self.feelMap['neg']])\n",
    "                        \n",
    "\n",
    " def text2vec(self, maxlen):\n",
    "     '''\n",
    "        # Convert sentences into vectors.\n",
    "     '''\n",
    "     # Vocab = {word : index}\n",
    "     self.Vocab = dict()\n",
    "     # self.Vocab['None']\n",
    "     for SentenceLabel in self.Pos+self.Neg:\n",
    "         vector = [0]*maxlen\n",
    "         for index, word in enumerate(SentenceLabel[0]):\n",
    "             if index >= maxlen:\n",
    "                 break\n",
    "             if word not in self.Vocab.keys():\n",
    "                 self.Vocab[word] = len(self.Vocab)\n",
    "                 vector[index] = len(self.Vocab) - 1\n",
    "             else:\n",
    "                 vector[index] = self.Vocab[word]\n",
    "         SentenceLabel[0] = vector\n",
    "         self.doConvert = True\n",
    "            \n",
    "            \n",
    " def split_dataset(self, split):\n",
    "     '''\n",
    "     Divide the dataset into a training set and a test set.\n",
    "     '''\n",
    "     trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "     trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "     trunk_num = int(1/(1-split))\n",
    "     pos_temp=list()\n",
    "     neg_temp=list()\n",
    "     for index in range(trunk_num):\n",
    "         pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "         neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "     self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "     self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "     random.shuffle(self.train)\n",
    "\n",
    "\n",
    " def get_dict_len(self):\n",
    "     '''\n",
    "     Obtain the length of a dictionary consisting of characters in a dataset.\n",
    "     '''\n",
    "     if self.doConvert:\n",
    "         return len(self.Vocab)\n",
    "     else:\n",
    "         print(\"Haven't finished Text2Vec\")\n",
    "         return -1\n",
    "        \n",
    " def create_train_dataset(self, epoch_size, batch_size):\n",
    "     dataset = ds.GeneratorDataset(source=Generator(input_list=self.train), column_names=[\"data\",\"label\"], shuffle=False)\n",
    "     dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "     dataset=dataset.repeat(epoch_size)\n",
    "     return dataset\n",
    "\n",
    " def create_test_dataset(self, batch_size):\n",
    "     dataset = ds.GeneratorDataset(source=Generator(input_list=self.test), column_names=[\"data\",\"label\"], shuffle=False)\n",
    "     dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aae808c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93f15124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[ 128, 2727,  122 ...    0,    0,    0],\n",
      " [ 726,    0,  198 ...    0,    0,    0],\n",
      " [7777,    5, 3466 ...    0,    0,    0],\n",
      " ...\n",
      " [  15, 5149, 5555 ...    0,    0,    0],\n",
      " [ 345, 2784, 3073 ...    0,    0,    0],\n",
      " [4773, 4774,    2 ...    0,    0,    0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, \n",
      " 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, \n",
      " 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1])}\n",
      "[  726     0   198  1716   111  5104    15  1457  1300   247  2577   655\n",
      "    64    10  3494   710  1793 11565   415    82   118  1609  3635 11566\n",
      "  1847   855  1177    10    15  1519   694    32  1442     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Code for displaying the data processing result:\n",
    "vocab_size=instance.get_dict_len()\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item =dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    " if i<1:\n",
    "     print(data)\n",
    "     print(data['data'][1])\n",
    " else:\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d89fbc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure training parameters.\n",
    "#Code for setting the learning rate:\n",
    "learning_rate = []\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num)\n",
    " for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num)\n",
    " for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in\n",
    " range(cfg.epoch_size - math.floor(cfg.epoch_size / 5)\n",
    " - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "829fd3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18848, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 0.00027819 -0.00079187  0.01734651 ... -0.000179   -0.00204298\n",
      "         0.01453898]\n",
      "       [ 0.0107973   0.00694471  0.00698016 ...  0.00153196 -0.00752804\n",
      "        -0.00645664]\n",
      "       [-0.01075113  0.01450333  0.00043253 ...  0.00591194 -0.00488818\n",
      "        -0.00259404]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.00917239  0.00591114 -0.00731728 ...  0.00213329 -0.00162149\n",
      "        -0.01235505]\n",
      "       [-0.00463171  0.00391021 -0.00767997 ... -0.00693404 -0.02037086\n",
      "         0.01444044]\n",
      "       [ 0.00574569  0.00505107  0.00224499 ...  0.0050235  -0.00736969\n",
      "        -0.02392101]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00478687  0.00272291 -0.0106408  ... -0.01782311 -0.00554349\n",
      "        -0.00289398]\n",
      "       [-0.01392197 -0.00551238  0.00839876 ... -0.00387033 -0.0106106\n",
      "         0.01638955]\n",
      "       [-0.012362    0.0154997   0.00908293 ... -0.00957141  0.01307474\n",
      "         0.0061835 ]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-0.02600851  0.01132349  0.00979101 ... -0.02621056  0.00204531\n",
      "        -0.00320306]\n",
      "       [ 0.00857447 -0.00050455 -0.01474948 ...  0.0161659   0.00946118\n",
      "        -0.00956189]\n",
      "       [-0.00131258  0.00544987 -0.01542953 ... -0.01877194 -0.02853668\n",
      "         0.01079531]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.02095752  0.00494966  0.01354242 ...  0.00701461 -0.00548345\n",
      "        -0.00836442]\n",
      "       [-0.01997795  0.00139434  0.00664195 ... -0.00803402 -0.00249576\n",
      "         0.00519098]\n",
      "       [ 0.01394954  0.00549533 -0.00622609 ...  0.01423885 -0.01054279\n",
      "         0.03282466]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.01023007  0.00208916 -0.00742745 ...  0.01283144 -0.00120995\n",
      "         0.00438201]\n",
      "       [-0.00146354  0.00197889 -0.01350089 ...  0.0236214  -0.01300495\n",
      "         0.01732804]\n",
      "       [-0.00629794 -0.00398165 -0.00381207 ...  0.00241435 -0.01590174\n",
      "         0.00436974]]]], bias_init=<mindspore.common.initializer.Uniform object at 0x00000179D30CA790>, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-2.57891370e-03  2.27917396e-02  1.12658262e-03 ... -2.77709472e-03\n",
      "         1.79185551e-02 -8.13312549e-03]\n",
      "       [-1.78551823e-02 -6.68266462e-03 -2.14977656e-03 ...  1.16566289e-02\n",
      "        -1.86278056e-02  6.22449629e-03]\n",
      "       [-3.10628526e-02  1.62627101e-02  1.77587271e-02 ... -2.59992038e-03\n",
      "         2.04352140e-02  5.11922128e-03]\n",
      "       [-4.83338861e-03  6.11251500e-03 -1.24922032e-02 ... -1.42317766e-03\n",
      "         7.60833174e-03  1.00502372e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-2.09615175e-02  9.24838893e-03  8.75884958e-04 ... -2.03892356e-03\n",
      "         1.33038480e-02 -4.76218760e-03]\n",
      "       [-1.51985302e-03  1.09063275e-02 -2.03066040e-03 ...  5.29472623e-03\n",
      "         1.48946308e-02 -2.01855320e-03]\n",
      "       [ 6.69338088e-03 -1.10105202e-02 -3.80333932e-03 ... -2.17788829e-03\n",
      "         9.04840790e-03 -5.17108152e-03]\n",
      "       [-3.02019798e-05 -2.07822286e-02 -1.86018681e-03 ... -1.88590288e-02\n",
      "         4.39590681e-03 -1.41790351e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-9.72166192e-03  1.28117539e-02  2.08497476e-02 ... -2.91866101e-02\n",
      "        -1.25320936e-02 -1.84666552e-02]\n",
      "       [-3.68809537e-03  1.68387331e-02  1.50979767e-02 ... -4.74889809e-03\n",
      "         5.31991129e-04  2.06501558e-02]\n",
      "       [ 1.32331653e-02  2.17756256e-03  1.61553686e-03 ... -2.66955147e-04\n",
      "         5.33332396e-03 -1.37460371e-02]\n",
      "       [ 1.16905691e-02  6.68457849e-03 -7.58070871e-03 ... -9.29302350e-03\n",
      "         1.65261403e-02 -5.98959019e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-1.05836580e-03 -5.68277529e-03 -8.45097844e-03 ...  9.01987869e-03\n",
      "         1.65879764e-02 -1.04386574e-02]\n",
      "       [ 3.23740230e-03  1.91949047e-02 -2.06596660e-03 ...  5.21237357e-03\n",
      "         1.01950290e-02  1.38686392e-02]\n",
      "       [ 5.38876187e-03  8.38561729e-03  1.72894001e-02 ... -6.52336096e-03\n",
      "        -1.05177844e-02 -2.72217277e-03]\n",
      "       [ 1.03329103e-02  3.93954455e-04  1.41742555e-02 ... -8.37850850e-03\n",
      "         8.73495918e-03 -1.59186835e-04]]]\n",
      "    \n",
      "    \n",
      "     [[[-7.08767038e-04  2.00617942e-03  2.42033042e-02 ...  1.57322418e-02\n",
      "        -2.03137342e-02  1.33223506e-02]\n",
      "       [-1.15379915e-02  1.44884631e-03  6.52009714e-03 ... -1.97727047e-02\n",
      "         6.13840704e-04  1.36581445e-02]\n",
      "       [ 1.95805598e-02  1.79164391e-02  5.63868613e-04 ... -7.51685258e-03\n",
      "        -1.18963011e-02 -1.06714508e-02]\n",
      "       [-1.33011946e-02  6.53223973e-03 -3.38053075e-03 ... -1.33151812e-02\n",
      "        -1.84158199e-02  1.43786531e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.48330035e-03 -1.51045248e-02 -3.46298655e-03 ... -2.94907298e-02\n",
      "         2.16865307e-03 -2.51962012e-03]\n",
      "       [ 5.26299467e-03  1.78841210e-03  1.20277144e-03 ...  1.29906079e-02\n",
      "         3.72114289e-03 -1.48301823e-02]\n",
      "       [-2.67597218e-03 -2.05496690e-04  8.84779636e-03 ... -3.71363218e-04\n",
      "        -2.66425721e-02 -4.95368801e-03]\n",
      "       [-9.41071939e-03  1.72314094e-03 -1.70289003e-03 ... -6.41545514e-03\n",
      "        -2.03033281e-03  1.21507226e-02]]]], bias_init=<mindspore.common.initializer.Uniform object at 0x00000179D30CAC70>, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 3.37924203e-03 -7.79407052e-03 -3.35884746e-03 ...  5.77450497e-04\n",
      "        -7.01047340e-03  4.12723422e-03]\n",
      "       [ 1.09321540e-02 -6.01573987e-03 -5.50515810e-03 ...  1.76486392e-02\n",
      "        -4.00253572e-03 -1.53181208e-02]\n",
      "       [ 2.03450501e-04  5.45269158e-03 -5.76612446e-03 ...  1.12509113e-02\n",
      "        -1.59300547e-02 -3.89400870e-03]\n",
      "       [-7.22652549e-05  1.20078474e-02 -1.45879118e-02 ...  5.86520554e-03\n",
      "         1.98806804e-02  4.05163784e-03]\n",
      "       [ 4.13483614e-03  2.44929339e-03  6.88368268e-03 ...  9.93468426e-03\n",
      "        -7.13420426e-03  1.53661275e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 9.72154550e-03  9.89714451e-03  1.52307181e-02 ...  9.11232643e-03\n",
      "        -3.42874601e-03 -1.97745971e-02]\n",
      "       [-1.11689465e-02 -4.64991061e-03 -1.92872947e-03 ...  1.58062309e-03\n",
      "        -6.93517097e-04 -1.01756966e-02]\n",
      "       [-1.93079398e-03 -1.19083989e-02  6.63646264e-03 ... -1.49657913e-02\n",
      "        -4.31815721e-03  8.33206903e-03]\n",
      "       [ 1.36117609e-02  3.19149601e-03  6.53116871e-03 ...  6.58783875e-03\n",
      "        -8.38816864e-04 -1.36022540e-02]\n",
      "       [ 3.26356385e-03  1.13654789e-02  7.39255780e-03 ...  7.81035190e-03\n",
      "        -1.11804586e-02 -1.65444575e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.27464579e-02 -4.76403860e-03 -1.29589960e-02 ...  1.43736824e-02\n",
      "         2.88038002e-03 -1.01766093e-02]\n",
      "       [-1.60836913e-02  1.66794937e-02 -5.36573702e-04 ... -2.98137963e-03\n",
      "        -4.02705232e-03 -1.03498464e-02]\n",
      "       [ 6.30944967e-03  4.87147644e-03 -1.95934027e-02 ...  1.98719441e-03\n",
      "        -7.49860378e-03  1.02935731e-02]\n",
      "       [-1.04148341e-02  1.11094890e-02 -3.00198537e-03 ... -1.51506262e-02\n",
      "        -8.58707353e-04  6.82217395e-03]\n",
      "       [ 1.96521580e-02 -6.48234459e-03 -1.22552868e-02 ...  1.03544006e-02\n",
      "         1.33844269e-02  1.18385898e-02]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 4.12312942e-03 -1.24481982e-02  5.84290829e-03 ... -1.29497321e-02\n",
      "         8.27279640e-04  1.31535577e-02]\n",
      "       [ 1.74506073e-04 -1.01425145e-02 -3.39481793e-03 ... -2.89353845e-03\n",
      "         5.08249737e-03 -5.70475077e-03]\n",
      "       [-1.32180657e-03 -2.37596198e-03 -1.01844389e-02 ...  1.46928495e-02\n",
      "         6.90109853e-04 -8.49734247e-03]\n",
      "       [-7.36352801e-03  1.00984909e-02  7.48514803e-03 ...  3.49802971e-02\n",
      "         3.87579273e-03  1.23211825e-02]\n",
      "       [ 1.80499181e-02 -1.32793598e-02  5.39697800e-03 ...  2.22676829e-03\n",
      "        -1.01669850e-02  2.97720963e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 3.28922365e-03 -1.42408144e-02 -3.08700744e-03 ...  4.25253343e-03\n",
      "         8.47912673e-03 -1.16108591e-02]\n",
      "       [ 9.61980224e-03  7.20223133e-03 -3.51318019e-03 ...  2.04179296e-03\n",
      "         3.98751302e-03 -1.51807647e-02]\n",
      "       [-2.74587376e-03  1.78754032e-02 -1.53563432e-02 ...  3.72233824e-03\n",
      "        -8.21800064e-03 -3.69806949e-04]\n",
      "       [-3.53472517e-03  8.99210281e-04  4.52788221e-03 ...  1.79103725e-02\n",
      "         4.35077632e-03  1.52035523e-02]\n",
      "       [ 7.08262669e-03  1.30743356e-02  5.16517134e-03 ...  9.90753155e-03\n",
      "        -1.27593465e-02  2.69527570e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-2.31140815e-02 -1.75011123e-03 -1.66513398e-03 ... -7.34763080e-03\n",
      "        -1.48882708e-02 -7.17145763e-03]\n",
      "       [-7.69637339e-03  1.29893632e-03 -9.29862377e-04 ... -5.59605937e-03\n",
      "        -2.14793975e-03  1.69088598e-03]\n",
      "       [-9.16719437e-03  3.36782984e-03  2.81687547e-03 ...  3.80595727e-03\n",
      "         5.82801877e-03  3.56508535e-03]\n",
      "       [-8.33201222e-04  3.93035775e-03  6.85476931e-03 ...  3.35737551e-03\n",
      "         1.46499428e-03  1.33573227e-02]\n",
      "       [-7.48279970e-03  1.13261184e-02  1.44990729e-02 ... -1.84068887e-03\n",
      "         1.66065563e-02  1.10717909e-02]]]], bias_init=<mindspore.common.initializer.Uniform object at 0x00000179D30CA880>, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<p=0.5>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# Define a TextCNN mode\n",
    "def _weight_variable(shape, factor=0.01):\n",
    " init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    " return Tensor(init_value)\n",
    "def make_conv_layer(kernel_size):\n",
    " weight_shape = (96, 1, *kernel_size)\n",
    " weight = _weight_variable(weight_shape)\n",
    " return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1, pad_mode=\"pad\", weight_init=weight, has_bias=True)\n",
    "\n",
    "class TextCNN(nn.Cell):\n",
    " def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "     super(TextCNN, self).__init__()\n",
    "     self.vec_length = vec_length\n",
    "     self.word_len = word_len\n",
    "     self.num_classes = num_classes\n",
    "     self.unsqueeze = ops.ExpandDims()\n",
    "     self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n",
    "     self.slice = ops.Slice()\n",
    "     self.layer1 = self.make_layer(kernel_height=3)\n",
    "     self.layer2 = self.make_layer(kernel_height=4)\n",
    "     self.layer3 = self.make_layer(kernel_height=5)\n",
    "     self.concat = ops.Concat(1)\n",
    "     self.fc = nn.Dense(96*3, self.num_classes)\n",
    "     self.drop = nn.Dropout(p=0.5)\n",
    "     self.print = ops.Print()\n",
    "     self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "\n",
    " def make_layer(self, kernel_height):\n",
    "     return nn.SequentialCell(\n",
    "         [\n",
    "             make_conv_layer((kernel_height,self.vec_length)),\n",
    "             nn.ReLU(),\n",
    "             nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n",
    "         ])\n",
    "    \n",
    " def construct(self,x):\n",
    "     x = self.unsqueeze(x, 1)\n",
    "     x = self.embedding(x)\n",
    "     x1 = self.layer1(x)\n",
    "     x2 = self.layer2(x)\n",
    "     x3 = self.layer3(x)\n",
    "     x1 = self.reducemean(x1, (2, 3))\n",
    "     x2 = self.reducemean(x2, (2, 3))\n",
    "     x3 = self.reducemean(x3, (2, 3))\n",
    "     x = self.concat((x1, x2, x3))\n",
    "     x = self.drop(x)\n",
    "     x = self.fc(x)\n",
    "     return x\n",
    "\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, num_classes=cfg.num_classes, vec_length=cfg.vec_length)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd7ad771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters.\n",
    "# Optimizer, loss function, checkpoint, and time monitor settings\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8961e81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 0.6973013281822205\n",
      "epoch: 1 step: 2, loss is 0.695469081401825\n",
      "epoch: 1 step: 3, loss is 0.6898638010025024\n",
      "epoch: 1 step: 4, loss is 0.6924630403518677\n",
      "epoch: 1 step: 5, loss is 0.693630576133728\n",
      "epoch: 1 step: 6, loss is 0.6910443305969238\n",
      "epoch: 1 step: 7, loss is 0.6881700158119202\n",
      "epoch: 1 step: 8, loss is 0.6828582286834717\n",
      "epoch: 1 step: 9, loss is 0.6851547956466675\n",
      "epoch: 1 step: 10, loss is 0.6964682340621948\n",
      "epoch: 1 step: 11, loss is 0.6865369081497192\n",
      "epoch: 1 step: 12, loss is 0.6884020566940308\n",
      "epoch: 1 step: 13, loss is 0.6969497203826904\n",
      "epoch: 1 step: 14, loss is 0.6922197937965393\n",
      "epoch: 1 step: 15, loss is 0.6972231864929199\n",
      "epoch: 1 step: 16, loss is 0.6979447603225708\n",
      "epoch: 1 step: 17, loss is 0.6974633932113647\n",
      "epoch: 1 step: 18, loss is 0.709978461265564\n",
      "epoch: 1 step: 19, loss is 0.6908731460571289\n",
      "epoch: 1 step: 20, loss is 0.694514274597168\n",
      "epoch: 1 step: 21, loss is 0.6897791624069214\n",
      "epoch: 1 step: 22, loss is 0.6855040788650513\n",
      "epoch: 1 step: 23, loss is 0.7038120627403259\n",
      "epoch: 1 step: 24, loss is 0.6844823360443115\n",
      "epoch: 1 step: 25, loss is 0.6988391876220703\n",
      "epoch: 1 step: 26, loss is 0.6959954500198364\n",
      "epoch: 1 step: 27, loss is 0.6917484998703003\n",
      "epoch: 1 step: 28, loss is 0.6865530014038086\n",
      "epoch: 1 step: 29, loss is 0.6916884779930115\n",
      "epoch: 1 step: 30, loss is 0.6966758966445923\n",
      "epoch: 1 step: 31, loss is 0.6897541880607605\n",
      "epoch: 1 step: 32, loss is 0.6906496286392212\n",
      "epoch: 1 step: 33, loss is 0.696287989616394\n",
      "epoch: 1 step: 34, loss is 0.6848236322402954\n",
      "epoch: 1 step: 35, loss is 0.6939420700073242\n",
      "epoch: 1 step: 36, loss is 0.6836326122283936\n",
      "epoch: 1 step: 37, loss is 0.6898232698440552\n",
      "epoch: 1 step: 38, loss is 0.6972866058349609\n",
      "epoch: 1 step: 39, loss is 0.6988848447799683\n",
      "epoch: 1 step: 40, loss is 0.6955322623252869\n",
      "epoch: 1 step: 41, loss is 0.689307689666748\n",
      "epoch: 1 step: 42, loss is 0.6999117732048035\n",
      "epoch: 1 step: 43, loss is 0.6911375522613525\n",
      "epoch: 1 step: 44, loss is 0.6979390382766724\n",
      "epoch: 1 step: 45, loss is 0.6892738938331604\n",
      "epoch: 1 step: 46, loss is 0.6918768286705017\n",
      "epoch: 1 step: 47, loss is 0.6940890550613403\n",
      "epoch: 1 step: 48, loss is 0.6980623006820679\n",
      "epoch: 1 step: 49, loss is 0.6960600018501282\n",
      "epoch: 1 step: 50, loss is 0.6915260553359985\n",
      "epoch: 1 step: 51, loss is 0.6877186298370361\n",
      "epoch: 1 step: 52, loss is 0.68989497423172\n",
      "epoch: 1 step: 53, loss is 0.6950157880783081\n",
      "epoch: 1 step: 54, loss is 0.6898171901702881\n",
      "epoch: 1 step: 55, loss is 0.6915262937545776\n",
      "epoch: 1 step: 56, loss is 0.6925351619720459\n",
      "epoch: 1 step: 57, loss is 0.6905806064605713\n",
      "epoch: 1 step: 58, loss is 0.6895667314529419\n",
      "epoch: 1 step: 59, loss is 0.6916400194168091\n",
      "epoch: 1 step: 60, loss is 0.6919517517089844\n",
      "epoch: 1 step: 61, loss is 0.6941339373588562\n",
      "epoch: 1 step: 62, loss is 0.692070484161377\n",
      "epoch: 1 step: 63, loss is 0.6909434795379639\n",
      "epoch: 1 step: 64, loss is 0.6883803606033325\n",
      "epoch: 1 step: 65, loss is 0.6914737224578857\n",
      "epoch: 1 step: 66, loss is 0.6906261444091797\n",
      "epoch: 1 step: 67, loss is 0.6929788589477539\n",
      "epoch: 1 step: 68, loss is 0.6922847628593445\n",
      "epoch: 1 step: 69, loss is 0.6864280700683594\n",
      "epoch: 1 step: 70, loss is 0.6912441849708557\n",
      "epoch: 1 step: 71, loss is 0.6936495304107666\n",
      "epoch: 1 step: 72, loss is 0.68696129322052\n",
      "epoch: 1 step: 73, loss is 0.6888349652290344\n",
      "epoch: 1 step: 74, loss is 0.6897921562194824\n",
      "epoch: 1 step: 75, loss is 0.6828707456588745\n",
      "epoch: 1 step: 76, loss is 0.684944748878479\n",
      "epoch: 1 step: 77, loss is 0.6918515563011169\n",
      "epoch: 1 step: 78, loss is 0.6880346536636353\n",
      "epoch: 1 step: 79, loss is 0.6817704439163208\n",
      "epoch: 1 step: 80, loss is 0.6891885995864868\n",
      "epoch: 1 step: 81, loss is 0.6774060130119324\n",
      "epoch: 1 step: 82, loss is 0.6807265877723694\n",
      "epoch: 1 step: 83, loss is 0.6787115335464478\n",
      "epoch: 1 step: 84, loss is 0.6807252168655396\n",
      "epoch: 1 step: 85, loss is 0.6688714027404785\n",
      "epoch: 1 step: 86, loss is 0.6842008829116821\n",
      "epoch: 1 step: 87, loss is 0.6795176267623901\n",
      "epoch: 1 step: 88, loss is 0.6665761470794678\n",
      "epoch: 1 step: 89, loss is 0.6653332114219666\n",
      "epoch: 1 step: 90, loss is 0.6741030216217041\n",
      "epoch: 1 step: 91, loss is 0.6643616557121277\n",
      "epoch: 1 step: 92, loss is 0.6809936761856079\n",
      "epoch: 1 step: 93, loss is 0.6715173721313477\n",
      "epoch: 1 step: 94, loss is 0.6953284740447998\n",
      "epoch: 1 step: 95, loss is 0.6642833948135376\n",
      "epoch: 1 step: 96, loss is 0.6659631729125977\n",
      "epoch: 1 step: 97, loss is 0.6677175760269165\n",
      "epoch: 1 step: 98, loss is 0.6653006076812744\n",
      "epoch: 1 step: 99, loss is 0.6415640711784363\n",
      "epoch: 1 step: 100, loss is 0.6498071551322937\n",
      "epoch: 1 step: 101, loss is 0.671424925327301\n",
      "epoch: 1 step: 102, loss is 0.6701800227165222\n",
      "epoch: 1 step: 103, loss is 0.6399107575416565\n",
      "epoch: 1 step: 104, loss is 0.6451573967933655\n",
      "epoch: 1 step: 105, loss is 0.6353158950805664\n",
      "epoch: 1 step: 106, loss is 0.6570260524749756\n",
      "epoch: 1 step: 107, loss is 0.6418440341949463\n",
      "epoch: 1 step: 108, loss is 0.6951228380203247\n",
      "epoch: 1 step: 109, loss is 0.6549656391143799\n",
      "epoch: 1 step: 110, loss is 0.6127814054489136\n",
      "epoch: 1 step: 111, loss is 0.6391742825508118\n",
      "epoch: 1 step: 112, loss is 0.6146265268325806\n",
      "epoch: 1 step: 113, loss is 0.601732075214386\n",
      "epoch: 1 step: 114, loss is 0.6222951412200928\n",
      "epoch: 1 step: 115, loss is 0.6681411862373352\n",
      "epoch: 1 step: 116, loss is 0.6064976453781128\n",
      "epoch: 1 step: 117, loss is 0.5661495923995972\n",
      "epoch: 1 step: 118, loss is 0.6065398454666138\n",
      "epoch: 1 step: 119, loss is 0.5426216721534729\n",
      "epoch: 1 step: 120, loss is 0.6451621055603027\n",
      "epoch: 1 step: 121, loss is 0.6172912120819092\n",
      "epoch: 1 step: 122, loss is 0.625531017780304\n",
      "epoch: 1 step: 123, loss is 0.549607515335083\n",
      "epoch: 1 step: 124, loss is 0.6170295476913452\n",
      "epoch: 1 step: 125, loss is 0.6220726370811462\n",
      "epoch: 1 step: 126, loss is 0.6140594482421875\n",
      "epoch: 1 step: 127, loss is 0.5786136984825134\n",
      "epoch: 1 step: 128, loss is 0.5487575531005859\n",
      "epoch: 1 step: 129, loss is 0.60126793384552\n",
      "epoch: 1 step: 130, loss is 0.5865514278411865\n",
      "epoch: 1 step: 131, loss is 0.5675814151763916\n",
      "epoch: 1 step: 132, loss is 0.5481787919998169\n",
      "epoch: 1 step: 133, loss is 0.5961882472038269\n",
      "epoch: 1 step: 134, loss is 0.523592472076416\n",
      "epoch: 1 step: 135, loss is 0.5727546811103821\n",
      "epoch: 1 step: 136, loss is 0.49719202518463135\n",
      "epoch: 1 step: 137, loss is 0.5991926193237305\n",
      "epoch: 1 step: 138, loss is 0.5461660623550415\n",
      "epoch: 1 step: 139, loss is 0.5964657068252563\n",
      "epoch: 1 step: 140, loss is 0.5557752847671509\n",
      "epoch: 1 step: 141, loss is 0.6064026951789856\n",
      "epoch: 1 step: 142, loss is 0.5104685425758362\n",
      "epoch: 1 step: 143, loss is 0.5998814702033997\n",
      "epoch: 1 step: 144, loss is 0.52911776304245\n",
      "epoch: 1 step: 145, loss is 0.5563150644302368\n",
      "epoch: 1 step: 146, loss is 0.6548969745635986\n",
      "epoch: 1 step: 147, loss is 0.5882201194763184\n",
      "epoch: 1 step: 148, loss is 0.5027062296867371\n",
      "epoch: 1 step: 149, loss is 0.5645673274993896\n",
      "epoch: 1 step: 150, loss is 0.5278425216674805\n",
      "epoch: 1 step: 151, loss is 0.4855687916278839\n",
      "epoch: 1 step: 152, loss is 0.536036491394043\n",
      "epoch: 1 step: 153, loss is 0.5672355890274048\n",
      "epoch: 1 step: 154, loss is 0.63633793592453\n",
      "epoch: 1 step: 155, loss is 0.43540751934051514\n",
      "epoch: 1 step: 156, loss is 0.5496456623077393\n",
      "epoch: 1 step: 157, loss is 0.4332565665245056\n",
      "epoch: 1 step: 158, loss is 0.5296585559844971\n",
      "epoch: 1 step: 159, loss is 0.4828348159790039\n",
      "epoch: 1 step: 160, loss is 0.4190504550933838\n",
      "epoch: 1 step: 161, loss is 0.4804030656814575\n",
      "epoch: 1 step: 162, loss is 0.5088366866111755\n",
      "epoch: 1 step: 163, loss is 0.5890721082687378\n",
      "epoch: 1 step: 164, loss is 0.5498055219650269\n",
      "epoch: 1 step: 165, loss is 0.5306302309036255\n",
      "epoch: 1 step: 166, loss is 0.4592953622341156\n",
      "epoch: 1 step: 167, loss is 0.4935252070426941\n",
      "epoch: 1 step: 168, loss is 0.43099814653396606\n",
      "epoch: 1 step: 169, loss is 0.4676738977432251\n",
      "epoch: 1 step: 170, loss is 0.48765140771865845\n",
      "epoch: 1 step: 171, loss is 0.5867079496383667\n",
      "epoch: 1 step: 172, loss is 0.5173599720001221\n",
      "epoch: 1 step: 173, loss is 0.5108099579811096\n",
      "epoch: 1 step: 174, loss is 0.4713502526283264\n",
      "epoch: 1 step: 175, loss is 0.3723815083503723\n",
      "epoch: 1 step: 176, loss is 0.5350828170776367\n",
      "epoch: 1 step: 177, loss is 0.5711379051208496\n",
      "epoch: 1 step: 178, loss is 0.5003001689910889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 179, loss is 0.5115837454795837\n",
      "epoch: 1 step: 180, loss is 0.5648525357246399\n",
      "epoch: 1 step: 181, loss is 0.4693623185157776\n",
      "epoch: 1 step: 182, loss is 0.4225767254829407\n",
      "epoch: 1 step: 183, loss is 0.4969969093799591\n",
      "epoch: 1 step: 184, loss is 0.40389835834503174\n",
      "epoch: 1 step: 185, loss is 0.3793712854385376\n",
      "epoch: 1 step: 186, loss is 0.48216065764427185\n",
      "epoch: 1 step: 187, loss is 0.5568665266036987\n",
      "epoch: 1 step: 188, loss is 0.5933369994163513\n",
      "epoch: 1 step: 189, loss is 0.4893414080142975\n",
      "epoch: 1 step: 190, loss is 0.4058995246887207\n",
      "epoch: 1 step: 191, loss is 0.5018218755722046\n",
      "epoch: 1 step: 192, loss is 0.42437395453453064\n",
      "epoch: 1 step: 193, loss is 0.5277137756347656\n",
      "epoch: 1 step: 194, loss is 0.4961119592189789\n",
      "epoch: 1 step: 195, loss is 0.4043952226638794\n",
      "epoch: 1 step: 196, loss is 0.44009166955947876\n",
      "epoch: 1 step: 197, loss is 0.3893536925315857\n",
      "epoch: 1 step: 198, loss is 0.5026965141296387\n",
      "epoch: 1 step: 199, loss is 0.5182700157165527\n",
      "epoch: 1 step: 200, loss is 0.5278711318969727\n",
      "epoch: 1 step: 201, loss is 0.5580512881278992\n",
      "epoch: 1 step: 202, loss is 0.4461210370063782\n",
      "epoch: 1 step: 203, loss is 0.40611475706100464\n",
      "epoch: 1 step: 204, loss is 0.42031237483024597\n",
      "epoch: 1 step: 205, loss is 0.41173356771469116\n",
      "epoch: 1 step: 206, loss is 0.40890616178512573\n",
      "epoch: 1 step: 207, loss is 0.43113723397254944\n",
      "epoch: 1 step: 208, loss is 0.4995355010032654\n",
      "epoch: 1 step: 209, loss is 0.4421371817588806\n",
      "epoch: 1 step: 210, loss is 0.4731474220752716\n",
      "epoch: 1 step: 211, loss is 0.4315781593322754\n",
      "epoch: 1 step: 212, loss is 0.48298701643943787\n",
      "epoch: 1 step: 213, loss is 0.37210214138031006\n",
      "epoch: 1 step: 214, loss is 0.4645851254463196\n",
      "epoch: 1 step: 215, loss is 0.5127520561218262\n",
      "epoch: 1 step: 216, loss is 0.41511547565460205\n",
      "epoch: 1 step: 217, loss is 0.4220927059650421\n",
      "epoch: 1 step: 218, loss is 0.4951469600200653\n",
      "epoch: 1 step: 219, loss is 0.37984099984169006\n",
      "epoch: 1 step: 220, loss is 0.3807736337184906\n",
      "epoch: 1 step: 221, loss is 0.4517112374305725\n",
      "epoch: 1 step: 222, loss is 0.42637065052986145\n",
      "epoch: 1 step: 223, loss is 0.4530118405818939\n",
      "epoch: 1 step: 224, loss is 0.38650041818618774\n",
      "epoch: 1 step: 225, loss is 0.2972918152809143\n",
      "epoch: 1 step: 226, loss is 0.4203939139842987\n",
      "epoch: 1 step: 227, loss is 0.44016802310943604\n",
      "epoch: 1 step: 228, loss is 0.44072452187538147\n",
      "epoch: 1 step: 229, loss is 0.49072518944740295\n",
      "epoch: 1 step: 230, loss is 0.40637683868408203\n",
      "epoch: 1 step: 231, loss is 0.37472498416900635\n",
      "epoch: 1 step: 232, loss is 0.3812454342842102\n",
      "epoch: 1 step: 233, loss is 0.43225419521331787\n",
      "epoch: 1 step: 234, loss is 0.3029554486274719\n",
      "epoch: 1 step: 235, loss is 0.34408724308013916\n",
      "epoch: 1 step: 236, loss is 0.36367267370224\n",
      "epoch: 1 step: 237, loss is 0.33225786685943604\n",
      "epoch: 1 step: 238, loss is 0.3553793728351593\n",
      "epoch: 1 step: 239, loss is 0.34750086069107056\n",
      "epoch: 1 step: 240, loss is 0.3685285449028015\n",
      "epoch: 1 step: 241, loss is 0.3556971848011017\n",
      "epoch: 1 step: 242, loss is 0.4002930521965027\n",
      "epoch: 1 step: 243, loss is 0.3908560872077942\n",
      "epoch: 1 step: 244, loss is 0.3055306077003479\n",
      "epoch: 1 step: 245, loss is 0.36030977964401245\n",
      "epoch: 1 step: 246, loss is 0.32607564330101013\n",
      "epoch: 1 step: 247, loss is 0.5133434534072876\n",
      "epoch: 1 step: 248, loss is 0.38605618476867676\n",
      "epoch: 1 step: 249, loss is 0.3289789855480194\n",
      "epoch: 1 step: 250, loss is 0.3809259533882141\n",
      "epoch: 1 step: 251, loss is 0.3297334909439087\n",
      "epoch: 1 step: 252, loss is 0.40872126817703247\n",
      "epoch: 1 step: 253, loss is 0.3868277668952942\n",
      "epoch: 1 step: 254, loss is 0.26721054315567017\n",
      "epoch: 1 step: 255, loss is 0.41678643226623535\n",
      "epoch: 1 step: 256, loss is 0.34299108386039734\n",
      "epoch: 1 step: 257, loss is 0.4919772148132324\n",
      "epoch: 1 step: 258, loss is 0.3665546476840973\n",
      "epoch: 1 step: 259, loss is 0.27579495310783386\n",
      "epoch: 1 step: 260, loss is 0.3783155381679535\n",
      "epoch: 1 step: 261, loss is 0.3741390109062195\n",
      "epoch: 1 step: 262, loss is 0.32027313113212585\n",
      "epoch: 1 step: 263, loss is 0.28466159105300903\n",
      "epoch: 1 step: 264, loss is 0.3254348039627075\n",
      "epoch: 1 step: 265, loss is 0.2748413681983948\n",
      "epoch: 1 step: 266, loss is 0.24060747027397156\n",
      "epoch: 1 step: 267, loss is 0.3820890188217163\n",
      "epoch: 1 step: 268, loss is 0.33951762318611145\n",
      "epoch: 1 step: 269, loss is 0.3472624123096466\n",
      "epoch: 1 step: 270, loss is 0.2859632968902588\n",
      "epoch: 1 step: 271, loss is 0.4303928017616272\n",
      "epoch: 1 step: 272, loss is 0.27207618951797485\n",
      "epoch: 1 step: 273, loss is 0.36527132987976074\n",
      "epoch: 1 step: 274, loss is 0.4305514693260193\n",
      "epoch: 1 step: 275, loss is 0.421710729598999\n",
      "epoch: 1 step: 276, loss is 0.31189820170402527\n",
      "epoch: 1 step: 277, loss is 0.2929932177066803\n",
      "epoch: 1 step: 278, loss is 0.33279338479042053\n",
      "epoch: 1 step: 279, loss is 0.2644374668598175\n",
      "epoch: 1 step: 280, loss is 0.37413620948791504\n",
      "epoch: 1 step: 281, loss is 0.2519071698188782\n",
      "epoch: 1 step: 282, loss is 0.3758898377418518\n",
      "epoch: 1 step: 283, loss is 0.2576667368412018\n",
      "epoch: 1 step: 284, loss is 0.3179480731487274\n",
      "epoch: 1 step: 285, loss is 0.21244457364082336\n",
      "epoch: 1 step: 286, loss is 0.2931814193725586\n",
      "epoch: 1 step: 287, loss is 0.41881418228149414\n",
      "epoch: 1 step: 288, loss is 0.3463042974472046\n",
      "epoch: 1 step: 289, loss is 0.276307612657547\n",
      "epoch: 1 step: 290, loss is 0.3377199172973633\n",
      "epoch: 1 step: 291, loss is 0.209449902176857\n",
      "epoch: 1 step: 292, loss is 0.3074181377887726\n",
      "epoch: 1 step: 293, loss is 0.29598623514175415\n",
      "epoch: 1 step: 294, loss is 0.3238489627838135\n",
      "epoch: 1 step: 295, loss is 0.35868194699287415\n",
      "epoch: 1 step: 296, loss is 0.2947966158390045\n",
      "epoch: 1 step: 297, loss is 0.3363306522369385\n",
      "epoch: 1 step: 298, loss is 0.27413201332092285\n",
      "epoch: 1 step: 299, loss is 0.3048757314682007\n",
      "epoch: 1 step: 300, loss is 0.29642945528030396\n",
      "epoch: 1 step: 301, loss is 0.2417527735233307\n",
      "epoch: 1 step: 302, loss is 0.297041118144989\n",
      "epoch: 1 step: 303, loss is 0.34241360425949097\n",
      "epoch: 1 step: 304, loss is 0.251030832529068\n",
      "epoch: 1 step: 305, loss is 0.3295823335647583\n",
      "epoch: 1 step: 306, loss is 0.14051727950572968\n",
      "epoch: 1 step: 307, loss is 0.31086021661758423\n",
      "epoch: 1 step: 308, loss is 0.26708412170410156\n",
      "epoch: 1 step: 309, loss is 0.1861475110054016\n",
      "epoch: 1 step: 310, loss is 0.23556169867515564\n",
      "epoch: 1 step: 311, loss is 0.2764112055301666\n",
      "epoch: 1 step: 312, loss is 0.4024140238761902\n",
      "epoch: 1 step: 313, loss is 0.3151934742927551\n",
      "epoch: 1 step: 314, loss is 0.25024306774139404\n",
      "epoch: 1 step: 315, loss is 0.262163370847702\n",
      "epoch: 1 step: 316, loss is 0.3346465826034546\n",
      "epoch: 1 step: 317, loss is 0.19920676946640015\n",
      "epoch: 1 step: 318, loss is 0.2702573537826538\n",
      "epoch: 1 step: 319, loss is 0.21879923343658447\n",
      "epoch: 1 step: 320, loss is 0.34403398633003235\n",
      "epoch: 1 step: 321, loss is 0.24236193299293518\n",
      "epoch: 1 step: 322, loss is 0.22695417702198029\n",
      "epoch: 1 step: 323, loss is 0.293623685836792\n",
      "epoch: 1 step: 324, loss is 0.16191346943378448\n",
      "epoch: 1 step: 325, loss is 0.38868194818496704\n",
      "epoch: 1 step: 326, loss is 0.43156009912490845\n",
      "epoch: 1 step: 327, loss is 0.2555670142173767\n",
      "epoch: 1 step: 328, loss is 0.3048120141029358\n",
      "epoch: 1 step: 329, loss is 0.32203975319862366\n",
      "epoch: 1 step: 330, loss is 0.24160955846309662\n",
      "epoch: 1 step: 331, loss is 0.2029503583908081\n",
      "epoch: 1 step: 332, loss is 0.2669512927532196\n",
      "epoch: 1 step: 333, loss is 0.17555883526802063\n",
      "epoch: 1 step: 334, loss is 0.1848931610584259\n",
      "epoch: 1 step: 335, loss is 0.25347864627838135\n",
      "epoch: 1 step: 336, loss is 0.29975253343582153\n",
      "epoch: 1 step: 337, loss is 0.27965861558914185\n",
      "epoch: 1 step: 338, loss is 0.29103273153305054\n",
      "epoch: 1 step: 339, loss is 0.18043720722198486\n",
      "epoch: 1 step: 340, loss is 0.29094943404197693\n",
      "epoch: 1 step: 341, loss is 0.21740028262138367\n",
      "epoch: 1 step: 342, loss is 0.2860734164714813\n",
      "epoch: 1 step: 343, loss is 0.26525166630744934\n",
      "epoch: 1 step: 344, loss is 0.1986967921257019\n",
      "epoch: 1 step: 345, loss is 0.17507809400558472\n",
      "epoch: 1 step: 346, loss is 0.23869147896766663\n",
      "epoch: 1 step: 347, loss is 0.3083711266517639\n",
      "epoch: 1 step: 348, loss is 0.2613397240638733\n",
      "epoch: 1 step: 349, loss is 0.29899168014526367\n",
      "epoch: 1 step: 350, loss is 0.3685736060142517\n",
      "epoch: 1 step: 351, loss is 0.32348740100860596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 352, loss is 0.19517117738723755\n",
      "epoch: 1 step: 353, loss is 0.22623595595359802\n",
      "epoch: 1 step: 354, loss is 0.24427101016044617\n",
      "epoch: 1 step: 355, loss is 0.25317585468292236\n",
      "epoch: 1 step: 356, loss is 0.19535784423351288\n",
      "epoch: 1 step: 357, loss is 0.2565336227416992\n",
      "epoch: 1 step: 358, loss is 0.21937289834022522\n",
      "epoch: 1 step: 359, loss is 0.35364192724227905\n",
      "epoch: 1 step: 360, loss is 0.25799888372421265\n",
      "epoch: 1 step: 361, loss is 0.2114674299955368\n",
      "epoch: 1 step: 362, loss is 0.20559744536876678\n",
      "epoch: 1 step: 363, loss is 0.33955225348472595\n",
      "epoch: 1 step: 364, loss is 0.34074538946151733\n",
      "epoch: 1 step: 365, loss is 0.23339179158210754\n",
      "epoch: 1 step: 366, loss is 0.2507880926132202\n",
      "epoch: 1 step: 367, loss is 0.28166836500167847\n",
      "epoch: 1 step: 368, loss is 0.3011850416660309\n",
      "epoch: 1 step: 369, loss is 0.15973493456840515\n",
      "epoch: 1 step: 370, loss is 0.22431302070617676\n",
      "epoch: 1 step: 371, loss is 0.3301381766796112\n",
      "epoch: 1 step: 372, loss is 0.2857823967933655\n",
      "epoch: 1 step: 373, loss is 0.2603324055671692\n",
      "epoch: 1 step: 374, loss is 0.12930017709732056\n",
      "epoch: 1 step: 375, loss is 0.1661963164806366\n",
      "epoch: 1 step: 376, loss is 0.2917655110359192\n",
      "epoch: 1 step: 377, loss is 0.17908889055252075\n",
      "epoch: 1 step: 378, loss is 0.3026973009109497\n",
      "epoch: 1 step: 379, loss is 0.2414400726556778\n",
      "epoch: 1 step: 380, loss is 0.2310972958803177\n",
      "epoch: 1 step: 381, loss is 0.19778555631637573\n",
      "epoch: 1 step: 382, loss is 0.26228123903274536\n",
      "epoch: 1 step: 383, loss is 0.14733824133872986\n",
      "epoch: 1 step: 384, loss is 0.17050406336784363\n",
      "epoch: 1 step: 385, loss is 0.17170344293117523\n",
      "epoch: 1 step: 386, loss is 0.16993102431297302\n",
      "epoch: 1 step: 387, loss is 0.22818328440189362\n",
      "epoch: 1 step: 388, loss is 0.23599296808242798\n",
      "epoch: 1 step: 389, loss is 0.16501620411872864\n",
      "epoch: 1 step: 390, loss is 0.27040719985961914\n",
      "epoch: 1 step: 391, loss is 0.24147039651870728\n",
      "epoch: 1 step: 392, loss is 0.16727161407470703\n",
      "epoch: 1 step: 393, loss is 0.1646120697259903\n",
      "epoch: 1 step: 394, loss is 0.1803162693977356\n",
      "epoch: 1 step: 395, loss is 0.16655448079109192\n",
      "epoch: 1 step: 396, loss is 0.2017231285572052\n",
      "epoch: 1 step: 397, loss is 0.2152710258960724\n",
      "epoch: 1 step: 398, loss is 0.16549713909626007\n",
      "epoch: 1 step: 399, loss is 0.1704963594675064\n",
      "epoch: 1 step: 400, loss is 0.17350152134895325\n",
      "epoch: 1 step: 401, loss is 0.2268638014793396\n",
      "epoch: 1 step: 402, loss is 0.1573238968849182\n",
      "epoch: 1 step: 403, loss is 0.19277259707450867\n",
      "epoch: 1 step: 404, loss is 0.28205984830856323\n",
      "epoch: 1 step: 405, loss is 0.20211470127105713\n",
      "epoch: 1 step: 406, loss is 0.2613097131252289\n",
      "epoch: 1 step: 407, loss is 0.21997345983982086\n",
      "epoch: 1 step: 408, loss is 0.156158447265625\n",
      "epoch: 1 step: 409, loss is 0.21842512488365173\n",
      "epoch: 1 step: 410, loss is 0.229879230260849\n",
      "epoch: 1 step: 411, loss is 0.14165592193603516\n",
      "epoch: 1 step: 412, loss is 0.14469897747039795\n",
      "epoch: 1 step: 413, loss is 0.11237538605928421\n",
      "epoch: 1 step: 414, loss is 0.1924254298210144\n",
      "epoch: 1 step: 415, loss is 0.12205079197883606\n",
      "epoch: 1 step: 416, loss is 0.2128370702266693\n",
      "epoch: 1 step: 417, loss is 0.2217976599931717\n",
      "epoch: 1 step: 418, loss is 0.20496997237205505\n",
      "epoch: 1 step: 419, loss is 0.1444905400276184\n",
      "epoch: 1 step: 420, loss is 0.26168325543403625\n",
      "epoch: 1 step: 421, loss is 0.14933907985687256\n",
      "epoch: 1 step: 422, loss is 0.29468610882759094\n",
      "epoch: 1 step: 423, loss is 0.1922362893819809\n",
      "epoch: 1 step: 424, loss is 0.18175949156284332\n",
      "epoch: 1 step: 425, loss is 0.23259201645851135\n",
      "epoch: 1 step: 426, loss is 0.16404202580451965\n",
      "epoch: 1 step: 427, loss is 0.11720606684684753\n",
      "epoch: 1 step: 428, loss is 0.14404070377349854\n",
      "epoch: 1 step: 429, loss is 0.17322924733161926\n",
      "epoch: 1 step: 430, loss is 0.11600366234779358\n",
      "epoch: 1 step: 431, loss is 0.1554778516292572\n",
      "epoch: 1 step: 432, loss is 0.12351833283901215\n",
      "epoch: 1 step: 433, loss is 0.16597220301628113\n",
      "epoch: 1 step: 434, loss is 0.07878896594047546\n",
      "epoch: 1 step: 435, loss is 0.1333971619606018\n",
      "epoch: 1 step: 436, loss is 0.2345781773328781\n",
      "epoch: 1 step: 437, loss is 0.11786285042762756\n",
      "epoch: 1 step: 438, loss is 0.1339147686958313\n",
      "epoch: 1 step: 439, loss is 0.20588776469230652\n",
      "epoch: 1 step: 440, loss is 0.13677722215652466\n",
      "epoch: 1 step: 441, loss is 0.1737964004278183\n",
      "epoch: 1 step: 442, loss is 0.13160452246665955\n",
      "epoch: 1 step: 443, loss is 0.17536002397537231\n",
      "epoch: 1 step: 444, loss is 0.17051035165786743\n",
      "epoch: 1 step: 445, loss is 0.1339452564716339\n",
      "epoch: 1 step: 446, loss is 0.16034860908985138\n",
      "epoch: 1 step: 447, loss is 0.13000762462615967\n",
      "epoch: 1 step: 448, loss is 0.15241912007331848\n",
      "epoch: 1 step: 449, loss is 0.22250057756900787\n",
      "epoch: 1 step: 450, loss is 0.14222046732902527\n",
      "epoch: 1 step: 451, loss is 0.09663552045822144\n",
      "epoch: 1 step: 452, loss is 0.21243643760681152\n",
      "epoch: 1 step: 453, loss is 0.08404231071472168\n",
      "epoch: 1 step: 454, loss is 0.15918654203414917\n",
      "epoch: 1 step: 455, loss is 0.04408804699778557\n",
      "epoch: 1 step: 456, loss is 0.13983604311943054\n",
      "epoch: 1 step: 457, loss is 0.09267192333936691\n",
      "epoch: 1 step: 458, loss is 0.06447415798902512\n",
      "epoch: 1 step: 459, loss is 0.09394785761833191\n",
      "epoch: 1 step: 460, loss is 0.09233465790748596\n",
      "epoch: 1 step: 461, loss is 0.13473454117774963\n",
      "epoch: 1 step: 462, loss is 0.145156592130661\n",
      "epoch: 1 step: 463, loss is 0.2094188630580902\n",
      "epoch: 1 step: 464, loss is 0.12694232165813446\n",
      "epoch: 1 step: 465, loss is 0.16014733910560608\n",
      "epoch: 1 step: 466, loss is 0.06212426722049713\n",
      "epoch: 1 step: 467, loss is 0.12734460830688477\n",
      "epoch: 1 step: 468, loss is 0.1402953565120697\n",
      "epoch: 1 step: 469, loss is 0.20957277715206146\n",
      "epoch: 1 step: 470, loss is 0.13891786336898804\n",
      "epoch: 1 step: 471, loss is 0.07788980007171631\n",
      "epoch: 1 step: 472, loss is 0.14533784985542297\n",
      "epoch: 1 step: 473, loss is 0.04508001729846001\n",
      "epoch: 1 step: 474, loss is 0.21111337840557098\n",
      "epoch: 1 step: 475, loss is 0.2283286154270172\n",
      "epoch: 1 step: 476, loss is 0.07552012801170349\n",
      "epoch: 1 step: 477, loss is 0.2036309689283371\n",
      "epoch: 1 step: 478, loss is 0.14228233695030212\n",
      "epoch: 1 step: 479, loss is 0.1320066750049591\n",
      "epoch: 1 step: 480, loss is 0.10394816845655441\n",
      "epoch: 1 step: 481, loss is 0.11829113960266113\n",
      "epoch: 1 step: 482, loss is 0.05424948036670685\n",
      "epoch: 1 step: 483, loss is 0.08337359130382538\n",
      "epoch: 1 step: 484, loss is 0.10040704905986786\n",
      "epoch: 1 step: 485, loss is 0.10627273470163345\n",
      "epoch: 1 step: 486, loss is 0.11356861144304276\n",
      "epoch: 1 step: 487, loss is 0.12013813108205795\n",
      "epoch: 1 step: 488, loss is 0.09298940747976303\n",
      "epoch: 1 step: 489, loss is 0.14210638403892517\n",
      "epoch: 1 step: 490, loss is 0.07872586697340012\n",
      "epoch: 1 step: 491, loss is 0.10986007004976273\n",
      "epoch: 1 step: 492, loss is 0.1268385946750641\n",
      "epoch: 1 step: 493, loss is 0.07583972066640854\n",
      "epoch: 1 step: 494, loss is 0.10179293900728226\n",
      "epoch: 1 step: 495, loss is 0.10340847074985504\n",
      "epoch: 1 step: 496, loss is 0.12320243567228317\n",
      "epoch: 1 step: 497, loss is 0.10844798386096954\n",
      "epoch: 1 step: 498, loss is 0.12997710704803467\n",
      "epoch: 1 step: 499, loss is 0.19756025075912476\n",
      "epoch: 1 step: 500, loss is 0.14275896549224854\n",
      "epoch: 1 step: 501, loss is 0.10221871733665466\n",
      "epoch: 1 step: 502, loss is 0.12054320424795151\n",
      "epoch: 1 step: 503, loss is 0.2065223604440689\n",
      "epoch: 1 step: 504, loss is 0.13599132001399994\n",
      "epoch: 1 step: 505, loss is 0.09889308363199234\n",
      "epoch: 1 step: 506, loss is 0.14884710311889648\n",
      "epoch: 1 step: 507, loss is 0.06126756593585014\n",
      "epoch: 1 step: 508, loss is 0.19386878609657288\n",
      "epoch: 1 step: 509, loss is 0.14992186427116394\n",
      "epoch: 1 step: 510, loss is 0.11527910828590393\n",
      "epoch: 1 step: 511, loss is 0.09235647320747375\n",
      "epoch: 1 step: 512, loss is 0.21439000964164734\n",
      "epoch: 1 step: 513, loss is 0.1940019428730011\n",
      "epoch: 1 step: 514, loss is 0.14677979052066803\n",
      "epoch: 1 step: 515, loss is 0.1902482807636261\n",
      "epoch: 1 step: 516, loss is 0.1369786262512207\n",
      "epoch: 1 step: 517, loss is 0.19680196046829224\n",
      "epoch: 1 step: 518, loss is 0.052489981055259705\n",
      "epoch: 1 step: 519, loss is 0.07700614631175995\n",
      "epoch: 1 step: 520, loss is 0.21152791380882263\n",
      "epoch: 1 step: 521, loss is 0.1058642566204071\n",
      "epoch: 1 step: 522, loss is 0.0700644850730896\n",
      "epoch: 1 step: 523, loss is 0.03655849024653435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 524, loss is 0.0561862550675869\n",
      "epoch: 1 step: 525, loss is 0.16027170419692993\n",
      "epoch: 1 step: 526, loss is 0.0683010071516037\n",
      "epoch: 1 step: 527, loss is 0.1729481816291809\n",
      "epoch: 1 step: 528, loss is 0.10944504290819168\n",
      "epoch: 1 step: 529, loss is 0.19449129700660706\n",
      "epoch: 1 step: 530, loss is 0.09506841003894806\n",
      "epoch: 1 step: 531, loss is 0.1252337545156479\n",
      "epoch: 1 step: 532, loss is 0.04992973804473877\n",
      "epoch: 1 step: 533, loss is 0.05496437847614288\n",
      "epoch: 1 step: 534, loss is 0.0705384910106659\n",
      "epoch: 1 step: 535, loss is 0.10230949521064758\n",
      "epoch: 1 step: 536, loss is 0.1396767646074295\n",
      "epoch: 1 step: 537, loss is 0.1417720764875412\n",
      "epoch: 1 step: 538, loss is 0.06444476544857025\n",
      "epoch: 1 step: 539, loss is 0.0993741899728775\n",
      "epoch: 1 step: 540, loss is 0.09792280197143555\n",
      "epoch: 1 step: 541, loss is 0.07745826989412308\n",
      "epoch: 1 step: 542, loss is 0.054382458329200745\n",
      "epoch: 1 step: 543, loss is 0.07800719141960144\n",
      "epoch: 1 step: 544, loss is 0.13376069068908691\n",
      "epoch: 1 step: 545, loss is 0.08133157342672348\n",
      "epoch: 1 step: 546, loss is 0.09812753647565842\n",
      "epoch: 1 step: 547, loss is 0.07845653593540192\n",
      "epoch: 1 step: 548, loss is 0.08790796995162964\n",
      "epoch: 1 step: 549, loss is 0.09500031173229218\n",
      "epoch: 1 step: 550, loss is 0.0799671858549118\n",
      "epoch: 1 step: 551, loss is 0.06050030514597893\n",
      "epoch: 1 step: 552, loss is 0.07221556454896927\n",
      "epoch: 1 step: 553, loss is 0.1724812090396881\n",
      "epoch: 1 step: 554, loss is 0.0703166201710701\n",
      "epoch: 1 step: 555, loss is 0.11876367032527924\n",
      "epoch: 1 step: 556, loss is 0.12403145432472229\n",
      "epoch: 1 step: 557, loss is 0.15219128131866455\n",
      "epoch: 1 step: 558, loss is 0.07136797904968262\n",
      "epoch: 1 step: 559, loss is 0.08391667157411575\n",
      "epoch: 1 step: 560, loss is 0.053329870104789734\n",
      "epoch: 1 step: 561, loss is 0.07898524403572083\n",
      "epoch: 1 step: 562, loss is 0.04651298746466637\n",
      "epoch: 1 step: 563, loss is 0.15057186782360077\n",
      "epoch: 1 step: 564, loss is 0.0545419417321682\n",
      "epoch: 1 step: 565, loss is 0.07065069675445557\n",
      "epoch: 1 step: 566, loss is 0.06825578957796097\n",
      "epoch: 1 step: 567, loss is 0.07975411415100098\n",
      "epoch: 1 step: 568, loss is 0.0597572848200798\n",
      "epoch: 1 step: 569, loss is 0.16489887237548828\n",
      "epoch: 1 step: 570, loss is 0.06316854059696198\n",
      "epoch: 1 step: 571, loss is 0.1783909946680069\n",
      "epoch: 1 step: 572, loss is 0.15461492538452148\n",
      "epoch: 1 step: 573, loss is 0.117494598031044\n",
      "epoch: 1 step: 574, loss is 0.1935061663389206\n",
      "epoch: 1 step: 575, loss is 0.08523523807525635\n",
      "epoch: 1 step: 576, loss is 0.03425116091966629\n",
      "epoch: 1 step: 577, loss is 0.16907042264938354\n",
      "epoch: 1 step: 578, loss is 0.10818573832511902\n",
      "epoch: 1 step: 579, loss is 0.03765092045068741\n",
      "epoch: 1 step: 580, loss is 0.062465306371450424\n",
      "epoch: 1 step: 581, loss is 0.057281482964754105\n",
      "epoch: 1 step: 582, loss is 0.050745315849781036\n",
      "epoch: 1 step: 583, loss is 0.02876329980790615\n",
      "epoch: 1 step: 584, loss is 0.058383915573358536\n",
      "epoch: 1 step: 585, loss is 0.16720537841320038\n",
      "epoch: 1 step: 586, loss is 0.05518466234207153\n",
      "epoch: 1 step: 587, loss is 0.06773031502962112\n",
      "epoch: 1 step: 588, loss is 0.10715286433696747\n",
      "epoch: 1 step: 589, loss is 0.10047618299722672\n",
      "epoch: 1 step: 590, loss is 0.054860882461071014\n",
      "epoch: 1 step: 591, loss is 0.04569078981876373\n",
      "epoch: 1 step: 592, loss is 0.09052211046218872\n",
      "epoch: 1 step: 593, loss is 0.09565534442663193\n",
      "epoch: 1 step: 594, loss is 0.047728948295116425\n",
      "epoch: 1 step: 595, loss is 0.050010085105895996\n",
      "epoch: 1 step: 596, loss is 0.05859490856528282\n",
      "Train epoch time: 109699.559 ms, per step time: 184.060 ms\n",
      "epoch: 2 step: 1, loss is 0.04946703463792801\n",
      "epoch: 2 step: 2, loss is 0.128347247838974\n",
      "epoch: 2 step: 3, loss is 0.05031895264983177\n",
      "epoch: 2 step: 4, loss is 0.03953104838728905\n",
      "epoch: 2 step: 5, loss is 0.08448787778615952\n",
      "epoch: 2 step: 6, loss is 0.07065156102180481\n",
      "epoch: 2 step: 7, loss is 0.049309566617012024\n",
      "epoch: 2 step: 8, loss is 0.02524612843990326\n",
      "epoch: 2 step: 9, loss is 0.04563279449939728\n",
      "epoch: 2 step: 10, loss is 0.0356040820479393\n",
      "epoch: 2 step: 11, loss is 0.04218791052699089\n",
      "epoch: 2 step: 12, loss is 0.03922994062304497\n",
      "epoch: 2 step: 13, loss is 0.049981776624917984\n",
      "epoch: 2 step: 14, loss is 0.05051238834857941\n",
      "epoch: 2 step: 15, loss is 0.09237220883369446\n",
      "epoch: 2 step: 16, loss is 0.07614462077617645\n",
      "epoch: 2 step: 17, loss is 0.026213960722088814\n",
      "epoch: 2 step: 18, loss is 0.031123364344239235\n",
      "epoch: 2 step: 19, loss is 0.03414541110396385\n",
      "epoch: 2 step: 20, loss is 0.04734455421566963\n",
      "epoch: 2 step: 21, loss is 0.07814275473356247\n",
      "epoch: 2 step: 22, loss is 0.09933345019817352\n",
      "epoch: 2 step: 23, loss is 0.08123588562011719\n",
      "epoch: 2 step: 24, loss is 0.031633708626031876\n",
      "epoch: 2 step: 25, loss is 0.07515741139650345\n",
      "epoch: 2 step: 26, loss is 0.027562078088521957\n",
      "epoch: 2 step: 27, loss is 0.08266361057758331\n",
      "epoch: 2 step: 28, loss is 0.16816692054271698\n",
      "epoch: 2 step: 29, loss is 0.037282638251781464\n",
      "epoch: 2 step: 30, loss is 0.05358525738120079\n",
      "epoch: 2 step: 31, loss is 0.062184713780879974\n",
      "epoch: 2 step: 32, loss is 0.0627906322479248\n",
      "epoch: 2 step: 33, loss is 0.06766961514949799\n",
      "epoch: 2 step: 34, loss is 0.034623946994543076\n",
      "epoch: 2 step: 35, loss is 0.019877834245562553\n",
      "epoch: 2 step: 36, loss is 0.017792437225580215\n",
      "epoch: 2 step: 37, loss is 0.0464509055018425\n",
      "epoch: 2 step: 38, loss is 0.030830487608909607\n",
      "epoch: 2 step: 39, loss is 0.0534067377448082\n",
      "epoch: 2 step: 40, loss is 0.04782719165086746\n",
      "epoch: 2 step: 41, loss is 0.023053482174873352\n",
      "epoch: 2 step: 42, loss is 0.03182258829474449\n",
      "epoch: 2 step: 43, loss is 0.03813979774713516\n",
      "epoch: 2 step: 44, loss is 0.042850036174058914\n",
      "epoch: 2 step: 45, loss is 0.055362518876791\n",
      "epoch: 2 step: 46, loss is 0.02223467081785202\n",
      "epoch: 2 step: 47, loss is 0.02925139106810093\n",
      "epoch: 2 step: 48, loss is 0.08429650962352753\n",
      "epoch: 2 step: 49, loss is 0.04663383960723877\n",
      "epoch: 2 step: 50, loss is 0.03342091664671898\n",
      "epoch: 2 step: 51, loss is 0.026078373193740845\n",
      "epoch: 2 step: 52, loss is 0.08661045134067535\n",
      "epoch: 2 step: 53, loss is 0.060166072100400925\n",
      "epoch: 2 step: 54, loss is 0.04580352082848549\n",
      "epoch: 2 step: 55, loss is 0.045226141810417175\n",
      "epoch: 2 step: 56, loss is 0.05785369127988815\n",
      "epoch: 2 step: 57, loss is 0.04788539558649063\n",
      "epoch: 2 step: 58, loss is 0.030524548143148422\n",
      "epoch: 2 step: 59, loss is 0.020472466945648193\n",
      "epoch: 2 step: 60, loss is 0.03397154062986374\n",
      "epoch: 2 step: 61, loss is 0.07706507295370102\n",
      "epoch: 2 step: 62, loss is 0.05209347605705261\n",
      "epoch: 2 step: 63, loss is 0.03992433100938797\n",
      "epoch: 2 step: 64, loss is 0.07051306962966919\n",
      "epoch: 2 step: 65, loss is 0.11444614082574844\n",
      "epoch: 2 step: 66, loss is 0.09247095137834549\n",
      "epoch: 2 step: 67, loss is 0.040899962186813354\n",
      "epoch: 2 step: 68, loss is 0.20323069393634796\n",
      "epoch: 2 step: 69, loss is 0.045327022671699524\n",
      "epoch: 2 step: 70, loss is 0.1029013991355896\n",
      "epoch: 2 step: 71, loss is 0.014689559116959572\n",
      "epoch: 2 step: 72, loss is 0.030360369011759758\n",
      "epoch: 2 step: 73, loss is 0.10265284776687622\n",
      "epoch: 2 step: 74, loss is 0.047607626765966415\n",
      "epoch: 2 step: 75, loss is 0.06647078692913055\n",
      "epoch: 2 step: 76, loss is 0.010874833911657333\n",
      "epoch: 2 step: 77, loss is 0.017801504582166672\n",
      "epoch: 2 step: 78, loss is 0.09919100999832153\n",
      "epoch: 2 step: 79, loss is 0.06213534623384476\n",
      "epoch: 2 step: 80, loss is 0.14260578155517578\n",
      "epoch: 2 step: 81, loss is 0.05929140746593475\n",
      "epoch: 2 step: 82, loss is 0.05247171223163605\n",
      "epoch: 2 step: 83, loss is 0.0178765207529068\n",
      "epoch: 2 step: 84, loss is 0.04569064825773239\n",
      "epoch: 2 step: 85, loss is 0.019548188894987106\n",
      "epoch: 2 step: 86, loss is 0.03815747797489166\n",
      "epoch: 2 step: 87, loss is 0.029871055856347084\n",
      "epoch: 2 step: 88, loss is 0.03291962295770645\n",
      "epoch: 2 step: 89, loss is 0.06697090715169907\n",
      "epoch: 2 step: 90, loss is 0.05332912877202034\n",
      "epoch: 2 step: 91, loss is 0.0327470600605011\n",
      "epoch: 2 step: 92, loss is 0.09246297180652618\n",
      "epoch: 2 step: 93, loss is 0.030522676184773445\n",
      "epoch: 2 step: 94, loss is 0.03329624608159065\n",
      "epoch: 2 step: 95, loss is 0.023288898169994354\n",
      "epoch: 2 step: 96, loss is 0.04699999839067459\n",
      "epoch: 2 step: 97, loss is 0.04631517082452774\n",
      "epoch: 2 step: 98, loss is 0.01699231192469597\n",
      "epoch: 2 step: 99, loss is 0.05274844169616699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 100, loss is 0.025838086381554604\n",
      "epoch: 2 step: 101, loss is 0.024708829820156097\n",
      "epoch: 2 step: 102, loss is 0.021644342690706253\n",
      "epoch: 2 step: 103, loss is 0.02983747236430645\n",
      "epoch: 2 step: 104, loss is 0.0430714450776577\n",
      "epoch: 2 step: 105, loss is 0.04728925973176956\n",
      "epoch: 2 step: 106, loss is 0.11792132258415222\n",
      "epoch: 2 step: 107, loss is 0.04197564721107483\n",
      "epoch: 2 step: 108, loss is 0.08654165267944336\n",
      "epoch: 2 step: 109, loss is 0.03271026909351349\n",
      "epoch: 2 step: 110, loss is 0.09239818900823593\n",
      "epoch: 2 step: 111, loss is 0.03798660635948181\n",
      "epoch: 2 step: 112, loss is 0.014649596996605396\n",
      "epoch: 2 step: 113, loss is 0.010762056335806847\n",
      "epoch: 2 step: 114, loss is 0.04231097549200058\n",
      "epoch: 2 step: 115, loss is 0.022679384797811508\n",
      "epoch: 2 step: 116, loss is 0.02379770018160343\n",
      "epoch: 2 step: 117, loss is 0.034982699900865555\n",
      "epoch: 2 step: 118, loss is 0.03810197487473488\n",
      "epoch: 2 step: 119, loss is 0.038688816130161285\n",
      "epoch: 2 step: 120, loss is 0.03155834227800369\n",
      "epoch: 2 step: 121, loss is 0.03396705538034439\n",
      "epoch: 2 step: 122, loss is 0.09453433752059937\n",
      "epoch: 2 step: 123, loss is 0.0360383465886116\n",
      "epoch: 2 step: 124, loss is 0.13907331228256226\n",
      "epoch: 2 step: 125, loss is 0.10430075228214264\n",
      "epoch: 2 step: 126, loss is 0.049911804497241974\n",
      "epoch: 2 step: 127, loss is 0.1209578663110733\n",
      "epoch: 2 step: 128, loss is 0.07047721743583679\n",
      "epoch: 2 step: 129, loss is 0.02140234224498272\n",
      "epoch: 2 step: 130, loss is 0.11261703819036484\n",
      "epoch: 2 step: 131, loss is 0.0337345115840435\n",
      "epoch: 2 step: 132, loss is 0.01829524338245392\n",
      "epoch: 2 step: 133, loss is 0.03392331302165985\n",
      "epoch: 2 step: 134, loss is 0.05869561433792114\n",
      "epoch: 2 step: 135, loss is 0.030660443007946014\n",
      "epoch: 2 step: 136, loss is 0.0063736168667674065\n",
      "epoch: 2 step: 137, loss is 0.033294059336185455\n",
      "epoch: 2 step: 138, loss is 0.10432885587215424\n",
      "epoch: 2 step: 139, loss is 0.04155996814370155\n",
      "epoch: 2 step: 140, loss is 0.04642300680279732\n",
      "epoch: 2 step: 141, loss is 0.038990847766399384\n",
      "epoch: 2 step: 142, loss is 0.0782909020781517\n",
      "epoch: 2 step: 143, loss is 0.04407888278365135\n",
      "epoch: 2 step: 144, loss is 0.03393259644508362\n",
      "epoch: 2 step: 145, loss is 0.07424242794513702\n",
      "epoch: 2 step: 146, loss is 0.05089215189218521\n",
      "epoch: 2 step: 147, loss is 0.02291985973715782\n",
      "epoch: 2 step: 148, loss is 0.01598774641752243\n",
      "epoch: 2 step: 149, loss is 0.018626639619469643\n",
      "epoch: 2 step: 150, loss is 0.028481513261795044\n",
      "epoch: 2 step: 151, loss is 0.01602037437260151\n",
      "epoch: 2 step: 152, loss is 0.008261745795607567\n",
      "epoch: 2 step: 153, loss is 0.01696297526359558\n",
      "epoch: 2 step: 154, loss is 0.02183976210653782\n",
      "epoch: 2 step: 155, loss is 0.03219921141862869\n",
      "epoch: 2 step: 156, loss is 0.017938923090696335\n",
      "epoch: 2 step: 157, loss is 0.011334546841681004\n",
      "epoch: 2 step: 158, loss is 0.03140765428543091\n",
      "epoch: 2 step: 159, loss is 0.01522547472268343\n",
      "epoch: 2 step: 160, loss is 0.011496585793793201\n",
      "epoch: 2 step: 161, loss is 0.010485697537660599\n",
      "epoch: 2 step: 162, loss is 0.01569613814353943\n",
      "epoch: 2 step: 163, loss is 0.01674300618469715\n",
      "epoch: 2 step: 164, loss is 0.0526961088180542\n",
      "epoch: 2 step: 165, loss is 0.04421373829245567\n",
      "epoch: 2 step: 166, loss is 0.03571048378944397\n",
      "epoch: 2 step: 167, loss is 0.018904192373156548\n",
      "epoch: 2 step: 168, loss is 0.009477091953158379\n",
      "epoch: 2 step: 169, loss is 0.011889005079865456\n",
      "epoch: 2 step: 170, loss is 0.06918594241142273\n",
      "epoch: 2 step: 171, loss is 0.021533913910388947\n",
      "epoch: 2 step: 172, loss is 0.054686084389686584\n",
      "epoch: 2 step: 173, loss is 0.01804247498512268\n",
      "epoch: 2 step: 174, loss is 0.027909591794013977\n",
      "epoch: 2 step: 175, loss is 0.0034294957295060158\n",
      "epoch: 2 step: 176, loss is 0.023642491549253464\n",
      "epoch: 2 step: 177, loss is 0.10410614311695099\n",
      "epoch: 2 step: 178, loss is 0.016081970185041428\n",
      "epoch: 2 step: 179, loss is 0.02237997017800808\n",
      "epoch: 2 step: 180, loss is 0.026132896542549133\n",
      "epoch: 2 step: 181, loss is 0.02756371907889843\n",
      "epoch: 2 step: 182, loss is 0.035331547260284424\n",
      "epoch: 2 step: 183, loss is 0.016727685928344727\n",
      "epoch: 2 step: 184, loss is 0.010790076106786728\n",
      "epoch: 2 step: 185, loss is 0.01166457124054432\n",
      "epoch: 2 step: 186, loss is 0.020508715882897377\n",
      "epoch: 2 step: 187, loss is 0.011396927759051323\n",
      "epoch: 2 step: 188, loss is 0.024284742772579193\n",
      "epoch: 2 step: 189, loss is 0.027240559458732605\n",
      "epoch: 2 step: 190, loss is 0.020260466262698174\n",
      "epoch: 2 step: 191, loss is 0.01687677763402462\n",
      "epoch: 2 step: 192, loss is 0.0227586030960083\n",
      "epoch: 2 step: 193, loss is 0.02621154487133026\n",
      "epoch: 2 step: 194, loss is 0.02565053664147854\n",
      "epoch: 2 step: 195, loss is 0.006155212409794331\n",
      "epoch: 2 step: 196, loss is 0.022212591022253036\n",
      "epoch: 2 step: 197, loss is 0.036337900906801224\n",
      "epoch: 2 step: 198, loss is 0.013982952572405338\n",
      "epoch: 2 step: 199, loss is 0.010466784238815308\n",
      "epoch: 2 step: 200, loss is 0.028084108605980873\n",
      "epoch: 2 step: 201, loss is 0.03888449817895889\n",
      "epoch: 2 step: 202, loss is 0.019497215747833252\n",
      "epoch: 2 step: 203, loss is 0.018497895449399948\n",
      "epoch: 2 step: 204, loss is 0.02352745458483696\n",
      "epoch: 2 step: 205, loss is 0.026113398373126984\n",
      "epoch: 2 step: 206, loss is 0.006983675993978977\n",
      "epoch: 2 step: 207, loss is 0.010899107903242111\n",
      "epoch: 2 step: 208, loss is 0.021404443308711052\n",
      "epoch: 2 step: 209, loss is 0.018786022439599037\n",
      "epoch: 2 step: 210, loss is 0.04589461907744408\n",
      "epoch: 2 step: 211, loss is 0.018648970872163773\n",
      "epoch: 2 step: 212, loss is 0.011921674944460392\n",
      "epoch: 2 step: 213, loss is 0.02908773347735405\n",
      "epoch: 2 step: 214, loss is 0.023225223645567894\n",
      "epoch: 2 step: 215, loss is 0.016597993671894073\n",
      "epoch: 2 step: 216, loss is 0.015378963202238083\n",
      "epoch: 2 step: 217, loss is 0.1074502095580101\n",
      "epoch: 2 step: 218, loss is 0.011596230790019035\n",
      "epoch: 2 step: 219, loss is 0.04884089156985283\n",
      "epoch: 2 step: 220, loss is 0.005571182817220688\n",
      "epoch: 2 step: 221, loss is 0.028059441596269608\n",
      "epoch: 2 step: 222, loss is 0.07354892045259476\n",
      "epoch: 2 step: 223, loss is 0.029684744775295258\n",
      "epoch: 2 step: 224, loss is 0.019294114783406258\n",
      "epoch: 2 step: 225, loss is 0.005327763967216015\n",
      "epoch: 2 step: 226, loss is 0.006379229016602039\n",
      "epoch: 2 step: 227, loss is 0.019005976617336273\n",
      "epoch: 2 step: 228, loss is 0.024587756022810936\n",
      "epoch: 2 step: 229, loss is 0.06199053302407265\n",
      "epoch: 2 step: 230, loss is 0.017227662727236748\n",
      "epoch: 2 step: 231, loss is 0.04752334579825401\n",
      "epoch: 2 step: 232, loss is 0.011270079761743546\n",
      "epoch: 2 step: 233, loss is 0.019028468057513237\n",
      "epoch: 2 step: 234, loss is 0.00913272425532341\n",
      "epoch: 2 step: 235, loss is 0.023104529827833176\n",
      "epoch: 2 step: 236, loss is 0.00786552019417286\n",
      "epoch: 2 step: 237, loss is 0.0211680606007576\n",
      "epoch: 2 step: 238, loss is 0.009444222785532475\n",
      "epoch: 2 step: 239, loss is 0.013877686113119125\n",
      "epoch: 2 step: 240, loss is 0.024185340851545334\n",
      "epoch: 2 step: 241, loss is 0.028602585196495056\n",
      "epoch: 2 step: 242, loss is 0.011800097301602364\n",
      "epoch: 2 step: 243, loss is 0.008566117845475674\n",
      "epoch: 2 step: 244, loss is 0.0073907081969082355\n",
      "epoch: 2 step: 245, loss is 0.014831545762717724\n",
      "epoch: 2 step: 246, loss is 0.03379282355308533\n",
      "epoch: 2 step: 247, loss is 0.01852996088564396\n",
      "epoch: 2 step: 248, loss is 0.01278261560946703\n",
      "epoch: 2 step: 249, loss is 0.007049785926938057\n",
      "epoch: 2 step: 250, loss is 0.01818038336932659\n",
      "epoch: 2 step: 251, loss is 0.01635640487074852\n",
      "epoch: 2 step: 252, loss is 0.027343418449163437\n",
      "epoch: 2 step: 253, loss is 0.016878139227628708\n",
      "epoch: 2 step: 254, loss is 0.012247076258063316\n",
      "epoch: 2 step: 255, loss is 0.031762752681970596\n",
      "epoch: 2 step: 256, loss is 0.00853907410055399\n",
      "epoch: 2 step: 257, loss is 0.04857613146305084\n",
      "epoch: 2 step: 258, loss is 0.018829215317964554\n",
      "epoch: 2 step: 259, loss is 0.03730645775794983\n",
      "epoch: 2 step: 260, loss is 0.011751321144402027\n",
      "epoch: 2 step: 261, loss is 0.021318253129720688\n",
      "epoch: 2 step: 262, loss is 0.0033993013203144073\n",
      "epoch: 2 step: 263, loss is 0.009065546095371246\n",
      "epoch: 2 step: 264, loss is 0.018208805471658707\n",
      "epoch: 2 step: 265, loss is 0.022961800917983055\n",
      "epoch: 2 step: 266, loss is 0.015199149027466774\n",
      "epoch: 2 step: 267, loss is 0.007988421246409416\n",
      "epoch: 2 step: 268, loss is 0.021188970655202866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 269, loss is 0.020271629095077515\n",
      "epoch: 2 step: 270, loss is 0.015585759654641151\n",
      "epoch: 2 step: 271, loss is 0.07314393669366837\n",
      "epoch: 2 step: 272, loss is 0.017709434032440186\n",
      "epoch: 2 step: 273, loss is 0.10437235236167908\n",
      "epoch: 2 step: 274, loss is 0.05109598487615585\n",
      "epoch: 2 step: 275, loss is 0.04768062382936478\n",
      "epoch: 2 step: 276, loss is 0.09846533834934235\n",
      "epoch: 2 step: 277, loss is 0.042379409074783325\n",
      "epoch: 2 step: 278, loss is 0.005368281155824661\n",
      "epoch: 2 step: 279, loss is 0.076487697660923\n",
      "epoch: 2 step: 280, loss is 0.010182775557041168\n",
      "epoch: 2 step: 281, loss is 0.020324358716607094\n",
      "epoch: 2 step: 282, loss is 0.010813113301992416\n",
      "epoch: 2 step: 283, loss is 0.020205648615956306\n",
      "epoch: 2 step: 284, loss is 0.010017288848757744\n",
      "epoch: 2 step: 285, loss is 0.005993732716888189\n",
      "epoch: 2 step: 286, loss is 0.00868483167141676\n",
      "epoch: 2 step: 287, loss is 0.011023540049791336\n",
      "epoch: 2 step: 288, loss is 0.004441044759005308\n",
      "epoch: 2 step: 289, loss is 0.010439259931445122\n",
      "epoch: 2 step: 290, loss is 0.00995531678199768\n",
      "epoch: 2 step: 291, loss is 0.05352598428726196\n",
      "epoch: 2 step: 292, loss is 0.02120811864733696\n",
      "epoch: 2 step: 293, loss is 0.027342040091753006\n",
      "epoch: 2 step: 294, loss is 0.03856849670410156\n",
      "epoch: 2 step: 295, loss is 0.022311603650450706\n",
      "epoch: 2 step: 296, loss is 0.020045997574925423\n",
      "epoch: 2 step: 297, loss is 0.025514446198940277\n",
      "epoch: 2 step: 298, loss is 0.03126819059252739\n",
      "epoch: 2 step: 299, loss is 0.025614891201257706\n",
      "epoch: 2 step: 300, loss is 0.021747877821326256\n",
      "epoch: 2 step: 301, loss is 0.010195400565862656\n",
      "epoch: 2 step: 302, loss is 0.009716945700347424\n",
      "epoch: 2 step: 303, loss is 0.01968158408999443\n",
      "epoch: 2 step: 304, loss is 0.010047893971204758\n",
      "epoch: 2 step: 305, loss is 0.0069811465218663216\n",
      "epoch: 2 step: 306, loss is 0.022753285244107246\n",
      "epoch: 2 step: 307, loss is 0.014115916565060616\n",
      "epoch: 2 step: 308, loss is 0.00516049237921834\n",
      "epoch: 2 step: 309, loss is 0.005678502842783928\n",
      "epoch: 2 step: 310, loss is 0.011137362569570541\n",
      "epoch: 2 step: 311, loss is 0.01363154873251915\n",
      "epoch: 2 step: 312, loss is 0.01088538859039545\n",
      "epoch: 2 step: 313, loss is 0.03232530131936073\n",
      "epoch: 2 step: 314, loss is 0.03031441755592823\n",
      "epoch: 2 step: 315, loss is 0.008154187351465225\n",
      "epoch: 2 step: 316, loss is 0.005192058626562357\n",
      "epoch: 2 step: 317, loss is 0.002657998353242874\n",
      "epoch: 2 step: 318, loss is 0.005692107602953911\n",
      "epoch: 2 step: 319, loss is 0.032952580600976944\n",
      "epoch: 2 step: 320, loss is 0.015016429126262665\n",
      "epoch: 2 step: 321, loss is 0.10668856650590897\n",
      "epoch: 2 step: 322, loss is 0.004521641414612532\n",
      "epoch: 2 step: 323, loss is 0.0079437755048275\n",
      "epoch: 2 step: 324, loss is 0.006739899981766939\n",
      "epoch: 2 step: 325, loss is 0.02752811647951603\n",
      "epoch: 2 step: 326, loss is 0.025322943925857544\n",
      "epoch: 2 step: 327, loss is 0.012776665389537811\n",
      "epoch: 2 step: 328, loss is 0.014711291529238224\n",
      "epoch: 2 step: 329, loss is 0.007039944641292095\n",
      "epoch: 2 step: 330, loss is 0.009850211441516876\n",
      "epoch: 2 step: 331, loss is 0.016543341800570488\n",
      "epoch: 2 step: 332, loss is 0.005441153421998024\n",
      "epoch: 2 step: 333, loss is 0.00812540017068386\n",
      "epoch: 2 step: 334, loss is 0.008547762408852577\n",
      "epoch: 2 step: 335, loss is 0.012004820629954338\n",
      "epoch: 2 step: 336, loss is 0.007025807164609432\n",
      "epoch: 2 step: 337, loss is 0.01105629000812769\n",
      "epoch: 2 step: 338, loss is 0.007243375293910503\n",
      "epoch: 2 step: 339, loss is 0.00808468833565712\n",
      "epoch: 2 step: 340, loss is 0.010212806053459644\n",
      "epoch: 2 step: 341, loss is 0.02200045809149742\n",
      "epoch: 2 step: 342, loss is 0.009440543130040169\n",
      "epoch: 2 step: 343, loss is 0.011085348203778267\n",
      "epoch: 2 step: 344, loss is 0.007111185230314732\n",
      "epoch: 2 step: 345, loss is 0.003366698045283556\n",
      "epoch: 2 step: 346, loss is 0.04046216607093811\n",
      "epoch: 2 step: 347, loss is 0.00759025476872921\n",
      "epoch: 2 step: 348, loss is 0.0049139754846692085\n",
      "epoch: 2 step: 349, loss is 0.01035224087536335\n",
      "epoch: 2 step: 350, loss is 0.01391390711069107\n",
      "epoch: 2 step: 351, loss is 0.014472328126430511\n",
      "epoch: 2 step: 352, loss is 0.005594948306679726\n",
      "epoch: 2 step: 353, loss is 0.036627329885959625\n",
      "epoch: 2 step: 354, loss is 0.01297141145914793\n",
      "epoch: 2 step: 355, loss is 0.00547889806330204\n",
      "epoch: 2 step: 356, loss is 0.005106881260871887\n",
      "epoch: 2 step: 357, loss is 0.004554069135338068\n",
      "epoch: 2 step: 358, loss is 0.009702633135020733\n",
      "epoch: 2 step: 359, loss is 0.0243205726146698\n",
      "epoch: 2 step: 360, loss is 0.01735013723373413\n",
      "epoch: 2 step: 361, loss is 0.0074186865240335464\n",
      "epoch: 2 step: 362, loss is 0.019059685990214348\n",
      "epoch: 2 step: 363, loss is 0.011005147360265255\n",
      "epoch: 2 step: 364, loss is 0.008913642726838589\n",
      "epoch: 2 step: 365, loss is 0.006625224836170673\n",
      "epoch: 2 step: 366, loss is 0.038117945194244385\n",
      "epoch: 2 step: 367, loss is 0.007833052426576614\n",
      "epoch: 2 step: 368, loss is 0.006030492950230837\n",
      "epoch: 2 step: 369, loss is 0.008054114878177643\n",
      "epoch: 2 step: 370, loss is 0.007680104114115238\n",
      "epoch: 2 step: 371, loss is 0.05280371755361557\n",
      "epoch: 2 step: 372, loss is 0.0111418841406703\n",
      "epoch: 2 step: 373, loss is 0.01996297389268875\n",
      "epoch: 2 step: 374, loss is 0.0014592834049835801\n",
      "epoch: 2 step: 375, loss is 0.002092804526910186\n",
      "epoch: 2 step: 376, loss is 0.013346853666007519\n",
      "epoch: 2 step: 377, loss is 0.02287452295422554\n",
      "epoch: 2 step: 378, loss is 0.015668218955397606\n",
      "epoch: 2 step: 379, loss is 0.011724120005965233\n",
      "epoch: 2 step: 380, loss is 0.008699299767613411\n",
      "epoch: 2 step: 381, loss is 0.005700815003365278\n",
      "epoch: 2 step: 382, loss is 0.014159041456878185\n",
      "epoch: 2 step: 383, loss is 0.005988620221614838\n",
      "epoch: 2 step: 384, loss is 0.024525444954633713\n",
      "epoch: 2 step: 385, loss is 0.007489125244319439\n",
      "epoch: 2 step: 386, loss is 0.015601616352796555\n",
      "epoch: 2 step: 387, loss is 0.004577335901558399\n",
      "epoch: 2 step: 388, loss is 0.003128939773887396\n",
      "epoch: 2 step: 389, loss is 0.00917956791818142\n",
      "epoch: 2 step: 390, loss is 0.014061741530895233\n",
      "epoch: 2 step: 391, loss is 0.006634578574448824\n",
      "epoch: 2 step: 392, loss is 0.006607559509575367\n",
      "epoch: 2 step: 393, loss is 0.0028490861877799034\n",
      "epoch: 2 step: 394, loss is 0.005129907745867968\n",
      "epoch: 2 step: 395, loss is 0.008009973913431168\n",
      "epoch: 2 step: 396, loss is 0.006908501032739878\n",
      "epoch: 2 step: 397, loss is 0.0063593448139727116\n",
      "epoch: 2 step: 398, loss is 0.008162368088960648\n",
      "epoch: 2 step: 399, loss is 0.00936852302402258\n",
      "epoch: 2 step: 400, loss is 0.004490361548960209\n",
      "epoch: 2 step: 401, loss is 0.008882192894816399\n",
      "epoch: 2 step: 402, loss is 0.00760625209659338\n",
      "epoch: 2 step: 403, loss is 0.013553635217249393\n",
      "epoch: 2 step: 404, loss is 0.012981176376342773\n",
      "epoch: 2 step: 405, loss is 0.003392955753952265\n",
      "epoch: 2 step: 406, loss is 0.014764686115086079\n",
      "epoch: 2 step: 407, loss is 0.012322495691478252\n",
      "epoch: 2 step: 408, loss is 0.009391330182552338\n",
      "epoch: 2 step: 409, loss is 0.00373164564371109\n",
      "epoch: 2 step: 410, loss is 0.002010381780564785\n",
      "epoch: 2 step: 411, loss is 0.003468860872089863\n",
      "epoch: 2 step: 412, loss is 0.014716086909174919\n",
      "epoch: 2 step: 413, loss is 0.013026813045144081\n",
      "epoch: 2 step: 414, loss is 0.005744964815676212\n",
      "epoch: 2 step: 415, loss is 0.010614803060889244\n",
      "epoch: 2 step: 416, loss is 0.007377059198915958\n",
      "epoch: 2 step: 417, loss is 0.006669471971690655\n",
      "epoch: 2 step: 418, loss is 0.004745899699628353\n",
      "epoch: 2 step: 419, loss is 0.004466602578759193\n",
      "epoch: 2 step: 420, loss is 0.01322401873767376\n",
      "epoch: 2 step: 421, loss is 0.0066144224256277084\n",
      "epoch: 2 step: 422, loss is 0.03788778558373451\n",
      "epoch: 2 step: 423, loss is 0.024055002257227898\n",
      "epoch: 2 step: 424, loss is 0.03886650875210762\n",
      "epoch: 2 step: 425, loss is 0.08585485816001892\n",
      "epoch: 2 step: 426, loss is 0.013030055910348892\n",
      "epoch: 2 step: 427, loss is 0.012806172482669353\n",
      "epoch: 2 step: 428, loss is 0.06757108122110367\n",
      "epoch: 2 step: 429, loss is 0.0066342903301119804\n",
      "epoch: 2 step: 430, loss is 0.0026871319860219955\n",
      "epoch: 2 step: 431, loss is 0.0030266237445175648\n",
      "epoch: 2 step: 432, loss is 0.003339491318911314\n",
      "epoch: 2 step: 433, loss is 0.01861925795674324\n",
      "epoch: 2 step: 434, loss is 0.0025618027430027723\n",
      "epoch: 2 step: 435, loss is 0.004667183849960566\n",
      "epoch: 2 step: 436, loss is 0.018686136230826378\n",
      "epoch: 2 step: 437, loss is 0.0033726603724062443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 438, loss is 0.0026271771639585495\n",
      "epoch: 2 step: 439, loss is 0.009536880999803543\n",
      "epoch: 2 step: 440, loss is 0.006382306572049856\n",
      "epoch: 2 step: 441, loss is 0.012367771938443184\n",
      "epoch: 2 step: 442, loss is 0.007204510271549225\n",
      "epoch: 2 step: 443, loss is 0.02627609111368656\n",
      "epoch: 2 step: 444, loss is 0.008799084462225437\n",
      "epoch: 2 step: 445, loss is 0.002627147827297449\n",
      "epoch: 2 step: 446, loss is 0.020168304443359375\n",
      "epoch: 2 step: 447, loss is 0.01496854331344366\n",
      "epoch: 2 step: 448, loss is 0.006256332155317068\n",
      "epoch: 2 step: 449, loss is 0.00666449498385191\n",
      "epoch: 2 step: 450, loss is 0.004919057246297598\n",
      "epoch: 2 step: 451, loss is 0.006529290694743395\n",
      "epoch: 2 step: 452, loss is 0.02729063108563423\n",
      "epoch: 2 step: 453, loss is 0.010080198757350445\n",
      "epoch: 2 step: 454, loss is 0.0093270568177104\n",
      "epoch: 2 step: 455, loss is 0.008004962466657162\n",
      "epoch: 2 step: 456, loss is 0.006331617012619972\n",
      "epoch: 2 step: 457, loss is 0.001337971305474639\n",
      "epoch: 2 step: 458, loss is 0.003736628917977214\n",
      "epoch: 2 step: 459, loss is 0.009218525141477585\n",
      "epoch: 2 step: 460, loss is 0.004664360545575619\n",
      "epoch: 2 step: 461, loss is 0.016847902908921242\n",
      "epoch: 2 step: 462, loss is 0.005773622076958418\n",
      "epoch: 2 step: 463, loss is 0.005139337852597237\n",
      "epoch: 2 step: 464, loss is 0.0022953751031309366\n",
      "epoch: 2 step: 465, loss is 0.003202836262062192\n",
      "epoch: 2 step: 466, loss is 0.003656673012301326\n",
      "epoch: 2 step: 467, loss is 0.0034155775792896748\n",
      "epoch: 2 step: 468, loss is 0.005797640886157751\n",
      "epoch: 2 step: 469, loss is 0.007135254796594381\n",
      "epoch: 2 step: 470, loss is 0.019230712205171585\n",
      "epoch: 2 step: 471, loss is 0.005265665240585804\n",
      "epoch: 2 step: 472, loss is 0.015970466658473015\n",
      "epoch: 2 step: 473, loss is 0.01510328147560358\n",
      "epoch: 2 step: 474, loss is 0.006469437852501869\n",
      "epoch: 2 step: 475, loss is 0.024198580533266068\n",
      "epoch: 2 step: 476, loss is 0.009638470597565174\n",
      "epoch: 2 step: 477, loss is 0.01355940941721201\n",
      "epoch: 2 step: 478, loss is 0.009393903426826\n",
      "epoch: 2 step: 479, loss is 0.0033185244537889957\n",
      "epoch: 2 step: 480, loss is 0.006230158731341362\n",
      "epoch: 2 step: 481, loss is 0.004400852136313915\n",
      "epoch: 2 step: 482, loss is 0.005399792455136776\n",
      "epoch: 2 step: 483, loss is 0.002244835952296853\n",
      "epoch: 2 step: 484, loss is 0.015531825833022594\n",
      "epoch: 2 step: 485, loss is 0.004782911390066147\n",
      "epoch: 2 step: 486, loss is 0.007858799770474434\n",
      "epoch: 2 step: 487, loss is 0.005504537373781204\n",
      "epoch: 2 step: 488, loss is 0.002052850788459182\n",
      "epoch: 2 step: 489, loss is 0.010168038308620453\n",
      "epoch: 2 step: 490, loss is 0.005431101191788912\n",
      "epoch: 2 step: 491, loss is 0.011981706134974957\n",
      "epoch: 2 step: 492, loss is 0.0066164713352918625\n",
      "epoch: 2 step: 493, loss is 0.004602399654686451\n",
      "epoch: 2 step: 494, loss is 0.003744269721210003\n",
      "epoch: 2 step: 495, loss is 0.011033207178115845\n",
      "epoch: 2 step: 496, loss is 0.0038764886558055878\n",
      "epoch: 2 step: 497, loss is 0.0034455773420631886\n",
      "epoch: 2 step: 498, loss is 0.017010804265737534\n",
      "epoch: 2 step: 499, loss is 0.005535636097192764\n",
      "epoch: 2 step: 500, loss is 0.014757481403648853\n",
      "epoch: 2 step: 501, loss is 0.013819998130202293\n",
      "epoch: 2 step: 502, loss is 0.027479827404022217\n",
      "epoch: 2 step: 503, loss is 0.0029676309786736965\n",
      "epoch: 2 step: 504, loss is 0.0030823913402855396\n",
      "epoch: 2 step: 505, loss is 0.008167825639247894\n",
      "epoch: 2 step: 506, loss is 0.009227745234966278\n",
      "epoch: 2 step: 507, loss is 0.0031297802925109863\n",
      "epoch: 2 step: 508, loss is 0.008234352804720402\n",
      "epoch: 2 step: 509, loss is 0.004360933788120747\n",
      "epoch: 2 step: 510, loss is 0.004969114437699318\n",
      "epoch: 2 step: 511, loss is 0.007746916264295578\n",
      "epoch: 2 step: 512, loss is 0.00954003818333149\n",
      "epoch: 2 step: 513, loss is 0.001855180598795414\n",
      "epoch: 2 step: 514, loss is 0.017982332035899162\n",
      "epoch: 2 step: 515, loss is 0.028758101165294647\n",
      "epoch: 2 step: 516, loss is 0.0022083960939198732\n",
      "epoch: 2 step: 517, loss is 0.0023844456300139427\n",
      "epoch: 2 step: 518, loss is 0.002030009403824806\n",
      "epoch: 2 step: 519, loss is 0.005757743027061224\n",
      "epoch: 2 step: 520, loss is 0.021936964243650436\n",
      "epoch: 2 step: 521, loss is 0.007152193691581488\n",
      "epoch: 2 step: 522, loss is 0.01953326351940632\n",
      "epoch: 2 step: 523, loss is 0.0015666862018406391\n",
      "epoch: 2 step: 524, loss is 0.0011358070187270641\n",
      "epoch: 2 step: 525, loss is 0.007175063714385033\n",
      "epoch: 2 step: 526, loss is 0.0051568360067903996\n",
      "epoch: 2 step: 527, loss is 0.005014582071453333\n",
      "epoch: 2 step: 528, loss is 0.004868611693382263\n",
      "epoch: 2 step: 529, loss is 0.02380458638072014\n",
      "epoch: 2 step: 530, loss is 0.0023501331452280283\n",
      "epoch: 2 step: 531, loss is 0.008820977061986923\n",
      "epoch: 2 step: 532, loss is 0.004573977552354336\n",
      "epoch: 2 step: 533, loss is 0.0020130532793700695\n",
      "epoch: 2 step: 534, loss is 0.004106299020349979\n",
      "epoch: 2 step: 535, loss is 0.003090051468461752\n",
      "epoch: 2 step: 536, loss is 0.003068986115977168\n",
      "epoch: 2 step: 537, loss is 0.007590049412101507\n",
      "epoch: 2 step: 538, loss is 0.005262246821075678\n",
      "epoch: 2 step: 539, loss is 0.004060057923197746\n",
      "epoch: 2 step: 540, loss is 0.0023265378549695015\n",
      "epoch: 2 step: 541, loss is 0.003920907154679298\n",
      "epoch: 2 step: 542, loss is 0.0014486198779195547\n",
      "epoch: 2 step: 543, loss is 0.010656939819455147\n",
      "epoch: 2 step: 544, loss is 0.005042768083512783\n",
      "epoch: 2 step: 545, loss is 0.00511306943371892\n",
      "epoch: 2 step: 546, loss is 0.0069382875226438046\n",
      "epoch: 2 step: 547, loss is 0.0026853447780013084\n",
      "epoch: 2 step: 548, loss is 0.002069464884698391\n",
      "epoch: 2 step: 549, loss is 0.003564476501196623\n",
      "epoch: 2 step: 550, loss is 0.0062670293264091015\n",
      "epoch: 2 step: 551, loss is 0.010634241625666618\n",
      "epoch: 2 step: 552, loss is 0.002697466406971216\n",
      "epoch: 2 step: 553, loss is 0.01304404903203249\n",
      "epoch: 2 step: 554, loss is 0.0018860037671402097\n",
      "epoch: 2 step: 555, loss is 0.013663804158568382\n",
      "epoch: 2 step: 556, loss is 0.0099055590108037\n",
      "epoch: 2 step: 557, loss is 0.007284773513674736\n",
      "epoch: 2 step: 558, loss is 0.005016758106648922\n",
      "epoch: 2 step: 559, loss is 0.0020881998352706432\n",
      "epoch: 2 step: 560, loss is 0.005397820379585028\n",
      "epoch: 2 step: 561, loss is 0.0026745558716356754\n",
      "epoch: 2 step: 562, loss is 0.003115096129477024\n",
      "epoch: 2 step: 563, loss is 0.0030052638612687588\n",
      "epoch: 2 step: 564, loss is 0.005788537673652172\n",
      "epoch: 2 step: 565, loss is 0.0038182134740054607\n",
      "epoch: 2 step: 566, loss is 0.006424997001886368\n",
      "epoch: 2 step: 567, loss is 0.004694555886089802\n",
      "epoch: 2 step: 568, loss is 0.006117885001003742\n",
      "epoch: 2 step: 569, loss is 0.004330544266849756\n",
      "epoch: 2 step: 570, loss is 0.005842844024300575\n",
      "epoch: 2 step: 571, loss is 0.031949181109666824\n",
      "epoch: 2 step: 572, loss is 0.003505210392177105\n",
      "epoch: 2 step: 573, loss is 0.013262210413813591\n",
      "epoch: 2 step: 574, loss is 0.04275883734226227\n",
      "epoch: 2 step: 575, loss is 0.0052492874674499035\n",
      "epoch: 2 step: 576, loss is 0.00342604354955256\n",
      "epoch: 2 step: 577, loss is 0.028142016381025314\n",
      "epoch: 2 step: 578, loss is 0.01706734299659729\n",
      "epoch: 2 step: 579, loss is 0.0014302909839898348\n",
      "epoch: 2 step: 580, loss is 0.0014827772974967957\n",
      "epoch: 2 step: 581, loss is 0.002156171714887023\n",
      "epoch: 2 step: 582, loss is 0.003112790174782276\n",
      "epoch: 2 step: 583, loss is 0.001631519291549921\n",
      "epoch: 2 step: 584, loss is 0.005488227121531963\n",
      "epoch: 2 step: 585, loss is 0.002361368853598833\n",
      "epoch: 2 step: 586, loss is 0.0018866891041398048\n",
      "epoch: 2 step: 587, loss is 0.004453251603990793\n",
      "epoch: 2 step: 588, loss is 0.004694798029959202\n",
      "epoch: 2 step: 589, loss is 0.018018649891018867\n",
      "epoch: 2 step: 590, loss is 0.00621719378978014\n",
      "epoch: 2 step: 591, loss is 0.006536863278597593\n",
      "epoch: 2 step: 592, loss is 0.011979497969150543\n",
      "epoch: 2 step: 593, loss is 0.01043642871081829\n",
      "epoch: 2 step: 594, loss is 0.0015386827290058136\n",
      "epoch: 2 step: 595, loss is 0.0037608672864735126\n",
      "epoch: 2 step: 596, loss is 0.011288458481431007\n",
      "Train epoch time: 111420.542 ms, per step time: 186.947 ms\n",
      "epoch: 3 step: 1, loss is 0.00251333462074399\n",
      "epoch: 3 step: 2, loss is 0.0012378751998767257\n",
      "epoch: 3 step: 3, loss is 0.0033146205823868513\n",
      "epoch: 3 step: 4, loss is 0.002038612961769104\n",
      "epoch: 3 step: 5, loss is 0.004691945388913155\n",
      "epoch: 3 step: 6, loss is 0.0021441068965941668\n",
      "epoch: 3 step: 7, loss is 0.005605688784271479\n",
      "epoch: 3 step: 8, loss is 0.023811878636479378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 9, loss is 0.0037738222163170576\n",
      "epoch: 3 step: 10, loss is 0.0018602304626256227\n",
      "epoch: 3 step: 11, loss is 0.0015787871088832617\n",
      "epoch: 3 step: 12, loss is 0.008791555650532246\n",
      "epoch: 3 step: 13, loss is 0.0049767643213272095\n",
      "epoch: 3 step: 14, loss is 0.014846229925751686\n",
      "epoch: 3 step: 15, loss is 0.0035001463256776333\n",
      "epoch: 3 step: 16, loss is 0.005064032040536404\n",
      "epoch: 3 step: 17, loss is 0.002774425083771348\n",
      "epoch: 3 step: 18, loss is 0.0010333681711927056\n",
      "epoch: 3 step: 19, loss is 0.0015181889757514\n",
      "epoch: 3 step: 20, loss is 0.002094816882163286\n",
      "epoch: 3 step: 21, loss is 0.0028935903683304787\n",
      "epoch: 3 step: 22, loss is 0.006891153287142515\n",
      "epoch: 3 step: 23, loss is 0.00998653657734394\n",
      "epoch: 3 step: 24, loss is 0.00104574766010046\n",
      "epoch: 3 step: 25, loss is 0.004611643962562084\n",
      "epoch: 3 step: 26, loss is 0.0007998172077350318\n",
      "epoch: 3 step: 27, loss is 0.003581422381103039\n",
      "epoch: 3 step: 28, loss is 0.016479047015309334\n",
      "epoch: 3 step: 29, loss is 0.00449483934789896\n",
      "epoch: 3 step: 30, loss is 0.005042770877480507\n",
      "epoch: 3 step: 31, loss is 0.002330202143639326\n",
      "epoch: 3 step: 32, loss is 0.0014338456094264984\n",
      "epoch: 3 step: 33, loss is 0.0026387041434645653\n",
      "epoch: 3 step: 34, loss is 0.0016328399069607258\n",
      "epoch: 3 step: 35, loss is 0.0028927335515618324\n",
      "epoch: 3 step: 36, loss is 0.003597564995288849\n",
      "epoch: 3 step: 37, loss is 0.002417807700112462\n",
      "epoch: 3 step: 38, loss is 0.0037270579487085342\n",
      "epoch: 3 step: 39, loss is 0.0026517801452428102\n",
      "epoch: 3 step: 40, loss is 0.0025872569531202316\n",
      "epoch: 3 step: 41, loss is 0.004010407254099846\n",
      "epoch: 3 step: 42, loss is 0.0029021943919360638\n",
      "epoch: 3 step: 43, loss is 0.0025445432402193546\n",
      "epoch: 3 step: 44, loss is 0.0051126317121088505\n",
      "epoch: 3 step: 45, loss is 0.004568110220134258\n",
      "epoch: 3 step: 46, loss is 0.0019713505171239376\n",
      "epoch: 3 step: 47, loss is 0.0035680639557540417\n",
      "epoch: 3 step: 48, loss is 0.017714068293571472\n",
      "epoch: 3 step: 49, loss is 0.005173064302653074\n",
      "epoch: 3 step: 50, loss is 0.0035875935573130846\n",
      "epoch: 3 step: 51, loss is 0.004789890721440315\n",
      "epoch: 3 step: 52, loss is 0.007233463227748871\n",
      "epoch: 3 step: 53, loss is 0.007876918651163578\n",
      "epoch: 3 step: 54, loss is 0.0016706762835383415\n",
      "epoch: 3 step: 55, loss is 0.005448811687529087\n",
      "epoch: 3 step: 56, loss is 0.004737379960715771\n",
      "epoch: 3 step: 57, loss is 0.0033497512340545654\n",
      "epoch: 3 step: 58, loss is 0.0017972394125536084\n",
      "epoch: 3 step: 59, loss is 0.0015670850407332182\n",
      "epoch: 3 step: 60, loss is 0.005358664784580469\n",
      "epoch: 3 step: 61, loss is 0.005319165997207165\n",
      "epoch: 3 step: 62, loss is 0.002323128981515765\n",
      "epoch: 3 step: 63, loss is 0.00304948422126472\n",
      "epoch: 3 step: 64, loss is 0.0037378042470663786\n",
      "epoch: 3 step: 65, loss is 0.004247716628015041\n",
      "epoch: 3 step: 66, loss is 0.012986755929887295\n",
      "epoch: 3 step: 67, loss is 0.002171454718336463\n",
      "epoch: 3 step: 68, loss is 0.007961745373904705\n",
      "epoch: 3 step: 69, loss is 0.004467553459107876\n",
      "epoch: 3 step: 70, loss is 0.006465963553637266\n",
      "epoch: 3 step: 71, loss is 0.0017201834125444293\n",
      "epoch: 3 step: 72, loss is 0.0017839911160990596\n",
      "epoch: 3 step: 73, loss is 0.013510545715689659\n",
      "epoch: 3 step: 74, loss is 0.004244101233780384\n",
      "epoch: 3 step: 75, loss is 0.0022590451408177614\n",
      "epoch: 3 step: 76, loss is 0.0014566826866939664\n",
      "epoch: 3 step: 77, loss is 0.0013776742853224277\n",
      "epoch: 3 step: 78, loss is 0.002982983598485589\n",
      "epoch: 3 step: 79, loss is 0.001163142966106534\n",
      "epoch: 3 step: 80, loss is 0.004742858000099659\n",
      "epoch: 3 step: 81, loss is 0.002359324134886265\n",
      "epoch: 3 step: 82, loss is 0.009719980880618095\n",
      "epoch: 3 step: 83, loss is 0.006188936065882444\n",
      "epoch: 3 step: 84, loss is 0.007838323712348938\n",
      "epoch: 3 step: 85, loss is 0.0031435894779860973\n",
      "epoch: 3 step: 86, loss is 0.0011881247628480196\n",
      "epoch: 3 step: 87, loss is 0.003324572229757905\n",
      "epoch: 3 step: 88, loss is 0.0023061626125127077\n",
      "epoch: 3 step: 89, loss is 0.0057171061635017395\n",
      "epoch: 3 step: 90, loss is 0.003304912708699703\n",
      "epoch: 3 step: 91, loss is 0.001743552042171359\n",
      "epoch: 3 step: 92, loss is 0.013589328154921532\n",
      "epoch: 3 step: 93, loss is 0.0020937316585332155\n",
      "epoch: 3 step: 94, loss is 0.002934491029009223\n",
      "epoch: 3 step: 95, loss is 0.0014111899072304368\n",
      "epoch: 3 step: 96, loss is 0.007253054063767195\n",
      "epoch: 3 step: 97, loss is 0.004522961564362049\n",
      "epoch: 3 step: 98, loss is 0.001598887611180544\n",
      "epoch: 3 step: 99, loss is 0.005104367155581713\n",
      "epoch: 3 step: 100, loss is 0.001422230969183147\n",
      "epoch: 3 step: 101, loss is 0.0023788854014128447\n",
      "epoch: 3 step: 102, loss is 0.0025870082899928093\n",
      "epoch: 3 step: 103, loss is 0.007870727218687534\n",
      "epoch: 3 step: 104, loss is 0.0019501185743138194\n",
      "epoch: 3 step: 105, loss is 0.0013483212096616626\n",
      "epoch: 3 step: 106, loss is 0.004710667300969362\n",
      "epoch: 3 step: 107, loss is 0.001748627983033657\n",
      "epoch: 3 step: 108, loss is 0.004856878891587257\n",
      "epoch: 3 step: 109, loss is 0.0030468502081930637\n",
      "epoch: 3 step: 110, loss is 0.004524708725512028\n",
      "epoch: 3 step: 111, loss is 0.0029006912373006344\n",
      "epoch: 3 step: 112, loss is 0.002055955585092306\n",
      "epoch: 3 step: 113, loss is 0.0012028890196233988\n",
      "epoch: 3 step: 114, loss is 0.001998736523091793\n",
      "epoch: 3 step: 115, loss is 0.006129148881882429\n",
      "epoch: 3 step: 116, loss is 0.002049675676971674\n",
      "epoch: 3 step: 117, loss is 0.006595776416361332\n",
      "epoch: 3 step: 118, loss is 0.0059523265808820724\n",
      "epoch: 3 step: 119, loss is 0.002713168039917946\n",
      "epoch: 3 step: 120, loss is 0.0028879251331090927\n",
      "epoch: 3 step: 121, loss is 0.003191367955878377\n",
      "epoch: 3 step: 122, loss is 0.0022761106956750154\n",
      "epoch: 3 step: 123, loss is 0.0026264910120517015\n",
      "epoch: 3 step: 124, loss is 0.0074959988705813885\n",
      "epoch: 3 step: 125, loss is 0.0021802231203764677\n",
      "epoch: 3 step: 126, loss is 0.0038747554644942284\n",
      "epoch: 3 step: 127, loss is 0.014017704874277115\n",
      "epoch: 3 step: 128, loss is 0.005601214710623026\n",
      "epoch: 3 step: 129, loss is 0.002599008148536086\n",
      "epoch: 3 step: 130, loss is 0.011673507280647755\n",
      "epoch: 3 step: 131, loss is 0.0025326733011752367\n",
      "epoch: 3 step: 132, loss is 0.0033278530463576317\n",
      "epoch: 3 step: 133, loss is 0.0015430953353643417\n",
      "epoch: 3 step: 134, loss is 0.0026270318776369095\n",
      "epoch: 3 step: 135, loss is 0.004755808971822262\n",
      "epoch: 3 step: 136, loss is 0.0008679850143380463\n",
      "epoch: 3 step: 137, loss is 0.0020641859155148268\n",
      "epoch: 3 step: 138, loss is 0.0011906924191862345\n",
      "epoch: 3 step: 139, loss is 0.0017155836103484035\n",
      "epoch: 3 step: 140, loss is 0.009674148634076118\n",
      "epoch: 3 step: 141, loss is 0.009430729784071445\n",
      "epoch: 3 step: 142, loss is 0.002693782327696681\n",
      "epoch: 3 step: 143, loss is 0.0027889274060726166\n",
      "epoch: 3 step: 144, loss is 0.001106338924728334\n",
      "epoch: 3 step: 145, loss is 0.005927506368607283\n",
      "epoch: 3 step: 146, loss is 0.001969298580661416\n",
      "epoch: 3 step: 147, loss is 0.0017769376281648874\n",
      "epoch: 3 step: 148, loss is 0.0022657341323792934\n",
      "epoch: 3 step: 149, loss is 0.0047972979955375195\n",
      "epoch: 3 step: 150, loss is 0.002641388215124607\n",
      "epoch: 3 step: 151, loss is 0.00280597060918808\n",
      "epoch: 3 step: 152, loss is 0.003295500995591283\n",
      "epoch: 3 step: 153, loss is 0.0011381859658285975\n",
      "epoch: 3 step: 154, loss is 0.002941905055195093\n",
      "epoch: 3 step: 155, loss is 0.004328166600316763\n",
      "epoch: 3 step: 156, loss is 0.0021188321989029646\n",
      "epoch: 3 step: 157, loss is 0.0037724338471889496\n",
      "epoch: 3 step: 158, loss is 0.0012910000514239073\n",
      "epoch: 3 step: 159, loss is 0.000438638060586527\n",
      "epoch: 3 step: 160, loss is 0.0010244412114843726\n",
      "epoch: 3 step: 161, loss is 0.0022577596828341484\n",
      "epoch: 3 step: 162, loss is 0.002144466619938612\n",
      "epoch: 3 step: 163, loss is 0.003205858403816819\n",
      "epoch: 3 step: 164, loss is 0.0027357060462236404\n",
      "epoch: 3 step: 165, loss is 0.007832457311451435\n",
      "epoch: 3 step: 166, loss is 0.0007065278477966785\n",
      "epoch: 3 step: 167, loss is 0.000847060582600534\n",
      "epoch: 3 step: 168, loss is 0.0016509178094565868\n",
      "epoch: 3 step: 169, loss is 0.0009906855411827564\n",
      "epoch: 3 step: 170, loss is 0.005557997617870569\n",
      "epoch: 3 step: 171, loss is 0.002193945925682783\n",
      "epoch: 3 step: 172, loss is 0.0009334491332992911\n",
      "epoch: 3 step: 173, loss is 0.008335055783390999\n",
      "epoch: 3 step: 174, loss is 0.007770948112010956\n",
      "epoch: 3 step: 175, loss is 0.000400508230086416\n",
      "epoch: 3 step: 176, loss is 0.0033124065957963467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 177, loss is 0.009631441906094551\n",
      "epoch: 3 step: 178, loss is 0.0017834408208727837\n",
      "epoch: 3 step: 179, loss is 0.0019823359325528145\n",
      "epoch: 3 step: 180, loss is 0.011531582102179527\n",
      "epoch: 3 step: 181, loss is 0.0004908301052637398\n",
      "epoch: 3 step: 182, loss is 0.004027003422379494\n",
      "epoch: 3 step: 183, loss is 0.0021129590459167957\n",
      "epoch: 3 step: 184, loss is 0.0013092614244669676\n",
      "epoch: 3 step: 185, loss is 0.0006659234641119838\n",
      "epoch: 3 step: 186, loss is 0.0005044363206252456\n",
      "epoch: 3 step: 187, loss is 0.0007057944312691689\n",
      "epoch: 3 step: 188, loss is 0.0015808448661118746\n",
      "epoch: 3 step: 189, loss is 0.004013881087303162\n",
      "epoch: 3 step: 190, loss is 0.0011823601089417934\n",
      "epoch: 3 step: 191, loss is 0.002101839054375887\n",
      "epoch: 3 step: 192, loss is 0.010810889303684235\n",
      "epoch: 3 step: 193, loss is 0.025455689057707787\n",
      "epoch: 3 step: 194, loss is 0.006820638198405504\n",
      "epoch: 3 step: 195, loss is 0.008311974816024303\n",
      "epoch: 3 step: 196, loss is 0.0020803080406039953\n",
      "epoch: 3 step: 197, loss is 0.0014645225601270795\n",
      "epoch: 3 step: 198, loss is 0.0008659771410748363\n",
      "epoch: 3 step: 199, loss is 0.0016763733001425862\n",
      "epoch: 3 step: 200, loss is 0.002226106822490692\n",
      "epoch: 3 step: 201, loss is 0.004977795761078596\n",
      "epoch: 3 step: 202, loss is 0.004022326786071062\n",
      "epoch: 3 step: 203, loss is 0.0024670639541000128\n",
      "epoch: 3 step: 204, loss is 0.0043919384479522705\n",
      "epoch: 3 step: 205, loss is 0.0034156402107328176\n",
      "epoch: 3 step: 206, loss is 0.006416205316781998\n",
      "epoch: 3 step: 207, loss is 0.002584655536338687\n",
      "epoch: 3 step: 208, loss is 0.0020408467389643192\n",
      "epoch: 3 step: 209, loss is 0.0024190174881368876\n",
      "epoch: 3 step: 210, loss is 0.003603521967306733\n",
      "epoch: 3 step: 211, loss is 0.008830818347632885\n",
      "epoch: 3 step: 212, loss is 0.0020779690239578485\n",
      "epoch: 3 step: 213, loss is 0.0014564127195626497\n",
      "epoch: 3 step: 214, loss is 0.009883835911750793\n",
      "epoch: 3 step: 215, loss is 0.0016552145825698972\n",
      "epoch: 3 step: 216, loss is 0.002006360562518239\n",
      "epoch: 3 step: 217, loss is 0.007174456492066383\n",
      "epoch: 3 step: 218, loss is 0.0032918136566877365\n",
      "epoch: 3 step: 219, loss is 0.0017003169050440192\n",
      "epoch: 3 step: 220, loss is 0.001456045894883573\n",
      "epoch: 3 step: 221, loss is 0.0014956446830183268\n",
      "epoch: 3 step: 222, loss is 0.009741762652993202\n",
      "epoch: 3 step: 223, loss is 0.002251171739771962\n",
      "epoch: 3 step: 224, loss is 0.0014590390492230654\n",
      "epoch: 3 step: 225, loss is 0.0006401532446034253\n",
      "epoch: 3 step: 226, loss is 0.0017195218242704868\n",
      "epoch: 3 step: 227, loss is 0.004143214784562588\n",
      "epoch: 3 step: 228, loss is 0.0015755703207105398\n",
      "epoch: 3 step: 229, loss is 0.0035706418566405773\n",
      "epoch: 3 step: 230, loss is 0.0015220281202346087\n",
      "epoch: 3 step: 231, loss is 0.0015640195924788713\n",
      "epoch: 3 step: 232, loss is 0.002019318053498864\n",
      "epoch: 3 step: 233, loss is 0.007245071232318878\n",
      "epoch: 3 step: 234, loss is 0.001630420796573162\n",
      "epoch: 3 step: 235, loss is 0.001604883000254631\n",
      "epoch: 3 step: 236, loss is 0.0013244288275018334\n",
      "epoch: 3 step: 237, loss is 0.005027720704674721\n",
      "epoch: 3 step: 238, loss is 0.0015744513366371393\n",
      "epoch: 3 step: 239, loss is 0.003963977098464966\n",
      "epoch: 3 step: 240, loss is 0.002586300252005458\n",
      "epoch: 3 step: 241, loss is 0.0021734051406383514\n",
      "epoch: 3 step: 242, loss is 0.010226845741271973\n",
      "epoch: 3 step: 243, loss is 0.0037429421208798885\n",
      "epoch: 3 step: 244, loss is 0.000521570211276412\n",
      "epoch: 3 step: 245, loss is 0.0056245895102620125\n",
      "epoch: 3 step: 246, loss is 0.0032799621112644672\n",
      "epoch: 3 step: 247, loss is 0.0008205122430808842\n",
      "epoch: 3 step: 248, loss is 0.002273520454764366\n",
      "epoch: 3 step: 249, loss is 0.0005797239136882126\n",
      "epoch: 3 step: 250, loss is 0.004889315459877253\n",
      "epoch: 3 step: 251, loss is 0.002508303616195917\n",
      "epoch: 3 step: 252, loss is 0.00668689189478755\n",
      "epoch: 3 step: 253, loss is 0.007109483703970909\n",
      "epoch: 3 step: 254, loss is 0.0012648120755329728\n",
      "epoch: 3 step: 255, loss is 0.0058440337888896465\n",
      "epoch: 3 step: 256, loss is 0.00023410655558109283\n",
      "epoch: 3 step: 257, loss is 0.003962772432714701\n",
      "epoch: 3 step: 258, loss is 0.001807264517992735\n",
      "epoch: 3 step: 259, loss is 0.002214522799476981\n",
      "epoch: 3 step: 260, loss is 0.0017286129295825958\n",
      "epoch: 3 step: 261, loss is 0.0021206752862781286\n",
      "epoch: 3 step: 262, loss is 0.0004678695695474744\n",
      "epoch: 3 step: 263, loss is 0.0010734559036791325\n",
      "epoch: 3 step: 264, loss is 0.002923128195106983\n",
      "epoch: 3 step: 265, loss is 0.0015089744701981544\n",
      "epoch: 3 step: 266, loss is 0.0011485773138701916\n",
      "epoch: 3 step: 267, loss is 0.0013536687474697828\n",
      "epoch: 3 step: 268, loss is 0.004689628258347511\n",
      "epoch: 3 step: 269, loss is 0.006662324070930481\n",
      "epoch: 3 step: 270, loss is 0.0012826171005144715\n",
      "epoch: 3 step: 271, loss is 0.0030864346772432327\n",
      "epoch: 3 step: 272, loss is 0.002985030645504594\n",
      "epoch: 3 step: 273, loss is 0.004005237482488155\n",
      "epoch: 3 step: 274, loss is 0.002528332406654954\n",
      "epoch: 3 step: 275, loss is 0.0028573067393153906\n",
      "epoch: 3 step: 276, loss is 0.011245700530707836\n",
      "epoch: 3 step: 277, loss is 0.002650382462888956\n",
      "epoch: 3 step: 278, loss is 0.0008219351293519139\n",
      "epoch: 3 step: 279, loss is 0.007877083495259285\n",
      "epoch: 3 step: 280, loss is 0.0024867744650691748\n",
      "epoch: 3 step: 281, loss is 0.001584229408763349\n",
      "epoch: 3 step: 282, loss is 0.0006098782178014517\n",
      "epoch: 3 step: 283, loss is 0.001273730187676847\n",
      "epoch: 3 step: 284, loss is 0.009046371094882488\n",
      "epoch: 3 step: 285, loss is 0.0007321200100705028\n",
      "epoch: 3 step: 286, loss is 0.004201142117381096\n",
      "epoch: 3 step: 287, loss is 0.007817191071808338\n",
      "epoch: 3 step: 288, loss is 0.002712882123887539\n",
      "epoch: 3 step: 289, loss is 0.0016319719143211842\n",
      "epoch: 3 step: 290, loss is 0.007683754898607731\n",
      "epoch: 3 step: 291, loss is 0.0013968689600005746\n",
      "epoch: 3 step: 292, loss is 0.002142959041520953\n",
      "epoch: 3 step: 293, loss is 0.0022479291073977947\n",
      "epoch: 3 step: 294, loss is 0.003250847337767482\n",
      "epoch: 3 step: 295, loss is 0.0028064455837011337\n",
      "epoch: 3 step: 296, loss is 0.001791543560102582\n",
      "epoch: 3 step: 297, loss is 0.002052301075309515\n",
      "epoch: 3 step: 298, loss is 0.0031276545487344265\n",
      "epoch: 3 step: 299, loss is 0.0072120013646781445\n",
      "epoch: 3 step: 300, loss is 0.001451684976927936\n",
      "epoch: 3 step: 301, loss is 0.002262098714709282\n",
      "epoch: 3 step: 302, loss is 0.001244785962626338\n",
      "epoch: 3 step: 303, loss is 0.000612687086686492\n",
      "epoch: 3 step: 304, loss is 0.0010407452937215567\n",
      "epoch: 3 step: 305, loss is 0.004091507755219936\n",
      "epoch: 3 step: 306, loss is 0.0011235822457820177\n",
      "epoch: 3 step: 307, loss is 0.0011039646342396736\n",
      "epoch: 3 step: 308, loss is 0.0007796616991981864\n",
      "epoch: 3 step: 309, loss is 0.0034904619678854942\n",
      "epoch: 3 step: 310, loss is 0.0016614975174888968\n",
      "epoch: 3 step: 311, loss is 0.0006982808699831367\n",
      "epoch: 3 step: 312, loss is 0.004724529571831226\n",
      "epoch: 3 step: 313, loss is 0.0014176461845636368\n",
      "epoch: 3 step: 314, loss is 0.0007087516132742167\n",
      "epoch: 3 step: 315, loss is 0.0005574424285441637\n",
      "epoch: 3 step: 316, loss is 0.0007820859900675714\n",
      "epoch: 3 step: 317, loss is 0.001004326157271862\n",
      "epoch: 3 step: 318, loss is 0.0020862999372184277\n",
      "epoch: 3 step: 319, loss is 0.0028480018954724073\n",
      "epoch: 3 step: 320, loss is 0.00271375454030931\n",
      "epoch: 3 step: 321, loss is 0.005842635408043861\n",
      "epoch: 3 step: 322, loss is 0.00255895615555346\n",
      "epoch: 3 step: 323, loss is 0.00048500264529138803\n",
      "epoch: 3 step: 324, loss is 0.0008674443233758211\n",
      "epoch: 3 step: 325, loss is 0.0015250238357111812\n",
      "epoch: 3 step: 326, loss is 0.011005740612745285\n",
      "epoch: 3 step: 327, loss is 0.000959619355853647\n",
      "epoch: 3 step: 328, loss is 0.0012237707851454616\n",
      "epoch: 3 step: 329, loss is 0.0014059292152523994\n",
      "epoch: 3 step: 330, loss is 0.0042795403860509396\n",
      "epoch: 3 step: 331, loss is 0.002272186102345586\n",
      "epoch: 3 step: 332, loss is 0.0020678434520959854\n",
      "epoch: 3 step: 333, loss is 0.0005989939090795815\n",
      "epoch: 3 step: 334, loss is 0.0005950065678916872\n",
      "epoch: 3 step: 335, loss is 0.0021198480390012264\n",
      "epoch: 3 step: 336, loss is 0.0026978959795087576\n",
      "epoch: 3 step: 337, loss is 0.0005756678292527795\n",
      "epoch: 3 step: 338, loss is 0.0018260735087096691\n",
      "epoch: 3 step: 339, loss is 0.0015188890974968672\n",
      "epoch: 3 step: 340, loss is 0.0015979661839082837\n",
      "epoch: 3 step: 341, loss is 0.003514267038553953\n",
      "epoch: 3 step: 342, loss is 0.0009964461205527186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 343, loss is 0.0032105231657624245\n",
      "epoch: 3 step: 344, loss is 0.0006675816839560866\n",
      "epoch: 3 step: 345, loss is 0.0005773629527539015\n",
      "epoch: 3 step: 346, loss is 0.0021025496535003185\n",
      "epoch: 3 step: 347, loss is 0.0012547338847070932\n",
      "epoch: 3 step: 348, loss is 0.0016907713143154979\n",
      "epoch: 3 step: 349, loss is 0.0012943135807290673\n",
      "epoch: 3 step: 350, loss is 0.001770001370459795\n",
      "epoch: 3 step: 351, loss is 0.0027716555632650852\n",
      "epoch: 3 step: 352, loss is 0.0008568963967263699\n",
      "epoch: 3 step: 353, loss is 0.0027890074998140335\n",
      "epoch: 3 step: 354, loss is 0.0009689048165455461\n",
      "epoch: 3 step: 355, loss is 0.0019211602630093694\n",
      "epoch: 3 step: 356, loss is 0.001730665098875761\n",
      "epoch: 3 step: 357, loss is 0.0010482785291969776\n",
      "epoch: 3 step: 358, loss is 0.0035659014247357845\n",
      "epoch: 3 step: 359, loss is 0.001331517705693841\n",
      "epoch: 3 step: 360, loss is 0.001349639380350709\n",
      "epoch: 3 step: 361, loss is 0.0007363992044702172\n",
      "epoch: 3 step: 362, loss is 0.0031550833955407143\n",
      "epoch: 3 step: 363, loss is 0.0011970315827056766\n",
      "epoch: 3 step: 364, loss is 0.001099594053812325\n",
      "epoch: 3 step: 365, loss is 0.0010283096926286817\n",
      "epoch: 3 step: 366, loss is 0.0034639451187103987\n",
      "epoch: 3 step: 367, loss is 0.0029770848341286182\n",
      "epoch: 3 step: 368, loss is 0.0016128928400576115\n",
      "epoch: 3 step: 369, loss is 0.000697062467224896\n",
      "epoch: 3 step: 370, loss is 0.0012734620831906796\n",
      "epoch: 3 step: 371, loss is 0.011238642036914825\n",
      "epoch: 3 step: 372, loss is 0.0031291297636926174\n",
      "epoch: 3 step: 373, loss is 0.0008224417106248438\n",
      "epoch: 3 step: 374, loss is 0.0008277749293483794\n",
      "epoch: 3 step: 375, loss is 0.0003023655153810978\n",
      "epoch: 3 step: 376, loss is 0.0017032281029969454\n",
      "epoch: 3 step: 377, loss is 0.0014842036180198193\n",
      "epoch: 3 step: 378, loss is 0.002902412321418524\n",
      "epoch: 3 step: 379, loss is 0.0011363743105903268\n",
      "epoch: 3 step: 380, loss is 0.0024599682074040174\n",
      "epoch: 3 step: 381, loss is 0.0005611393135040998\n",
      "epoch: 3 step: 382, loss is 0.0010003091301769018\n",
      "epoch: 3 step: 383, loss is 0.0011065755970776081\n",
      "epoch: 3 step: 384, loss is 0.0012297043576836586\n",
      "epoch: 3 step: 385, loss is 0.000965339713729918\n",
      "epoch: 3 step: 386, loss is 0.003202521475031972\n",
      "epoch: 3 step: 387, loss is 0.0015972190303727984\n",
      "epoch: 3 step: 388, loss is 0.0021385913714766502\n",
      "epoch: 3 step: 389, loss is 0.0009491463424637914\n",
      "epoch: 3 step: 390, loss is 0.0016185904387384653\n",
      "epoch: 3 step: 391, loss is 0.000794825260527432\n",
      "epoch: 3 step: 392, loss is 0.0005307025276124477\n",
      "epoch: 3 step: 393, loss is 0.0004846308147534728\n",
      "epoch: 3 step: 394, loss is 0.0010300172725692391\n",
      "epoch: 3 step: 395, loss is 0.0009725982672534883\n",
      "epoch: 3 step: 396, loss is 0.0009373975335620344\n",
      "epoch: 3 step: 397, loss is 0.0034761650022119284\n",
      "epoch: 3 step: 398, loss is 0.0005452195182442665\n",
      "epoch: 3 step: 399, loss is 0.0008177143754437566\n",
      "epoch: 3 step: 400, loss is 0.0029263326432555914\n",
      "epoch: 3 step: 401, loss is 0.0006405903841368854\n",
      "epoch: 3 step: 402, loss is 0.003937929403036833\n",
      "epoch: 3 step: 403, loss is 0.0006350334733724594\n",
      "epoch: 3 step: 404, loss is 0.002370506292209029\n",
      "epoch: 3 step: 405, loss is 0.0002378353092353791\n",
      "epoch: 3 step: 406, loss is 0.0026509659364819527\n",
      "epoch: 3 step: 407, loss is 0.002812395105138421\n",
      "epoch: 3 step: 408, loss is 0.0010541421361267567\n",
      "epoch: 3 step: 409, loss is 0.0028948630206286907\n",
      "epoch: 3 step: 410, loss is 0.0015704412944614887\n",
      "epoch: 3 step: 411, loss is 0.0007842895574867725\n",
      "epoch: 3 step: 412, loss is 0.001043402822688222\n",
      "epoch: 3 step: 413, loss is 0.0009103415068238974\n",
      "epoch: 3 step: 414, loss is 0.0021234320010989904\n",
      "epoch: 3 step: 415, loss is 0.0018220075871795416\n",
      "epoch: 3 step: 416, loss is 0.0019217606168240309\n",
      "epoch: 3 step: 417, loss is 0.0013447535457089543\n",
      "epoch: 3 step: 418, loss is 0.001132070436142385\n",
      "epoch: 3 step: 419, loss is 0.0006830866332165897\n",
      "epoch: 3 step: 420, loss is 0.0038862735964357853\n",
      "epoch: 3 step: 421, loss is 0.0020067417062819004\n",
      "epoch: 3 step: 422, loss is 0.0036078905686736107\n",
      "epoch: 3 step: 423, loss is 0.0003544448991306126\n",
      "epoch: 3 step: 424, loss is 0.001997113460674882\n",
      "epoch: 3 step: 425, loss is 0.006232171785086393\n",
      "epoch: 3 step: 426, loss is 0.0009799028048291802\n",
      "epoch: 3 step: 427, loss is 0.000604152912274003\n",
      "epoch: 3 step: 428, loss is 0.0036586932837963104\n",
      "epoch: 3 step: 429, loss is 0.003938054200261831\n",
      "epoch: 3 step: 430, loss is 0.013088013045489788\n",
      "epoch: 3 step: 431, loss is 0.0006117529701441526\n",
      "epoch: 3 step: 432, loss is 0.0006179117481224239\n",
      "epoch: 3 step: 433, loss is 0.0004533210303634405\n",
      "epoch: 3 step: 434, loss is 0.0007652119966223836\n",
      "epoch: 3 step: 435, loss is 0.0026951839681714773\n",
      "epoch: 3 step: 436, loss is 0.0013072940055280924\n",
      "epoch: 3 step: 437, loss is 0.0007042372599244118\n",
      "epoch: 3 step: 438, loss is 0.0006374125368893147\n",
      "epoch: 3 step: 439, loss is 0.0029661126900464296\n",
      "epoch: 3 step: 440, loss is 0.0010452760616317391\n",
      "epoch: 3 step: 441, loss is 0.0008367580594494939\n",
      "epoch: 3 step: 442, loss is 0.0008725030347704887\n",
      "epoch: 3 step: 443, loss is 0.0015504158800467849\n",
      "epoch: 3 step: 444, loss is 0.0053163981065154076\n",
      "epoch: 3 step: 445, loss is 0.0035572643391788006\n",
      "epoch: 3 step: 446, loss is 0.003929456230252981\n",
      "epoch: 3 step: 447, loss is 0.0016469585243612528\n",
      "epoch: 3 step: 448, loss is 0.0013216821243986487\n",
      "epoch: 3 step: 449, loss is 0.006692655850201845\n",
      "epoch: 3 step: 450, loss is 0.00023600022541359067\n",
      "epoch: 3 step: 451, loss is 0.003317576367408037\n",
      "epoch: 3 step: 452, loss is 0.0007226683665066957\n",
      "epoch: 3 step: 453, loss is 0.002164212753996253\n",
      "epoch: 3 step: 454, loss is 0.009178776293992996\n",
      "epoch: 3 step: 455, loss is 0.0007477860199287534\n",
      "epoch: 3 step: 456, loss is 0.0007674015359953046\n",
      "epoch: 3 step: 457, loss is 0.00035201775608584285\n",
      "epoch: 3 step: 458, loss is 0.0005283568170852959\n",
      "epoch: 3 step: 459, loss is 0.0006975220167078078\n",
      "epoch: 3 step: 460, loss is 0.0007638451643288136\n",
      "epoch: 3 step: 461, loss is 0.0005899899988435209\n",
      "epoch: 3 step: 462, loss is 0.0015078126452863216\n",
      "epoch: 3 step: 463, loss is 0.009056955575942993\n",
      "epoch: 3 step: 464, loss is 0.0008867497090250254\n",
      "epoch: 3 step: 465, loss is 0.0005217897705733776\n",
      "epoch: 3 step: 466, loss is 0.0025890804827213287\n",
      "epoch: 3 step: 467, loss is 0.0009940831223502755\n",
      "epoch: 3 step: 468, loss is 0.0008584127062931657\n",
      "epoch: 3 step: 469, loss is 0.002792049665004015\n",
      "epoch: 3 step: 470, loss is 0.0013544713146984577\n",
      "epoch: 3 step: 471, loss is 0.00048625876661390066\n",
      "epoch: 3 step: 472, loss is 0.0004298692801967263\n",
      "epoch: 3 step: 473, loss is 6.0660640883725137e-05\n",
      "epoch: 3 step: 474, loss is 0.0008420362137258053\n",
      "epoch: 3 step: 475, loss is 0.0031730541959404945\n",
      "epoch: 3 step: 476, loss is 0.0012899164576083422\n",
      "epoch: 3 step: 477, loss is 0.0005332622677087784\n",
      "epoch: 3 step: 478, loss is 0.0033732042647898197\n",
      "epoch: 3 step: 479, loss is 0.0005610345397144556\n",
      "epoch: 3 step: 480, loss is 0.0021954281255602837\n",
      "epoch: 3 step: 481, loss is 0.0005426357965916395\n",
      "epoch: 3 step: 482, loss is 0.0011373271699994802\n",
      "epoch: 3 step: 483, loss is 0.00044783909106627107\n",
      "epoch: 3 step: 484, loss is 0.0006494143744930625\n",
      "epoch: 3 step: 485, loss is 0.0005089645273983479\n",
      "epoch: 3 step: 486, loss is 0.0007482532528229058\n",
      "epoch: 3 step: 487, loss is 0.002001283224672079\n",
      "epoch: 3 step: 488, loss is 0.0008361453074030578\n",
      "epoch: 3 step: 489, loss is 0.0009603656362742186\n",
      "epoch: 3 step: 490, loss is 0.0013631799956783652\n",
      "epoch: 3 step: 491, loss is 0.0004930925206281245\n",
      "epoch: 3 step: 492, loss is 0.0003978838212788105\n",
      "epoch: 3 step: 493, loss is 0.001376804313622415\n",
      "epoch: 3 step: 494, loss is 0.00043988623656332493\n",
      "epoch: 3 step: 495, loss is 0.0012702515814453363\n",
      "epoch: 3 step: 496, loss is 0.0007240175036713481\n",
      "epoch: 3 step: 497, loss is 0.0006182129145599902\n",
      "epoch: 3 step: 498, loss is 0.0008995846146717668\n",
      "epoch: 3 step: 499, loss is 0.0075357044115662575\n",
      "epoch: 3 step: 500, loss is 0.001307563972659409\n",
      "epoch: 3 step: 501, loss is 0.0014920525718480349\n",
      "epoch: 3 step: 502, loss is 0.0015336021315306425\n",
      "epoch: 3 step: 503, loss is 0.0016545974649488926\n",
      "epoch: 3 step: 504, loss is 0.001528668450191617\n",
      "epoch: 3 step: 505, loss is 0.0012430772185325623\n",
      "epoch: 3 step: 506, loss is 0.007187743205577135\n",
      "epoch: 3 step: 507, loss is 0.0006020047003403306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 508, loss is 0.0027318489737808704\n",
      "epoch: 3 step: 509, loss is 0.0014585313620045781\n",
      "epoch: 3 step: 510, loss is 0.0005975640378892422\n",
      "epoch: 3 step: 511, loss is 0.002078673802316189\n",
      "epoch: 3 step: 512, loss is 0.002552863210439682\n",
      "epoch: 3 step: 513, loss is 0.002011217875406146\n",
      "epoch: 3 step: 514, loss is 0.0012101861648261547\n",
      "epoch: 3 step: 515, loss is 0.027787478640675545\n",
      "epoch: 3 step: 516, loss is 0.007723640184849501\n",
      "epoch: 3 step: 517, loss is 0.0008014862542040646\n",
      "epoch: 3 step: 518, loss is 0.0006870978395454586\n",
      "epoch: 3 step: 519, loss is 0.0013966740807518363\n",
      "epoch: 3 step: 520, loss is 0.005327657330781221\n",
      "epoch: 3 step: 521, loss is 0.0007668447215110064\n",
      "epoch: 3 step: 522, loss is 0.0009921382879838347\n",
      "epoch: 3 step: 523, loss is 0.0009937757859006524\n",
      "epoch: 3 step: 524, loss is 0.001851545413956046\n",
      "epoch: 3 step: 525, loss is 0.004544113762676716\n",
      "epoch: 3 step: 526, loss is 0.000862718210555613\n",
      "epoch: 3 step: 527, loss is 0.00107235973700881\n",
      "epoch: 3 step: 528, loss is 0.000783995958045125\n",
      "epoch: 3 step: 529, loss is 0.003638572758063674\n",
      "epoch: 3 step: 530, loss is 0.0006329115713015199\n",
      "epoch: 3 step: 531, loss is 0.0008339433697983623\n",
      "epoch: 3 step: 532, loss is 0.0005427164724096656\n",
      "epoch: 3 step: 533, loss is 0.0007446715608239174\n",
      "epoch: 3 step: 534, loss is 0.0002882172411773354\n",
      "epoch: 3 step: 535, loss is 0.0009849965572357178\n",
      "epoch: 3 step: 536, loss is 0.0003286898136138916\n",
      "epoch: 3 step: 537, loss is 0.0018894114764407277\n",
      "epoch: 3 step: 538, loss is 0.0014776636380702257\n",
      "epoch: 3 step: 539, loss is 0.0013108211569488049\n",
      "epoch: 3 step: 540, loss is 0.0003480835002847016\n",
      "epoch: 3 step: 541, loss is 0.0005438479129225016\n",
      "epoch: 3 step: 542, loss is 0.00045748683623969555\n",
      "epoch: 3 step: 543, loss is 0.0006067652720957994\n",
      "epoch: 3 step: 544, loss is 0.0020897737704217434\n",
      "epoch: 3 step: 545, loss is 0.0015513405669480562\n",
      "epoch: 3 step: 546, loss is 0.0017126656603068113\n",
      "epoch: 3 step: 547, loss is 0.00042370345909148455\n",
      "epoch: 3 step: 548, loss is 0.0011777011677622795\n",
      "epoch: 3 step: 549, loss is 0.0007608765736222267\n",
      "epoch: 3 step: 550, loss is 0.0012794571230188012\n",
      "epoch: 3 step: 551, loss is 0.001050841761752963\n",
      "epoch: 3 step: 552, loss is 0.000619790400378406\n",
      "epoch: 3 step: 553, loss is 0.0010972723830491304\n",
      "epoch: 3 step: 554, loss is 0.0002690737892407924\n",
      "epoch: 3 step: 555, loss is 0.0026316686999052763\n",
      "epoch: 3 step: 556, loss is 0.0009651023428887129\n",
      "epoch: 3 step: 557, loss is 0.003199343802407384\n",
      "epoch: 3 step: 558, loss is 0.0027294778265058994\n",
      "epoch: 3 step: 559, loss is 0.0005973149673081934\n",
      "epoch: 3 step: 560, loss is 0.0007201331900432706\n",
      "epoch: 3 step: 561, loss is 0.001110312296077609\n",
      "epoch: 3 step: 562, loss is 0.002245450159534812\n",
      "epoch: 3 step: 563, loss is 0.0008435380877926946\n",
      "epoch: 3 step: 564, loss is 0.0015123388729989529\n",
      "epoch: 3 step: 565, loss is 0.0006660895887762308\n",
      "epoch: 3 step: 566, loss is 0.0006360337138175964\n",
      "epoch: 3 step: 567, loss is 0.0028672872576862574\n",
      "epoch: 3 step: 568, loss is 0.0006608735420741141\n",
      "epoch: 3 step: 569, loss is 0.0019680792465806007\n",
      "epoch: 3 step: 570, loss is 0.0011698241578415036\n",
      "epoch: 3 step: 571, loss is 0.003875021357089281\n",
      "epoch: 3 step: 572, loss is 0.0007398286834359169\n",
      "epoch: 3 step: 573, loss is 0.0007938260096125305\n",
      "epoch: 3 step: 574, loss is 0.0020520417019724846\n",
      "epoch: 3 step: 575, loss is 0.0009292511967942119\n",
      "epoch: 3 step: 576, loss is 0.00047950787120498717\n",
      "epoch: 3 step: 577, loss is 0.023330582305788994\n",
      "epoch: 3 step: 578, loss is 0.0007424911018460989\n",
      "epoch: 3 step: 579, loss is 0.0008575693354941905\n",
      "epoch: 3 step: 580, loss is 0.0007842671475373209\n",
      "epoch: 3 step: 581, loss is 0.0005906530423089862\n",
      "epoch: 3 step: 582, loss is 0.001043198280967772\n",
      "epoch: 3 step: 583, loss is 0.0004956487100571394\n",
      "epoch: 3 step: 584, loss is 0.00031426287023350596\n",
      "epoch: 3 step: 585, loss is 0.000692998874001205\n",
      "epoch: 3 step: 586, loss is 0.0003023038152605295\n",
      "epoch: 3 step: 587, loss is 0.00030263897497206926\n",
      "epoch: 3 step: 588, loss is 0.0005906299338676035\n",
      "epoch: 3 step: 589, loss is 0.002639580750837922\n",
      "epoch: 3 step: 590, loss is 0.0008867866126820445\n",
      "epoch: 3 step: 591, loss is 0.00044215406524017453\n",
      "epoch: 3 step: 592, loss is 0.0011248223017901182\n",
      "epoch: 3 step: 593, loss is 0.03223251923918724\n",
      "epoch: 3 step: 594, loss is 0.0014522225828841329\n",
      "epoch: 3 step: 595, loss is 0.0005061744013801217\n",
      "epoch: 3 step: 596, loss is 0.0006420020945370197\n",
      "Train epoch time: 107609.707 ms, per step time: 180.553 ms\n",
      "epoch: 4 step: 1, loss is 0.0009142954950220883\n",
      "epoch: 4 step: 2, loss is 0.003046823898330331\n",
      "epoch: 4 step: 3, loss is 0.009635218419134617\n",
      "epoch: 4 step: 4, loss is 0.0005248073721304536\n",
      "epoch: 4 step: 5, loss is 0.0024534850381314754\n",
      "epoch: 4 step: 6, loss is 0.002642877632752061\n",
      "epoch: 4 step: 7, loss is 0.0002580065920483321\n",
      "epoch: 4 step: 8, loss is 0.0007624026038683951\n",
      "epoch: 4 step: 9, loss is 0.0007069170824252069\n",
      "epoch: 4 step: 10, loss is 0.00040262890979647636\n",
      "epoch: 4 step: 11, loss is 0.004467953462153673\n",
      "epoch: 4 step: 12, loss is 0.0005477967206388712\n",
      "epoch: 4 step: 13, loss is 0.0003640004142653197\n",
      "epoch: 4 step: 14, loss is 0.0010486713144928217\n",
      "epoch: 4 step: 15, loss is 0.0029958912637084723\n",
      "epoch: 4 step: 16, loss is 0.0006303200498223305\n",
      "epoch: 4 step: 17, loss is 0.0004624768625944853\n",
      "epoch: 4 step: 18, loss is 0.00037419036380015314\n",
      "epoch: 4 step: 19, loss is 0.0016861450858414173\n",
      "epoch: 4 step: 20, loss is 0.002050942974165082\n",
      "epoch: 4 step: 21, loss is 0.001070409081876278\n",
      "epoch: 4 step: 22, loss is 0.0009929290972650051\n",
      "epoch: 4 step: 23, loss is 0.0009425576427020133\n",
      "epoch: 4 step: 24, loss is 0.000657839176710695\n",
      "epoch: 4 step: 25, loss is 0.0019837147556245327\n",
      "epoch: 4 step: 26, loss is 0.00036299132625572383\n",
      "epoch: 4 step: 27, loss is 0.0005606317427009344\n",
      "epoch: 4 step: 28, loss is 0.0017554914811626077\n",
      "epoch: 4 step: 29, loss is 0.0011244764318689704\n",
      "epoch: 4 step: 30, loss is 0.002533746650442481\n",
      "epoch: 4 step: 31, loss is 0.0024335135240107775\n",
      "epoch: 4 step: 32, loss is 0.0002329777053091675\n",
      "epoch: 4 step: 33, loss is 0.0012655844911932945\n",
      "epoch: 4 step: 34, loss is 0.0005149514181539416\n",
      "epoch: 4 step: 35, loss is 0.0004552664759103209\n",
      "epoch: 4 step: 36, loss is 0.00016465887892991304\n",
      "epoch: 4 step: 37, loss is 0.0005922347772866488\n",
      "epoch: 4 step: 38, loss is 0.0007404500502161682\n",
      "epoch: 4 step: 39, loss is 0.00161882140673697\n",
      "epoch: 4 step: 40, loss is 0.0004290947108529508\n",
      "epoch: 4 step: 41, loss is 0.0005305074737407267\n",
      "epoch: 4 step: 42, loss is 0.0008720540208742023\n",
      "epoch: 4 step: 43, loss is 0.0003772888449020684\n",
      "epoch: 4 step: 44, loss is 0.001940890564583242\n",
      "epoch: 4 step: 45, loss is 0.0007447050302289426\n",
      "epoch: 4 step: 46, loss is 0.00022378975700121373\n",
      "epoch: 4 step: 47, loss is 0.0003558274474926293\n",
      "epoch: 4 step: 48, loss is 0.0015217297477647662\n",
      "epoch: 4 step: 49, loss is 0.0008466190192848444\n",
      "epoch: 4 step: 50, loss is 0.000625082291662693\n",
      "epoch: 4 step: 51, loss is 0.0014907432487234473\n",
      "epoch: 4 step: 52, loss is 0.000402105157263577\n",
      "epoch: 4 step: 53, loss is 0.0015990540850907564\n",
      "epoch: 4 step: 54, loss is 0.0003535661962814629\n",
      "epoch: 4 step: 55, loss is 0.0007122404640540481\n",
      "epoch: 4 step: 56, loss is 0.01415279321372509\n",
      "epoch: 4 step: 57, loss is 0.001303099561482668\n",
      "epoch: 4 step: 58, loss is 0.000645524705760181\n",
      "epoch: 4 step: 59, loss is 0.0010729115456342697\n",
      "epoch: 4 step: 60, loss is 0.0008399456273764372\n",
      "epoch: 4 step: 61, loss is 0.0013203986454755068\n",
      "epoch: 4 step: 62, loss is 0.001362312352284789\n",
      "epoch: 4 step: 63, loss is 0.0006693066097795963\n",
      "epoch: 4 step: 64, loss is 0.0029344663489609957\n",
      "epoch: 4 step: 65, loss is 0.0013413236010819674\n",
      "epoch: 4 step: 66, loss is 0.0010628728196024895\n",
      "epoch: 4 step: 67, loss is 0.00038459576899185777\n",
      "epoch: 4 step: 68, loss is 0.0009198971674777567\n",
      "epoch: 4 step: 69, loss is 0.001733400160446763\n",
      "epoch: 4 step: 70, loss is 0.000506544834934175\n",
      "epoch: 4 step: 71, loss is 0.0004084639367647469\n",
      "epoch: 4 step: 72, loss is 0.0019134852336719632\n",
      "epoch: 4 step: 73, loss is 0.006527891382575035\n",
      "epoch: 4 step: 74, loss is 0.0011756967287510633\n",
      "epoch: 4 step: 75, loss is 0.00061663897940889\n",
      "epoch: 4 step: 76, loss is 0.0004813377163372934\n",
      "epoch: 4 step: 77, loss is 0.0005715580191463232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 78, loss is 0.0005862310063093901\n",
      "epoch: 4 step: 79, loss is 0.0008188940701074898\n",
      "epoch: 4 step: 80, loss is 0.0006333521450869739\n",
      "epoch: 4 step: 81, loss is 0.0014927085721865296\n",
      "epoch: 4 step: 82, loss is 0.001058823661878705\n",
      "epoch: 4 step: 83, loss is 0.0005476605147123337\n",
      "epoch: 4 step: 84, loss is 0.0007796216523274779\n",
      "epoch: 4 step: 85, loss is 0.0016875698929652572\n",
      "epoch: 4 step: 86, loss is 0.005690303631126881\n",
      "epoch: 4 step: 87, loss is 0.00018843142606783658\n",
      "epoch: 4 step: 88, loss is 0.0006588185206055641\n",
      "epoch: 4 step: 89, loss is 0.0004437825991772115\n",
      "epoch: 4 step: 90, loss is 0.0016883539501577616\n",
      "epoch: 4 step: 91, loss is 0.0025844324845820665\n",
      "epoch: 4 step: 92, loss is 0.0010033224243670702\n",
      "epoch: 4 step: 93, loss is 0.0008838747162371874\n",
      "epoch: 4 step: 94, loss is 0.000800902140326798\n",
      "epoch: 4 step: 95, loss is 0.00032166732125915587\n",
      "epoch: 4 step: 96, loss is 0.0007417488377541304\n",
      "epoch: 4 step: 97, loss is 0.0019153701141476631\n",
      "epoch: 4 step: 98, loss is 0.0003854558744933456\n",
      "epoch: 4 step: 99, loss is 0.0005894367350265384\n",
      "epoch: 4 step: 100, loss is 0.0005030228057876229\n",
      "epoch: 4 step: 101, loss is 0.0009511513053439558\n",
      "epoch: 4 step: 102, loss is 0.0004679044068325311\n",
      "epoch: 4 step: 103, loss is 0.0009479416767135262\n",
      "epoch: 4 step: 104, loss is 0.0008051480399444699\n",
      "epoch: 4 step: 105, loss is 0.0005930374609306455\n",
      "epoch: 4 step: 106, loss is 0.0009320933022536337\n",
      "epoch: 4 step: 107, loss is 0.0001876296882983297\n",
      "epoch: 4 step: 108, loss is 0.0004944012616761029\n",
      "epoch: 4 step: 109, loss is 0.0007027250248938799\n",
      "epoch: 4 step: 110, loss is 0.000769182457588613\n",
      "epoch: 4 step: 111, loss is 0.007095756474882364\n",
      "epoch: 4 step: 112, loss is 0.000515692459885031\n",
      "epoch: 4 step: 113, loss is 0.00026076758513227105\n",
      "epoch: 4 step: 114, loss is 0.00029591776547022164\n",
      "epoch: 4 step: 115, loss is 0.0005780344363301992\n",
      "epoch: 4 step: 116, loss is 0.000623938802164048\n",
      "epoch: 4 step: 117, loss is 0.0008725497173145413\n",
      "epoch: 4 step: 118, loss is 0.004367093555629253\n",
      "epoch: 4 step: 119, loss is 0.00018547711079008877\n",
      "epoch: 4 step: 120, loss is 0.0015095252310857177\n",
      "epoch: 4 step: 121, loss is 0.0011870948364958167\n",
      "epoch: 4 step: 122, loss is 0.000856257975101471\n",
      "epoch: 4 step: 123, loss is 0.0010270039783790708\n",
      "epoch: 4 step: 124, loss is 0.0011457030195742846\n",
      "epoch: 4 step: 125, loss is 0.0004383389023132622\n",
      "epoch: 4 step: 126, loss is 0.000932093826122582\n",
      "epoch: 4 step: 127, loss is 0.002277601510286331\n",
      "epoch: 4 step: 128, loss is 0.0006445947801694274\n",
      "epoch: 4 step: 129, loss is 0.00014914627536199987\n",
      "epoch: 4 step: 130, loss is 0.001609350903891027\n",
      "epoch: 4 step: 131, loss is 0.0008022484835237265\n",
      "epoch: 4 step: 132, loss is 0.0008307733223773539\n",
      "epoch: 4 step: 133, loss is 0.0006044461042620242\n",
      "epoch: 4 step: 134, loss is 0.0006331295007839799\n",
      "epoch: 4 step: 135, loss is 0.00038411913556046784\n",
      "epoch: 4 step: 136, loss is 0.0001732232776703313\n",
      "epoch: 4 step: 137, loss is 0.00027824309654533863\n",
      "epoch: 4 step: 138, loss is 0.0009674387983977795\n",
      "epoch: 4 step: 139, loss is 0.00040521970367990434\n",
      "epoch: 4 step: 140, loss is 0.00033906297176145017\n",
      "epoch: 4 step: 141, loss is 0.0010164690902456641\n",
      "epoch: 4 step: 142, loss is 0.0006822473369538784\n",
      "epoch: 4 step: 143, loss is 0.002360440557822585\n",
      "epoch: 4 step: 144, loss is 0.00044929940486326814\n",
      "epoch: 4 step: 145, loss is 0.000634176773019135\n",
      "epoch: 4 step: 146, loss is 0.0021513502579182386\n",
      "epoch: 4 step: 147, loss is 0.0001736709091346711\n",
      "epoch: 4 step: 148, loss is 0.0007737250998616219\n",
      "epoch: 4 step: 149, loss is 0.000751820916775614\n",
      "epoch: 4 step: 150, loss is 0.000625861226581037\n",
      "epoch: 4 step: 151, loss is 0.0023334308061748743\n",
      "epoch: 4 step: 152, loss is 0.004164912737905979\n",
      "epoch: 4 step: 153, loss is 0.0004598891537170857\n",
      "epoch: 4 step: 154, loss is 0.003828444518148899\n",
      "epoch: 4 step: 155, loss is 0.0010340780718252063\n",
      "epoch: 4 step: 156, loss is 0.0003496775752864778\n",
      "epoch: 4 step: 157, loss is 0.0010596651118248701\n",
      "epoch: 4 step: 158, loss is 0.001573161338455975\n",
      "epoch: 4 step: 159, loss is 0.00044641830027103424\n",
      "epoch: 4 step: 160, loss is 0.005729805212467909\n",
      "epoch: 4 step: 161, loss is 0.00031972461147233844\n",
      "epoch: 4 step: 162, loss is 0.0008904426940716803\n",
      "epoch: 4 step: 163, loss is 0.0008974896045401692\n",
      "epoch: 4 step: 164, loss is 0.0032975024078041315\n",
      "epoch: 4 step: 165, loss is 0.0007822875631973147\n",
      "epoch: 4 step: 166, loss is 0.00017413287423551083\n",
      "epoch: 4 step: 167, loss is 0.0002919343241956085\n",
      "epoch: 4 step: 168, loss is 0.01664220355451107\n",
      "epoch: 4 step: 169, loss is 0.0008714733994565904\n",
      "epoch: 4 step: 170, loss is 0.001014637527987361\n",
      "epoch: 4 step: 171, loss is 0.001956073334440589\n",
      "epoch: 4 step: 172, loss is 0.0011931722983717918\n",
      "epoch: 4 step: 173, loss is 0.0002891938202083111\n",
      "epoch: 4 step: 174, loss is 0.0006558835157193244\n",
      "epoch: 4 step: 175, loss is 0.0002007882430916652\n",
      "epoch: 4 step: 176, loss is 0.0005095397355034947\n",
      "epoch: 4 step: 177, loss is 0.003315888112410903\n",
      "epoch: 4 step: 178, loss is 0.0015259430510923266\n",
      "epoch: 4 step: 179, loss is 0.0010530317667871714\n",
      "epoch: 4 step: 180, loss is 0.0013267743634060025\n",
      "epoch: 4 step: 181, loss is 0.0009746692376211286\n",
      "epoch: 4 step: 182, loss is 0.001088770804926753\n",
      "epoch: 4 step: 183, loss is 0.0009798657847568393\n",
      "epoch: 4 step: 184, loss is 0.00037427450297400355\n",
      "epoch: 4 step: 185, loss is 0.0001331157109234482\n",
      "epoch: 4 step: 186, loss is 0.00035636487882584333\n",
      "epoch: 4 step: 187, loss is 0.00041679502464830875\n",
      "epoch: 4 step: 188, loss is 0.000660315272398293\n",
      "epoch: 4 step: 189, loss is 0.0005478904931806028\n",
      "epoch: 4 step: 190, loss is 0.002892888616770506\n",
      "epoch: 4 step: 191, loss is 0.001142792054452002\n",
      "epoch: 4 step: 192, loss is 0.0005328434053808451\n",
      "epoch: 4 step: 193, loss is 0.0008885248098522425\n",
      "epoch: 4 step: 194, loss is 0.0010123468236997724\n",
      "epoch: 4 step: 195, loss is 0.0009581678896211088\n",
      "epoch: 4 step: 196, loss is 0.00034936692100018263\n",
      "epoch: 4 step: 197, loss is 0.0008231998072005808\n",
      "epoch: 4 step: 198, loss is 0.000943561433814466\n",
      "epoch: 4 step: 199, loss is 0.0008974042139016092\n",
      "epoch: 4 step: 200, loss is 0.0013265479356050491\n",
      "epoch: 4 step: 201, loss is 0.00045803480315953493\n",
      "epoch: 4 step: 202, loss is 0.0016436659498140216\n",
      "epoch: 4 step: 203, loss is 0.0003164323279634118\n",
      "epoch: 4 step: 204, loss is 0.0009362507844343781\n",
      "epoch: 4 step: 205, loss is 0.013993816450238228\n",
      "epoch: 4 step: 206, loss is 0.0008015755447559059\n",
      "epoch: 4 step: 207, loss is 0.00041514489566907287\n",
      "epoch: 4 step: 208, loss is 0.0010629664175212383\n",
      "epoch: 4 step: 209, loss is 0.0005376353510655463\n",
      "epoch: 4 step: 210, loss is 0.0007667886093258858\n",
      "epoch: 4 step: 211, loss is 0.0011817357735708356\n",
      "epoch: 4 step: 212, loss is 0.0010593973565846682\n",
      "epoch: 4 step: 213, loss is 0.0033541403245180845\n",
      "epoch: 4 step: 214, loss is 0.0006747805746272206\n",
      "epoch: 4 step: 215, loss is 0.0006401202408596873\n",
      "epoch: 4 step: 216, loss is 0.00045736529864370823\n",
      "epoch: 4 step: 217, loss is 0.0031833539251238108\n",
      "epoch: 4 step: 218, loss is 0.0029938844963908195\n",
      "epoch: 4 step: 219, loss is 0.0003948390658479184\n",
      "epoch: 4 step: 220, loss is 0.00027618318563327193\n",
      "epoch: 4 step: 221, loss is 0.0012964444467797875\n",
      "epoch: 4 step: 222, loss is 0.006777321919798851\n",
      "epoch: 4 step: 223, loss is 0.0017044589621946216\n",
      "epoch: 4 step: 224, loss is 0.0005427670548669994\n",
      "epoch: 4 step: 225, loss is 0.00019486773817334324\n",
      "epoch: 4 step: 226, loss is 0.0001299607683904469\n",
      "epoch: 4 step: 227, loss is 0.0007714357343502343\n",
      "epoch: 4 step: 228, loss is 0.0007171423058025539\n",
      "epoch: 4 step: 229, loss is 0.0005763980443589389\n",
      "epoch: 4 step: 230, loss is 0.0022485130466520786\n",
      "epoch: 4 step: 231, loss is 0.0010118132922798395\n",
      "epoch: 4 step: 232, loss is 0.00029636736144311726\n",
      "epoch: 4 step: 233, loss is 0.0008149959030561149\n",
      "epoch: 4 step: 234, loss is 0.0008908815216273069\n",
      "epoch: 4 step: 235, loss is 0.0008069881587289274\n",
      "epoch: 4 step: 236, loss is 0.00024164168280549347\n",
      "epoch: 4 step: 237, loss is 0.0008929408504627645\n",
      "epoch: 4 step: 238, loss is 0.00044695776887238026\n",
      "epoch: 4 step: 239, loss is 0.0008732381975278258\n",
      "epoch: 4 step: 240, loss is 0.004191181622445583\n",
      "epoch: 4 step: 241, loss is 0.0004882935900241137\n",
      "epoch: 4 step: 242, loss is 0.0004515601322054863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 243, loss is 0.00035363645292818546\n",
      "epoch: 4 step: 244, loss is 0.00043834804091602564\n",
      "epoch: 4 step: 245, loss is 0.0005638175643980503\n",
      "epoch: 4 step: 246, loss is 0.0021363685373216867\n",
      "epoch: 4 step: 247, loss is 0.0003540718462318182\n",
      "epoch: 4 step: 248, loss is 0.0006291775498539209\n",
      "epoch: 4 step: 249, loss is 0.0005637489375658333\n",
      "epoch: 4 step: 250, loss is 0.0006868296768516302\n",
      "epoch: 4 step: 251, loss is 0.000555549340788275\n",
      "epoch: 4 step: 252, loss is 0.0017904802225530148\n",
      "epoch: 4 step: 253, loss is 0.0009321554680354893\n",
      "epoch: 4 step: 254, loss is 0.0006437670672312379\n",
      "epoch: 4 step: 255, loss is 0.0006579551263712347\n",
      "epoch: 4 step: 256, loss is 0.00014764275692868978\n",
      "epoch: 4 step: 257, loss is 0.0005463006673380733\n",
      "epoch: 4 step: 258, loss is 0.0008140108548104763\n",
      "epoch: 4 step: 259, loss is 0.0011077006347477436\n",
      "epoch: 4 step: 260, loss is 0.0003425783070269972\n",
      "epoch: 4 step: 261, loss is 0.00029060401720926166\n",
      "epoch: 4 step: 262, loss is 0.0006650976138189435\n",
      "epoch: 4 step: 263, loss is 0.000705489597748965\n",
      "epoch: 4 step: 264, loss is 0.001518881763331592\n",
      "epoch: 4 step: 265, loss is 0.0006900521693751216\n",
      "epoch: 4 step: 266, loss is 0.0007702646544203162\n",
      "epoch: 4 step: 267, loss is 0.008615625090897083\n",
      "epoch: 4 step: 268, loss is 0.00014601708971895278\n",
      "epoch: 4 step: 269, loss is 0.003511239541694522\n",
      "epoch: 4 step: 270, loss is 0.0008569155470468104\n",
      "epoch: 4 step: 271, loss is 0.0006672308081761003\n",
      "epoch: 4 step: 272, loss is 0.0010659706313163042\n",
      "epoch: 4 step: 273, loss is 0.0017246261704713106\n",
      "epoch: 4 step: 274, loss is 0.00100582221057266\n",
      "epoch: 4 step: 275, loss is 0.0009536933503113687\n",
      "epoch: 4 step: 276, loss is 0.002009797841310501\n",
      "epoch: 4 step: 277, loss is 0.0014071585610508919\n",
      "epoch: 4 step: 278, loss is 0.00018064584583044052\n",
      "epoch: 4 step: 279, loss is 0.0010396521538496017\n",
      "epoch: 4 step: 280, loss is 0.0012013695668429136\n",
      "epoch: 4 step: 281, loss is 0.0010444854851812124\n",
      "epoch: 4 step: 282, loss is 0.0006610105629079044\n",
      "epoch: 4 step: 283, loss is 0.0009396392852067947\n",
      "epoch: 4 step: 284, loss is 0.0015921765007078648\n",
      "epoch: 4 step: 285, loss is 0.00030501920264214277\n",
      "epoch: 4 step: 286, loss is 0.000134569316287525\n",
      "epoch: 4 step: 287, loss is 0.0004893664154224098\n",
      "epoch: 4 step: 288, loss is 0.00044559192610904574\n",
      "epoch: 4 step: 289, loss is 0.00020306874648667872\n",
      "epoch: 4 step: 290, loss is 0.0005603173049166799\n",
      "epoch: 4 step: 291, loss is 0.0012678082566708326\n",
      "epoch: 4 step: 292, loss is 0.00141428061760962\n",
      "epoch: 4 step: 293, loss is 0.00039083280717022717\n",
      "epoch: 4 step: 294, loss is 0.0008141522412188351\n",
      "epoch: 4 step: 295, loss is 0.0008585242903791368\n",
      "epoch: 4 step: 296, loss is 0.003325565718114376\n",
      "epoch: 4 step: 297, loss is 0.0008051779586821795\n",
      "epoch: 4 step: 298, loss is 0.000593477045185864\n",
      "epoch: 4 step: 299, loss is 0.0011295306030660868\n",
      "epoch: 4 step: 300, loss is 0.012821401469409466\n",
      "epoch: 4 step: 301, loss is 0.000663451966829598\n",
      "epoch: 4 step: 302, loss is 0.0005999123677611351\n",
      "epoch: 4 step: 303, loss is 0.0021664779633283615\n",
      "epoch: 4 step: 304, loss is 0.0005676995497196913\n",
      "epoch: 4 step: 305, loss is 0.00045002764090895653\n",
      "epoch: 4 step: 306, loss is 0.00046616472536697984\n",
      "epoch: 4 step: 307, loss is 0.0006589104887098074\n",
      "epoch: 4 step: 308, loss is 0.0005564183229580522\n",
      "epoch: 4 step: 309, loss is 0.007395726162940264\n",
      "epoch: 4 step: 310, loss is 0.00037377793341875076\n",
      "epoch: 4 step: 311, loss is 0.0006555027794092894\n",
      "epoch: 4 step: 312, loss is 0.0023792346473783255\n",
      "epoch: 4 step: 313, loss is 0.0016436214791610837\n",
      "epoch: 4 step: 314, loss is 0.0006113286945037544\n",
      "epoch: 4 step: 315, loss is 0.00015893191448412836\n",
      "epoch: 4 step: 316, loss is 0.0005074740620329976\n",
      "epoch: 4 step: 317, loss is 0.0013862395426258445\n",
      "epoch: 4 step: 318, loss is 0.00017299683531746268\n",
      "epoch: 4 step: 319, loss is 0.0016756788827478886\n",
      "epoch: 4 step: 320, loss is 0.002007797360420227\n",
      "epoch: 4 step: 321, loss is 0.0012953844852745533\n",
      "epoch: 4 step: 322, loss is 0.0003426017938181758\n",
      "epoch: 4 step: 323, loss is 0.0007923561497591436\n",
      "epoch: 4 step: 324, loss is 7.378837472060695e-05\n",
      "epoch: 4 step: 325, loss is 0.0012860920978710055\n",
      "epoch: 4 step: 326, loss is 0.0010623265989124775\n",
      "epoch: 4 step: 327, loss is 0.0028220145031809807\n",
      "epoch: 4 step: 328, loss is 0.0010938956402242184\n",
      "epoch: 4 step: 329, loss is 0.0007550865411758423\n",
      "epoch: 4 step: 330, loss is 0.0009578641038388014\n",
      "epoch: 4 step: 331, loss is 0.0012338297674432397\n",
      "epoch: 4 step: 332, loss is 0.0007347209611907601\n",
      "epoch: 4 step: 333, loss is 0.0004953363677486777\n",
      "epoch: 4 step: 334, loss is 0.00026715098647400737\n",
      "epoch: 4 step: 335, loss is 0.00041639135451987386\n",
      "epoch: 4 step: 336, loss is 0.0004378723388072103\n",
      "epoch: 4 step: 337, loss is 0.0007587599102407694\n",
      "epoch: 4 step: 338, loss is 0.0005641900934278965\n",
      "epoch: 4 step: 339, loss is 0.0007475165766663849\n",
      "epoch: 4 step: 340, loss is 0.0007686924654990435\n",
      "epoch: 4 step: 341, loss is 0.0006704795523546636\n",
      "epoch: 4 step: 342, loss is 0.0003283732512500137\n",
      "epoch: 4 step: 343, loss is 0.019582556560635567\n",
      "epoch: 4 step: 344, loss is 0.0013574291951954365\n",
      "epoch: 4 step: 345, loss is 0.0007819208549335599\n",
      "epoch: 4 step: 346, loss is 0.00104866037145257\n",
      "epoch: 4 step: 347, loss is 0.001077866880223155\n",
      "epoch: 4 step: 348, loss is 0.0004306719929445535\n",
      "epoch: 4 step: 349, loss is 0.0012904254253953695\n",
      "epoch: 4 step: 350, loss is 0.000829768949188292\n",
      "epoch: 4 step: 351, loss is 0.0013252458302304149\n",
      "epoch: 4 step: 352, loss is 0.00027402883279137313\n",
      "epoch: 4 step: 353, loss is 0.0009091790998354554\n",
      "epoch: 4 step: 354, loss is 0.00271729938685894\n",
      "epoch: 4 step: 355, loss is 0.0008435870986431837\n",
      "epoch: 4 step: 356, loss is 0.0006939296144992113\n",
      "epoch: 4 step: 357, loss is 0.0004953774623572826\n",
      "epoch: 4 step: 358, loss is 0.00045922212302684784\n",
      "epoch: 4 step: 359, loss is 0.0006046918570064008\n",
      "epoch: 4 step: 360, loss is 0.0022799819707870483\n",
      "epoch: 4 step: 361, loss is 0.0010732661467045546\n",
      "epoch: 4 step: 362, loss is 0.002455702982842922\n",
      "epoch: 4 step: 363, loss is 0.0003271770547144115\n",
      "epoch: 4 step: 364, loss is 0.0003851679211948067\n",
      "epoch: 4 step: 365, loss is 0.000659979647025466\n",
      "epoch: 4 step: 366, loss is 0.0011860972736030817\n",
      "epoch: 4 step: 367, loss is 0.0012503411853685975\n",
      "epoch: 4 step: 368, loss is 0.0003735572681762278\n",
      "epoch: 4 step: 369, loss is 0.003158202627673745\n",
      "epoch: 4 step: 370, loss is 0.0009216485777869821\n",
      "epoch: 4 step: 371, loss is 0.0049514975398778915\n",
      "epoch: 4 step: 372, loss is 0.0010773164685815573\n",
      "epoch: 4 step: 373, loss is 0.0005409555742517114\n",
      "epoch: 4 step: 374, loss is 0.00018437570543028414\n",
      "epoch: 4 step: 375, loss is 0.00021692272275686264\n",
      "epoch: 4 step: 376, loss is 0.0014644123148173094\n",
      "epoch: 4 step: 377, loss is 0.0017708437517285347\n",
      "epoch: 4 step: 378, loss is 0.0006452279631048441\n",
      "epoch: 4 step: 379, loss is 0.00047316416748799384\n",
      "epoch: 4 step: 380, loss is 0.00037357519613578916\n",
      "epoch: 4 step: 381, loss is 0.0011055240174755454\n",
      "epoch: 4 step: 382, loss is 0.0010157612850889564\n",
      "epoch: 4 step: 383, loss is 0.0005522998981177807\n",
      "epoch: 4 step: 384, loss is 0.0012927664211019874\n",
      "epoch: 4 step: 385, loss is 8.955507655628026e-05\n",
      "epoch: 4 step: 386, loss is 0.0008354339515790343\n",
      "epoch: 4 step: 387, loss is 0.00020890551968477666\n",
      "epoch: 4 step: 388, loss is 0.0005272637936286628\n",
      "epoch: 4 step: 389, loss is 0.0021488370839506388\n",
      "epoch: 4 step: 390, loss is 0.0007793541881255805\n",
      "epoch: 4 step: 391, loss is 0.021352730691432953\n",
      "epoch: 4 step: 392, loss is 0.00038312957622110844\n",
      "epoch: 4 step: 393, loss is 0.0005409122677519917\n",
      "epoch: 4 step: 394, loss is 0.0002223402407253161\n",
      "epoch: 4 step: 395, loss is 0.0010430235415697098\n",
      "epoch: 4 step: 396, loss is 0.00024084310280159116\n",
      "epoch: 4 step: 397, loss is 0.0005837404751218855\n",
      "epoch: 4 step: 398, loss is 0.0005942738498561084\n",
      "epoch: 4 step: 399, loss is 0.001762498402968049\n",
      "epoch: 4 step: 400, loss is 0.0005401589442044497\n",
      "epoch: 4 step: 401, loss is 0.0014970777556300163\n",
      "epoch: 4 step: 402, loss is 0.000592562893871218\n",
      "epoch: 4 step: 403, loss is 0.0007796477875672281\n",
      "epoch: 4 step: 404, loss is 0.0006197623442858458\n",
      "epoch: 4 step: 405, loss is 0.00018066268239635974\n",
      "epoch: 4 step: 406, loss is 0.0004941257066093385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 407, loss is 0.000577404978685081\n",
      "epoch: 4 step: 408, loss is 0.0007110236911103129\n",
      "epoch: 4 step: 409, loss is 0.002597525017336011\n",
      "epoch: 4 step: 410, loss is 0.0016517096664756536\n",
      "epoch: 4 step: 411, loss is 0.00016827831859700382\n",
      "epoch: 4 step: 412, loss is 0.0005503942957147956\n",
      "epoch: 4 step: 413, loss is 0.0005697251763194799\n",
      "epoch: 4 step: 414, loss is 0.0007295742980204523\n",
      "epoch: 4 step: 415, loss is 0.0005466490983963013\n",
      "epoch: 4 step: 416, loss is 0.0026768811512738466\n",
      "epoch: 4 step: 417, loss is 0.0002832973259501159\n",
      "epoch: 4 step: 418, loss is 0.00123522337526083\n",
      "epoch: 4 step: 419, loss is 0.0006161880446597934\n",
      "epoch: 4 step: 420, loss is 0.0009034522227011621\n",
      "epoch: 4 step: 421, loss is 0.0005314509035088122\n",
      "epoch: 4 step: 422, loss is 0.0009445046889595687\n",
      "epoch: 4 step: 423, loss is 0.0003889875370077789\n",
      "epoch: 4 step: 424, loss is 0.0026963362470269203\n",
      "epoch: 4 step: 425, loss is 0.0017353224102407694\n",
      "epoch: 4 step: 426, loss is 0.0017053545452654362\n",
      "epoch: 4 step: 427, loss is 0.00015251887089107186\n",
      "epoch: 4 step: 428, loss is 0.0020297032315284014\n",
      "epoch: 4 step: 429, loss is 0.000759473885409534\n",
      "epoch: 4 step: 430, loss is 0.00034979655174538493\n",
      "epoch: 4 step: 431, loss is 0.0009006557520478964\n",
      "epoch: 4 step: 432, loss is 0.0005620936863124371\n",
      "epoch: 4 step: 433, loss is 0.0004283045418560505\n",
      "epoch: 4 step: 434, loss is 0.00040164258098229766\n",
      "epoch: 4 step: 435, loss is 0.00024347662110812962\n",
      "epoch: 4 step: 436, loss is 0.0004630379844456911\n",
      "epoch: 4 step: 437, loss is 0.00017889708396978676\n",
      "epoch: 4 step: 438, loss is 0.0001638802350498736\n",
      "epoch: 4 step: 439, loss is 0.0010338954161852598\n",
      "epoch: 4 step: 440, loss is 0.0013132656458765268\n",
      "epoch: 4 step: 441, loss is 0.002713675145059824\n",
      "epoch: 4 step: 442, loss is 0.0005815087351948023\n",
      "epoch: 4 step: 443, loss is 0.0016627638833597302\n",
      "epoch: 4 step: 444, loss is 0.00035468157147988677\n",
      "epoch: 4 step: 445, loss is 0.00014845220721326768\n",
      "epoch: 4 step: 446, loss is 0.00043846643529832363\n",
      "epoch: 4 step: 447, loss is 0.000964250648394227\n",
      "epoch: 4 step: 448, loss is 0.0012035593390464783\n",
      "epoch: 4 step: 449, loss is 0.0011907313019037247\n",
      "epoch: 4 step: 450, loss is 0.002152383793145418\n",
      "epoch: 4 step: 451, loss is 0.0003862586454488337\n",
      "epoch: 4 step: 452, loss is 0.0015311050228774548\n",
      "epoch: 4 step: 453, loss is 0.0011137081310153008\n",
      "epoch: 4 step: 454, loss is 0.0005717085441574454\n",
      "epoch: 4 step: 455, loss is 0.0015495591796934605\n",
      "epoch: 4 step: 456, loss is 0.00037853975663892925\n",
      "epoch: 4 step: 457, loss is 0.00040821696165949106\n",
      "epoch: 4 step: 458, loss is 0.0014049563324078918\n",
      "epoch: 4 step: 459, loss is 0.0003085942007601261\n",
      "epoch: 4 step: 460, loss is 0.00043143387301824987\n",
      "epoch: 4 step: 461, loss is 0.0008762429351918399\n",
      "epoch: 4 step: 462, loss is 0.0013015000149607658\n",
      "epoch: 4 step: 463, loss is 0.00043191519216634333\n",
      "epoch: 4 step: 464, loss is 0.0002446402795612812\n",
      "epoch: 4 step: 465, loss is 0.0008950602496042848\n",
      "epoch: 4 step: 466, loss is 0.0009145920048467815\n",
      "epoch: 4 step: 467, loss is 0.00027802493423223495\n",
      "epoch: 4 step: 468, loss is 0.0006798497634008527\n",
      "epoch: 4 step: 469, loss is 0.0005948942853137851\n",
      "epoch: 4 step: 470, loss is 0.001575817121192813\n",
      "epoch: 4 step: 471, loss is 0.0007105623371899128\n",
      "epoch: 4 step: 472, loss is 0.0005318493931554258\n",
      "epoch: 4 step: 473, loss is 5.813377720187418e-05\n",
      "epoch: 4 step: 474, loss is 0.0009101248579099774\n",
      "epoch: 4 step: 475, loss is 0.0017954136710613966\n",
      "epoch: 4 step: 476, loss is 0.001065816031768918\n",
      "epoch: 4 step: 477, loss is 0.0006456958362832665\n",
      "epoch: 4 step: 478, loss is 0.0005741609493270516\n",
      "epoch: 4 step: 479, loss is 0.0002834248007275164\n",
      "epoch: 4 step: 480, loss is 0.0023533273488283157\n",
      "epoch: 4 step: 481, loss is 0.0005341056967154145\n",
      "epoch: 4 step: 482, loss is 0.00047845853259786963\n",
      "epoch: 4 step: 483, loss is 0.00018780198297463357\n",
      "epoch: 4 step: 484, loss is 0.0005370379658415914\n",
      "epoch: 4 step: 485, loss is 0.0005131865036673844\n",
      "epoch: 4 step: 486, loss is 0.001164183602668345\n",
      "epoch: 4 step: 487, loss is 0.0013486773241311312\n",
      "epoch: 4 step: 488, loss is 0.0009431247017346323\n",
      "epoch: 4 step: 489, loss is 0.001331230509094894\n",
      "epoch: 4 step: 490, loss is 0.0007593042682856321\n",
      "epoch: 4 step: 491, loss is 0.0005218202713876963\n",
      "epoch: 4 step: 492, loss is 0.00041411438724026084\n",
      "epoch: 4 step: 493, loss is 0.0002896469959523529\n",
      "epoch: 4 step: 494, loss is 0.0004808654193766415\n",
      "epoch: 4 step: 495, loss is 0.0011851030867546797\n",
      "epoch: 4 step: 496, loss is 0.0005138210253790021\n",
      "epoch: 4 step: 497, loss is 0.0009234293247573078\n",
      "epoch: 4 step: 498, loss is 0.0006641007494181395\n",
      "epoch: 4 step: 499, loss is 0.000450615247245878\n",
      "epoch: 4 step: 500, loss is 0.0029583682771772146\n",
      "epoch: 4 step: 501, loss is 0.0002873468038160354\n",
      "epoch: 4 step: 502, loss is 0.0008461250108666718\n",
      "epoch: 4 step: 503, loss is 0.001593911787495017\n",
      "epoch: 4 step: 504, loss is 0.0004947694251313806\n",
      "epoch: 4 step: 505, loss is 0.0008032905170693994\n",
      "epoch: 4 step: 506, loss is 0.0007319393334910274\n",
      "epoch: 4 step: 507, loss is 0.00034632167080417275\n",
      "epoch: 4 step: 508, loss is 0.0017084749415516853\n",
      "epoch: 4 step: 509, loss is 0.0012092373799532652\n",
      "epoch: 4 step: 510, loss is 0.001319828093983233\n",
      "epoch: 4 step: 511, loss is 0.0030919183045625687\n",
      "epoch: 4 step: 512, loss is 0.0003040183219127357\n",
      "epoch: 4 step: 513, loss is 0.0018154444405809045\n",
      "epoch: 4 step: 514, loss is 0.0004051311989314854\n",
      "epoch: 4 step: 515, loss is 0.0006026983028277755\n",
      "epoch: 4 step: 516, loss is 0.001901586540043354\n",
      "epoch: 4 step: 517, loss is 0.0007344192126765847\n",
      "epoch: 4 step: 518, loss is 0.00027249279082752764\n",
      "epoch: 4 step: 519, loss is 0.0015056662959977984\n",
      "epoch: 4 step: 520, loss is 0.0035340730100870132\n",
      "epoch: 4 step: 521, loss is 0.000756329856812954\n",
      "epoch: 4 step: 522, loss is 0.0011752862483263016\n",
      "epoch: 4 step: 523, loss is 0.0001860444899648428\n",
      "epoch: 4 step: 524, loss is 0.0003701981622725725\n",
      "epoch: 4 step: 525, loss is 0.0005258829914964736\n",
      "epoch: 4 step: 526, loss is 0.0010823628399521112\n",
      "epoch: 4 step: 527, loss is 0.000680724042467773\n",
      "epoch: 4 step: 528, loss is 0.00136431900318712\n",
      "epoch: 4 step: 529, loss is 0.0011656705755740404\n",
      "epoch: 4 step: 530, loss is 0.00033270573476329446\n",
      "epoch: 4 step: 531, loss is 0.0010779914446175098\n",
      "epoch: 4 step: 532, loss is 0.0016993237659335136\n",
      "epoch: 4 step: 533, loss is 0.0005423666443675756\n",
      "epoch: 4 step: 534, loss is 0.0001664913143031299\n",
      "epoch: 4 step: 535, loss is 0.0006224109092727304\n",
      "epoch: 4 step: 536, loss is 0.0006915199337527156\n",
      "epoch: 4 step: 537, loss is 0.0010970751754939556\n",
      "epoch: 4 step: 538, loss is 0.000981175689958036\n",
      "epoch: 4 step: 539, loss is 0.0007453839061781764\n",
      "epoch: 4 step: 540, loss is 0.0005702467169612646\n",
      "epoch: 4 step: 541, loss is 0.00014003453543409705\n",
      "epoch: 4 step: 542, loss is 0.00045431547914631665\n",
      "epoch: 4 step: 543, loss is 0.00027402807609178126\n",
      "epoch: 4 step: 544, loss is 0.0008299219771288335\n",
      "epoch: 4 step: 545, loss is 0.0002694839786272496\n",
      "epoch: 4 step: 546, loss is 0.0005993952509015799\n",
      "epoch: 4 step: 547, loss is 0.0005114588420838118\n",
      "epoch: 4 step: 548, loss is 0.0006193954031914473\n",
      "epoch: 4 step: 549, loss is 0.0005328869447112083\n",
      "epoch: 4 step: 550, loss is 0.0009224796667695045\n",
      "epoch: 4 step: 551, loss is 0.0007183485431596637\n",
      "epoch: 4 step: 552, loss is 0.00031758256955072284\n",
      "epoch: 4 step: 553, loss is 0.0018763188272714615\n",
      "epoch: 4 step: 554, loss is 0.00020349473925307393\n",
      "epoch: 4 step: 555, loss is 0.0008237112779170275\n",
      "epoch: 4 step: 556, loss is 0.0004079531063325703\n",
      "epoch: 4 step: 557, loss is 0.0010027892421931028\n",
      "epoch: 4 step: 558, loss is 0.000632344395853579\n",
      "epoch: 4 step: 559, loss is 0.0002977599506266415\n",
      "epoch: 4 step: 560, loss is 0.00046919219312258065\n",
      "epoch: 4 step: 561, loss is 0.0005259079625830054\n",
      "epoch: 4 step: 562, loss is 0.001116934814490378\n",
      "epoch: 4 step: 563, loss is 0.00036215141881257296\n",
      "epoch: 4 step: 564, loss is 0.0006904443143866956\n",
      "epoch: 4 step: 565, loss is 0.005434565246105194\n",
      "epoch: 4 step: 566, loss is 0.00030998719739727676\n",
      "epoch: 4 step: 567, loss is 0.004591668024659157\n",
      "epoch: 4 step: 568, loss is 0.0015360466204583645\n",
      "epoch: 4 step: 569, loss is 0.0006895121186971664\n",
      "epoch: 4 step: 570, loss is 0.00036568212090060115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 571, loss is 0.0005751006538048387\n",
      "epoch: 4 step: 572, loss is 0.00033876363886520267\n",
      "epoch: 4 step: 573, loss is 0.0006561881164088845\n",
      "epoch: 4 step: 574, loss is 0.006803364958614111\n",
      "epoch: 4 step: 575, loss is 0.0008383302483707666\n",
      "epoch: 4 step: 576, loss is 0.00020334456348791718\n",
      "epoch: 4 step: 577, loss is 0.000977130839601159\n",
      "epoch: 4 step: 578, loss is 0.0008503232384100556\n",
      "epoch: 4 step: 579, loss is 0.00026175682432949543\n",
      "epoch: 4 step: 580, loss is 0.0007281180587597191\n",
      "epoch: 4 step: 581, loss is 0.0007681372808292508\n",
      "epoch: 4 step: 582, loss is 0.0005760818021371961\n",
      "epoch: 4 step: 583, loss is 0.0005073759239166975\n",
      "epoch: 4 step: 584, loss is 0.00034306757152080536\n",
      "epoch: 4 step: 585, loss is 0.00040538483881391585\n",
      "epoch: 4 step: 586, loss is 0.0002640231396071613\n",
      "epoch: 4 step: 587, loss is 0.00041906567639671266\n",
      "epoch: 4 step: 588, loss is 0.0013691240455955267\n",
      "epoch: 4 step: 589, loss is 0.0011165792820975184\n",
      "epoch: 4 step: 590, loss is 0.0021648453548550606\n",
      "epoch: 4 step: 591, loss is 0.0005517362151294947\n",
      "epoch: 4 step: 592, loss is 0.001518978038802743\n",
      "epoch: 4 step: 593, loss is 0.0008494349895045161\n",
      "epoch: 4 step: 594, loss is 0.00014609258505515754\n",
      "epoch: 4 step: 595, loss is 0.0003636945621110499\n",
      "epoch: 4 step: 596, loss is 0.0009044886101037264\n",
      "Train epoch time: 119955.476 ms, per step time: 201.268 ms\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "# Start training.\n",
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d4724e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    " sentence = sentence.lower().strip()\n",
    " sentence = sentence.replace('\\n','')\\\n",
    "        .replace('\"','')\\\n",
    "        .replace('\\'','')\\\n",
    "        .replace('.','')\\\n",
    "        .replace(',','')\\\n",
    "        .replace('[','')\\\n",
    "        .replace(']','')\\\n",
    "        .replace('(','')\\\n",
    "        .replace(')','')\\\n",
    "        .replace(':','')\\\n",
    "        .replace('--','')\\\n",
    "        .replace('-',' ')\\\n",
    "        .replace('\\\\','')\\\n",
    "        .replace('0','')\\\n",
    "        .replace('1','')\\\n",
    "        .replace('2','')\\\n",
    "        .replace('3','')\\\n",
    "        .replace('4','')\\\n",
    "        .replace('5','')\\\n",
    "        .replace('6','')\\\n",
    "        .replace('7','')\\\n",
    "        .replace('8','')\\\n",
    "        .replace('9','')\\\n",
    "        .replace('`','')\\\n",
    "        .replace('=','')\\\n",
    "        .replace('$','')\\\n",
    "        .replace('/','')\\\n",
    "        .replace('*','')\\\n",
    "        .replace(';','')\\\n",
    "        .replace('<b>','')\\\n",
    "        .replace('%','')\\\n",
    "        .replace(\" \",\" \")\n",
    " sentence = sentence.split(' ')\n",
    " maxlen = cfg.word_len\n",
    " vector = [0]*maxlen\n",
    " for index, word in enumerate(sentence):\n",
    "     if index >= maxlen:\n",
    "         break\n",
    "     if word not in instance.Vocab.keys():\n",
    "         print(word,\"The word does not appear in the dictionary.\")\n",
    "     else:\n",
    "         vector[index] = instance.Vocab[word]\n",
    " sentence = vector\n",
    " return sentence\n",
    "\n",
    "def inference(review_en):\n",
    " review_en = preprocess(review_en)\n",
    " input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    " output = net(input_en)\n",
    " if np.argmax(np.array(output[0])) == 1:\n",
    "     print(\"Positive comments\")\n",
    " else:\n",
    "     print(\"Negative comments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "997cddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"the movie is so boring\"\n",
    "inference(review_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mon_env: mindspore)",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
