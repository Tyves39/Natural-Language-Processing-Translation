{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254cc8ff-69fd-4b63-907c-ececca016f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860a4155-1237-4683-ad63-a7cda5a2f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    " \"\"\"\n",
    " Tokenization/string cleaning for all datasets except for SST.\n",
    " Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    " \"\"\"\n",
    " string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    " string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    " string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    " string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    " string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    " string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    " string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    " string = re.sub(r\",\", \" , \", string)\n",
    " string = re.sub(r\"!\", \" ! \", string)\n",
    " string = re.sub(r\"\\(\", \" \\( \", string)\n",
    " string = re.sub(r\"\\)\", \" \\) \", string)\n",
    " string = re.sub(r\"\\?\", \" \\? \", string)\n",
    " string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    " return string.strip().lower()\n",
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    " \"\"\"\n",
    " Loads MR polarity data from files, splits the data into words and generates labels.\n",
    " Returns split sentences and labels.\n",
    " \"\"\"\n",
    " # Load data from files\n",
    " positive_examples = list(open(positive_data_file, \"r\", encoding='latin1').readlines())\n",
    " positive_examples = [s.strip() for s in positive_examples]\n",
    " negative_examples = list(open(negative_data_file, \"r\", encoding='latin1').readlines())\n",
    " negative_examples = [s.strip() for s in negative_examples]\n",
    " # Split by words\n",
    " x = positive_examples + negative_examples\n",
    " x = [clean_str(sent) for sent in x]\n",
    " x = np.array(x)\n",
    " # Generate labels\n",
    " positive_labels = [1] * len(positive_examples)\n",
    " negative_labels = [0] * len(negative_examples)\n",
    " y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "\n",
    " shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    " shuffled_x = x[shuffle_indices]\n",
    " shuffled_y = y[shuffle_indices]\n",
    "\n",
    " return shuffled_x, shuffled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262ddaba-b0b3-4e83-ae71-231b7f478e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_file = 'data/rt-polarity.pos'\n",
    "negative_data_file = 'data/rt-polarity.neg'\n",
    "x, y = load_data_and_labels(positive_data_file, negative_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124e15f4-77fd-4e1c-ae0a-2dec229822d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"it would n't be my preferred way of spending 100 minutes or 7 00\",\n",
       "       \"but the power of these subjects is obscured by the majority of the film that shows a stationary camera on a subject that could be mistaken for giving a public oration , rather than contributing to a film 's narrative\",\n",
       "       \"it moves quickly , adroitly , and without fuss it does n't give you time to reflect on the inanity and the cold war datedness of its premise\",\n",
       "       \"the film 's needlessly opaque intro takes its doe eyed crudup out of pre 9 11 new york and onto a cross country road trip of the homeric kind\",\n",
       "       'plods along , minus the twisted humor and eye popping visuals that have made miike a cult hero'],\n",
       "      dtype='<U266')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87571368-cb04-41f4-ac88-5c23eb6952a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dcf35d3-81be-4128-a246-a0af30d21098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7059, 12332, 17420,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [ 7059, 12332,  5918,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for doc in x:\n",
    " for word in doc.split(' '):\n",
    "     if word.strip():\n",
    "         vocab.add(word.strip().lower())\n",
    "         \n",
    "# write to vocab.txt file\n",
    "with open('data/vocab.txt', 'w') as file:\n",
    " for word in vocab:\n",
    "     file.write(word)\n",
    "     file.write('\\n')\n",
    "     \n",
    "test_size = 2000\n",
    "x_train, y_train = x[:-2000], y[:-2000]\n",
    "x_test, y_test = x[-2000:], y[-2000:]\n",
    "label_map = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "\n",
    "class Config():\n",
    " embedding_dim = 100 # word embedding dimention\n",
    " max_seq_len = 200 # max sequence length\n",
    " vocab_file = 'data/vocab.txt' # vocab_file_length\n",
    "config = Config()\n",
    "\n",
    "class Preprocessor():\n",
    " def __init__(self, config):\n",
    "     self.config = config\n",
    "     # initial the map of word and index\n",
    "     token2idx = {\"[PAD]\": 0, \"[UNK]\": 1} # {wordï¼šid}\n",
    "     with open(config.vocab_file, 'r') as reader:\n",
    "         for index, line in enumerate(reader):\n",
    "             token = line.strip()\n",
    "             token2idx[token] = index+2\n",
    "\n",
    "     self.token2idx = token2idx\n",
    "\n",
    " def transform(self, text_list):\n",
    "     # tokenization, and transform word to coresponding index\n",
    "     idx_list = [[self.token2idx.get(word.strip().lower(), self.token2idx['[UNK]']) for word in text.split(' ')] for text in text_list]\n",
    "     idx_padding = pad_sequences(idx_list, self.config.max_seq_len, padding='post')\n",
    "\n",
    "     return idx_padding\n",
    "preprocessor = Preprocessor(config)\n",
    "preprocessor.transform(['I love working', 'I love eating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4acf272a-7372-4314-b100-47ef218b417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.preprocessor = Preprocessor(config)\n",
    "        self.class_name = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "    def build_model(self):\n",
    "        # build model architecture\n",
    "        idx_input = tf.keras.layers.Input((self.config.max_seq_len,))\n",
    "        input_embedding = tf.keras.layers.Embedding(\n",
    "            len(self.preprocessor.token2idx),\n",
    "            self.config.embedding_dim,\n",
    "            input_length=self.config.max_seq_len,\n",
    "            mask_zero=True\n",
    "        )(idx_input)\n",
    "        convs = []\n",
    "        for kernel_size in [2, 3, 4, 5]:\n",
    "            c = tf.keras.layers.Conv1D(128, kernel_size, activation='relu')(input_embedding)\n",
    "            c = tf.keras.layers.GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        fea_cnn = tf.keras.layers.Concatenate()(convs)\n",
    "        fea_cnn = tf.keras.layers.Dropout(rate=0.5)(fea_cnn)\n",
    "        fea_dense = tf.keras.layers.Dense(128, activation='relu')(fea_cnn)\n",
    "        fea_dense = tf.keras.layers.Dropout(rate=0.5)(fea_dense)\n",
    "        fea_dense = tf.keras.layers.Dense(64, activation='relu')(fea_dense)\n",
    "        fea_dense = tf.keras.layers.Dropout(rate=0.3)(fea_dense)\n",
    "        output = tf.keras.layers.Dense(2, activation='softmax')(fea_dense)\n",
    "\n",
    "        model = tf.keras.Model(inputs=idx_input, outputs=output)\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, x_train, y_train, x_valid=None, y_valid=None, epochs=5, batch_size=128, **kwargs):\n",
    "        # train\n",
    "        self.build_model()\n",
    "\n",
    "        x_train = self.preprocessor.transform(x_train)\n",
    "        if x_valid is not None and y_valid is not None:\n",
    "            x_valid = self.preprocessor.transform(x_valid)\n",
    "        self.model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            validation_data=(x_valid, y_valid) if x_valid is not None and y_valid is not None else None,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        # evaluate\n",
    "        x_test = self.preprocessor.transform(x_test)\n",
    "        y_pred_probs = self.model.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=-1)\n",
    "        result = classification_report(y_test, y_pred, target_names=['negative', 'positive'])\n",
    "        print(result)\n",
    "\n",
    "    def single_predict(self, text):\n",
    "        # predict\n",
    "        input_idx = self.preprocessor.transform([text])\n",
    "        predict_prob = self.model.predict(input_idx)[0]\n",
    "        predict_label_id = np.argmax(predict_prob)\n",
    "        predict_label_name = self.class_name[predict_label_id]\n",
    "        predict_label_prob = predict_prob[predict_label_id]\n",
    "\n",
    "        return predict_label_name, predict_label_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21da436f-60e9-4465-a752-444f1063a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 100)     1876600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 199, 128)     25728       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 128)     38528       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 197, 128)     51328       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 196, 128)     64128       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          65664       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            130         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,130,362\n",
      "Trainable params: 2,130,362\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8662 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8662/8662 [==============================] - 36s 4ms/sample - loss: 0.6944 - accuracy: 0.5132 - val_loss: 0.6912 - val_accuracy: 0.5880\n",
      "Epoch 2/10\n",
      "8662/8662 [==============================] - 36s 4ms/sample - loss: 0.6738 - accuracy: 0.5789 - val_loss: 0.6058 - val_accuracy: 0.6960\n",
      "Epoch 3/10\n",
      "8662/8662 [==============================] - 36s 4ms/sample - loss: 0.4763 - accuracy: 0.7787 - val_loss: 0.4748 - val_accuracy: 0.7725\n",
      "Epoch 4/10\n",
      "8662/8662 [==============================] - 32s 4ms/sample - loss: 0.2160 - accuracy: 0.9254 - val_loss: 0.5707 - val_accuracy: 0.7735\n",
      "Epoch 5/10\n",
      "8662/8662 [==============================] - 32s 4ms/sample - loss: 0.0887 - accuracy: 0.9730 - val_loss: 0.8105 - val_accuracy: 0.7550\n",
      "Epoch 6/10\n",
      "8662/8662 [==============================] - 31s 4ms/sample - loss: 0.0444 - accuracy: 0.9865 - val_loss: 1.0893 - val_accuracy: 0.7425\n",
      "Epoch 7/10\n",
      "8662/8662 [==============================] - 32s 4ms/sample - loss: 0.0226 - accuracy: 0.9938 - val_loss: 1.1730 - val_accuracy: 0.7530\n",
      "Epoch 8/10\n",
      "8662/8662 [==============================] - 31s 4ms/sample - loss: 0.0149 - accuracy: 0.9957 - val_loss: 1.3362 - val_accuracy: 0.7570\n",
      "Epoch 9/10\n",
      "8662/8662 [==============================] - 31s 4ms/sample - loss: 0.0182 - accuracy: 0.9946 - val_loss: 1.3754 - val_accuracy: 0.7460\n",
      "Epoch 10/10\n",
      "8662/8662 [==============================] - 31s 4ms/sample - loss: 0.0141 - accuracy: 0.9961 - val_loss: 1.5334 - val_accuracy: 0.7400\n"
     ]
    }
   ],
   "source": [
    "textcnn = TextCNN(config)\n",
    "textcnn.fit(x_train, y_train, x_test, y_test, epochs=10) # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da3636ae-b419-4239-923f-01f452195482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.70      0.73      1019\n",
      "    positive       0.71      0.78      0.75       981\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.74      0.74      0.74      2000\n",
      "weighted avg       0.74      0.74      0.74      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textcnn.evaluate(x_test, y_test) # Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00e99fb4-24e7-4b37-9697-0ef277832936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('positive', 0.98573536)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcnn.single_predict(\"beautiful actors, great movie.\") # single sentence predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf1d65b8-e7e7-45da-9ee0-23ab1c1d8e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', 0.999998)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcnn.single_predict(\"it's really boring\") # single sentence predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-2.1.0",
   "language": "python",
   "name": "hcip-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
